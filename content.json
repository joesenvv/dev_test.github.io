[{"title":"理解函数式编程","date":"2018-05-11T16:00:00.000Z","path":"2018/05/understand-functional-programing/","text":"源自于我做的一次公司内部的技术分享，这是初稿。 PPT 就不贴了。 一. 序章: 可计算模型1936 阿隆佐·邱奇发表可计算函数的第一份精确定义，即Lambda演算(λ演算)。λ演算是一套从数学逻辑中发展，以变量绑定和替换的规则，来研究函数如何抽象化定义、函数如何被应用以及递归的形式系统。1936 艾伦·图灵提出图灵机设想，通过TAPE,HEAD,TABLE,STATE四个部分模拟人的纸笔运算1945 冯·诺伊曼提出冯·诺伊曼结构(存储程序型电脑)，是通用图灵机实现1958 约翰·麦卡锡，发明了一种列表处理语言（Lisp），这种语言是一种阿隆佐lambda演算在现实世界的实现，而且它能在冯·诺伊曼计算机上运行 图灵机设想和 Lamdba 演算都是一种抽象计算模型，是解决可计算问题的一种方案，通俗来讲，就是将计算以数学的方式抽象化，以便证明和推导。图灵机和 Lamdba 演算这两个可计算理论已经被证明是等价的。 函数式的起源Lambda 演算甚至还要早于图灵机，但图灵机由于其更具实现意义，也更容易被理解，因此很早就有了硬件实现基础，而 Lambda 演算是一套更接近软件而非硬件的形式系统，Lambda 演算的第一台硬件则要等到1973年的 MIT 人工智能实验室开发的Lisp机。 二. 什么是函数式编程函数式编程是一种编程范式，我们常见的编程范式有命令式编程，函数式编程，逻辑式编程，常见的面向对象编程和面向过程编程都是命令式编程。 命令式编程是面向计算机硬件的抽象，如变量(抽象存储单元)，表达式(算数运算与内存读写)，控制语句(跳转指令)，最终得到一个冯诺依曼机的指令序列。 函数式编程的基础模型来源于λ演算，是阿隆佐思想的在现实世界中的实现。不过不是全部的lambda演算思想都可以运用到实际中，因为lambda演算在设计的时候就不是为了在各种现实世界中的限制下工作的(毕竟是数学家捣鼓出来的东西)。 目前的函数式编程语言基本都是翻译为冯诺依曼指令实现的。 三. 函数式编程概念变量: 命令式: 代表存储可变状态的单元(内存地址)，相当于地址的别名 x = x + 1 函数式: 代表数学函数中的变量，映射到某个值，相当于值的别名 2x = 4 函数: 命令式: 描述求解过程(怎么做)，本质上是一系列的冯诺依曼机指令，can do anything 函数式: 数学概念里的函数，描述映射(计算)关系(做什么)，也称为纯函数/无状态函数 四. 函数式编程特性1. 不可变语义比如我们有一个 Point类，其 moveBy 方法对 x,y 坐标进行偏移，得到新的 Point 对象: // 命令式写法: 直接修改内存值 func (p *Point) moveBy(deltaX, deltaY int) { p.x = p.x + deltaX p.y = p.y + deltaX } // 函数式写法: 新建一个 Point 对象，函数本身只需关心对象的映射，而非对象复用等实现细节。在语义上，新旧 Point 代表完全独立并可不变的两个&quot;Point 值&quot;。 func moveBy(p *Point, deltaX, deltaY int) *Point { return &amp;Point{ x: p.x+deltaX, y: p.y+deltaY, } } 没有可变状态，也就没有 for, while 循环，使得函数式编程严重依赖递归。 这里的不可变，指的是语义上的不可变，而非其底层实现上的不变。比如Erlang 虚拟机用 C 实现，通过写时拷贝来实现不可变语义，但针对比如binary 升级，Pid打包等，虽然底层的实现结构可能会改变，但对应用层来说语义是不变的。 2. 纯函数 确定性: 相同输入得到相同输出，函数的返回值和参数以外的其他隐藏信息或状态无关 无副作用: 函数不能有语义上可观察的副作用，诸如“触发事件”，使输出设备输出，或更改输出值以外对象的内容等 纯函数都是显式数据流的，即所有的输入都通过参数传入，所有输出都通过返回值传出。 纯函数即无状态函数，微服务中的无状态服务也有一个无状态似，但关注的是整个处理流程的无状态，即把状态往两边推到输入(如Network)和输出(DB，Cache，Log 等)，这样服务能够横向扩展和动态伸缩(就像一个巨大的纯函数)。而对函数式而言，理论上演算过程中的任一处理步骤(每次函数调用)都是无状态的，因此可伸缩和并发的粒度更小。 在实现上，无状态和不可变语义一样，是语义上的，无状态是指函数本身不能有可以改变状态的指令，从系统层次上来看，要说绝对没状态是不可能的，比如调用函数会导致进程栈增长，页面换入换出等。另外，在 Haskell 语言中，还有惰性求值，会使得在执行纯函数时执行一些外部操作(比如文件 IO)。因此从这个角度来看，我们用命令式语言也可以实现无状态效果。 3. 递归与尾递归递归在几乎所有命令式编程语言中都有，即函数调用自身，在函数式编程中，由于没有可变状态，for, while 循环都只能通过递归来实现，因此函数式编程严重依赖递归: 以阶乘为例: // 自底向上，递推求解 func fact_1(n int) int { acc := 1 for k:=1; k&lt;=n; k++ { acc = acc*k } return acc } // 自顶向下，递归求解 func fact_2(n int) int { if n == 0 { return 1 } else { return fact_2(n-1)*n } } // 自顶向下，递归求解 - Erlang fact(N) when N == 0 -&gt; 1; fact(N) when N &gt; 0 -&gt; N*fact(N-1). // 递归求解 - Erlang - 尾递归版 fact1(N) -&gt; fact(1, N). fact(Acc, N) when N == 0 -&gt; Acc; fact(Acc, N) when N &gt; 0 -&gt; fact(Acc*N, N-1). 我们知道在现代冯诺依曼结构计算机中，递归和普通函数一样，是通过函数调用栈来实现的，为了防止函数栈肆意扩展(导致栈溢出)，通常函数式语言的编译器都会实现尾调用优化。尾调用是指一个函数里的最后一个动作是返回一个函数的调用结果的情形，即最后一步新调用的返回值直接被当前函数的返回结果。这种情况下，函数可以直接复用当前函数栈的栈空间执行尾调用，减少栈空间使用并提高效率。(尾递归是函数式基于命令式语言实现时的一个实现上的优化(栈空间是有限的)，并非Lamda 演算本身的内容。) 递归是函数式中非常核心的一个概念，其在函数式语言中的低位要比在命令式语言高很多，理解递归是理解函数式的基础。递归是另一种我们思考和解决问题的方式，在一些情景下，使用递归会让问题简化许多。比如经典的找零钱问题: // 若干面值钞票，给一张大面值货币要兑换成零钱，求有多少种兑换方式 // 递归版本 func countChange(money int, coins []int) int { if money &lt; 0 || len(coins) == 0 { return 0 } if money == 0 { return 1 } return countChange(money, coins[1:]) + countChange(money-coins[0], coins) } // 迭代版本 func countChange2(changes []int, money int) int { n := len(changes) dp := make([]int, money+1) for i:=1; i&lt;=money; i++ { dp[i] = 0 } dp[0] = 1 for i:=0; i&lt;n; i++ { for j:=changes[i]; j&lt;=money; j++{ dp[j] += dp[j-changes[i]] } } return dp[money] } 可以看到，递归版比迭代版要容易理解得多，后者需要手动维护状态，而前者将状态隐藏到递归过程中了。类似的还有背包问题，都是理解递归很好的例子。 4. 惰性求值/乱序求值表达式不在它被绑定到变量之后就立即求值，而是在该值被取用的时候求值。 A = dosomething1() B = dosomething2(A) C = dosomething3() 编译器很容易就可以推断出，A-&gt;B 和 C 可以乱序(无状态)或者并行(不可变语义)执行，并且对 A，B，C 的求值可以等到它们被引用的时候再计算，因此函数式本身是比较好优化的。 Haskell 语言天生就是惰性的，你可以用[1..]表示1到无穷大的列表，只要有获取这个列表的第 N 个元素时，Haskell 才会去计算它:[1..] !! 999返回列表中下标为999的元素，这里会计算1-1000的列表，最终返回1000。另外个例子，如果 xs 是一个列表，doubleMe 会将 xs 中的元素*2，那么在Haskell中，doubleMe(doubleMe(doubleMe(xs)))只会遍历列表一次。 5. 柯里化(currying)柯里化:将一个多参数函数分解成多个单参数函数，然后将单参函数多层封装起来，每层函数都返回一个函数去接收下一个参数，这可以简化函数的多个参数。比如前面我们的尾递归版阶乘计算，可以写成: fact2(Acc) -&gt; fun(N) -&gt; fact(Acc, N) end. F = fact2(1) F(10) 换成 Lua 可能更容易理解，比如我们有两个参数的 add : add = function(x, y) return x + y end print(add(3,4)) 可将其柯里化为: add = function(x) return function(y) return x + y end end print(add(3)(4)) 上面我们列举的柯里化看起来很像Lua,Go等语言中的闭包(词法闭包，Lexical Closure)，实际上柯里化是Lambda 演算中的概念，用于简化推导流程。闭包只是一种实现方式，如C++ STL 的bind1st和bind2nd也可以实现固化函数参数的作用。 柯里化所要表达是:如果你固定某些参数，你将得到接受余下参数的一个函数,所以对于有两个变量的函数x+y，如果固定了x=3，则得到有一个变量的函数3+y(是不是很像多项式一个一个变量求解)。这就是求值策略中的部分求值策略。柯里化具有延迟计算、参数复用、动态生成函数的作用。 6. 高阶函数函数式的要义之一就是将函数当做普通对象一样，可以被当做参数传递，也可以作为函数返回值(在λ演算创始人阿隆佐·邱奇的眼里，一切且函数，变量也是函数)。所谓高阶函数就是以函数为参数或返回值的函数，有了高阶函数，就可以将复用的粒度降至函数级别，相对面向对象而言，复用粒度更低。 sumInts(A, B) when A &gt; B -&gt; 0; sumInts(A, B) -&gt; A + sumInts(A+1, B). sumFacts(A, B) when A &gt; B -&gt; 0; sumFacts(A, B) -&gt; fact1(A) + sumFacts(A+1, B). sum(F, A, B) when A &gt; B -&gt; 0; sum(F, A, B) -&gt; F(A) + sum(F, A+1, B). &gt; sum(fun fact1/1, 3, 6) 函数式语言通常提供了非常强大的”集合类”，可以基于高阶函数提供便捷的集合操作: 37&gt; L = [1,2,3]. [1,2,3] 38&gt; lists:map(fun(E) -&gt; 2*E end, L). [2,4,6] 39&gt; lists:foldl(fun(E, Acc) -&gt; Acc+E end, 0, L). 6 得益于函数式的无状态和不可变语义，将lists:map 函数改写为并发执行的 map 只需几十行代码，安全无副作用。 可能接触过面向对象编程的人都接触或使用过设计模式，在函数式编程里面，通过 function 实现设计模式要比 class, interface 那一套要简洁灵活得多。比如lists:map本身就是一个很好用的访问者模式。 7. Monad前面提到，函数式中变量不可变，且纯函数没有副作用，那么函数式如何处理可变状态比如 IO 呢，在 Erlang 里，IO 通过 C 实现，即引入了可变性和副作用，并且 Erlang没有对这种副作用代码和纯代码分隔开，这得依靠程序员来做。而纯函数式语言 Haskell 采用了另一种方案: Monad。 单子(Monad)是来自范畴论中的概念，范畴论是数学的一门学科，以抽象的方法来处理数学概念，将这些概念形式化成一组组的“对象”及“态射”。 Monad 的概念被引入到 Haskell，表示”注入”和”提取”的概念，用于处理 IO，串联函数等。 Haskell严格地把纯代码从那些有副作用的代码(如 IO)中分隔开。就是说，它给纯代码提供了完全的副作用隔离。除了帮助程序员推断他们自己代码的正确性，它还使编译器可以自动采取优化和并行化成为可能。而 Monad 加上 Haskell 的类型类即成为分离纯函数和副作用函数的利器: 图片出处 上图说明了 Haskell 如何通过 Monad 管理纯函数副作用函数。具体到代码，看起来像是这样: name2reply :: String -&gt; String name2reply name = &quot;Pleased to meet you, &quot; ++ name ++ &quot;.\\n&quot; ++ &quot;Your name contains &quot; ++ charcount ++ &quot; characters.&quot; where charcount = show (length name) main :: IO () main = do putStrLn &quot;Greetings once again. What is your name?&quot; inpStr &lt;- getLine let outStr = name2reply inpStr putStrLn outStr Haskell在纯代码和I/O动作之间做了很明确的区分。很多语言没有这种区分。 在C或者Java(包括 Erlang)这样的语言中，编译器不能保证一个函数对于同样的参数总是返回同样的结果，或者保证函数没有副作用。程序中的很多错误都是由意料之外的副作用造成的。做好这种隔离有利于程序员和编译器更好地思考和优化程序。 五. 函数式的优缺点 并发性: 函数无副作用(天然可重入)，原生并发友好 确定性: 可读性高，易于测试和调试，错误易于重现 没有锁和指针就没有伤害 具有很大的优化潜力，如惰性求值，并发，缓存函数计算结果等，很多原本需要程序来做的事情，都可以由编译器来做。比如动态规划的缓存，MapReduce 等。 缺点: 处理可变状态如 IO 的能力弱(要么使用可变状态，要么使用 Monad) 为了维持不可变性，拷贝的开销 运行效率，依靠并发 六. 函数式的实现1. ErlangErlang 不是一门纯函数式语言(提供了外部可变状态组件，如进程字典，Ets)，但它充分利用函数式的无状态和不可变语义，将函数式的各种优势很好地利用了起来。Erlang 的具体介绍和细节我们就谈了，我着重讲一下其三个让其它语言”眼红”的三个特性: 高并发Erlang 是 Actor 模型，一个 Actor 即一个 Erlang 进程，创建一个进程是微秒级的。Erlang 可以说是最早支持协程的语言之一，Actor 与 Actor 之间通过消息交互，加上不可变语义，使得 Erlang 比其它语言更易于实现安全的并发(Erlang/OTP并发编程实践作者将共享内存比作这个时代的goto)。 热更新说起代码热替换，估计没有语言比 Erlang 做得更好了，这是 Erlang 的黑科技，一个用了你就回不去的功能。热更新对于游戏服务器的意义是很大的，游戏开发版本更新快，有时候有些不大不小的问题(比如一些调试日志忘删除了，某个小功能可能无法使用)，通过重启来解决的代价太大(用户流失)，用 Erlang 直接无缝热更，比无缝重启要方便快捷太多。 热更新的原理是代码版本替换，一方面仰仗于Erlang运行时的代码加载机制，另一方面，函数式也功不可没，一个具有内部状态的函数是很难做热更新的。 容错性Erlang 有一个很有意思的slogan，叫”let it crash”，这看起来与命令式编程中的”defensive coding(防御式编程)”背道而驰，在大型分布式程序的构建过程中，代码会遇到各种各样的异常，代码编写者不可能或者说很难预料并且处理到所有的异常，在这个背景下，Erlang 提出”let it crash” 的概念: 既然未预料的异常无可避免，那么就应该统一隔离处理并且恢复它，注意，这里提到了三个词: 错误隔离，错误处理和故障恢复，大部分语言最多做到第一步(比如 Go 语言)，而 Erlang 把整个三步都做到了虚拟机中。一个进程挂了，会由监督者发现，并且根据重启策略重启。 Actor 的状态隔离与轻量为整个容错性提供基础保障。 总的来说，Erlang 在并发，健壮性，可伸缩性方面，都有非常出色的表现。适用于 IO 密集型的软实时系统(设计之初就是为了解决电信通信问题的)。 2. HaskellErlang并不是一门纯函数式语言的话，进程字典，ets，IO 等状态性和副作用，Haskell 则剑走偏锋，号称是纯函数式语言。Haskell 我只看了一部分，以下是一些基础特性: 支持惰性求值 更彻底的函数(+,-) 静态类型系统 + 类型推导 + 强大的typeclass (==, Functor)，支持自定义类 原生柯里化(map (+1) [1,2,3]) 引入范畴论的 Monad 七. 最后1. 为什么函数式没有崛起?图灵机是一个具备实现意义的 model，或者换句话说，lambda 演算则是更抽象，更上层的计算模型，图灵机也更符合人解决问题的方式(纸笔推演)，更容易被没有数学背景的工程师所理解，因此很快在现实世界中实现推广。 一个有意思的论题是Worse Is Better，由 Common Lisp 专家Richard P. Gabriel在90年代反思 Lisp 这么牛的语言却日渐式微提出的观点，他提出软件设计有以下四大目标: 简单性，正确性，一致性，完整性 函数式语言: 正确性 = 一致性 &gt; 完整性 &gt; 简单(为了接口简单，宁愿实现复杂) 命令式语言: 简单性（实现简单优于接口简单）&gt; 正确性 &gt; 完整性 &gt; 一致性 原文 Worse Is Better 中的 Worse 是指C/Unix将其简单性优于其它目标的做法(在作者看来是糟糕的)，Better 是指 C/Unix 当下的处境(当然是非常流行)，因此可以粗略理解为”简单的就是好的”。作者将C/Unix 的流行归功于他们实现简单，使得其像”病毒”一样可快速移植到与传播，用户也更愿意接受，等到用户产生依赖，再逐步完善。 当然，可能还有一个原因，那个年代计算机是稀缺资源，比较注重效率，而在冯诺依曼机上的实现的函数式语言自然没有早期的命令式语言快，自然也让 C，C+这类”简单粗暴”的语言飞速推广。再借助 OS，编译器等形成生态。 2. 函数式近几年开始受到更多的关注由于软件复杂度和 CPU 核数的增加，多线程和其他形式的并行化变得越来越普遍， 管理全局副作用变得越来越困难，函数式的好处(正确性和一致性)开始体现出来，几十年前写的 Erlang 代码放到今天，无需任何改动，核数越多，跑得越快，因此现在的软件设计理念都会从函数式思想中学习一些东西。 从设计上来讲，现代语言都或多或少吸收了函数式的一些特性: 如function，闭包，尾递归，高阶函数等等。从理念上来讲，函数式的无状态为并发提供了另一种解决方案，比如近几年推崇的无状态服务设计，Erlang 的 Actor 模型等。 如今很多语言都支持多种编程范式，一些函数式语言也有可变状态，一些命令式语言支持部分函数式特性，理解函数式可以扩展我们解决问题的思路，找到更简洁有效的解决方案，如递归而不是迭代，函数注入而不是对象注入等。最后，我认为每个程序员都可以去学习一门函数式语言，扩展自己的思维，写出更灵活，安全，”纯洁”的代码。 参考: 什么是函数式思维? Haskell: Real World Haskell 中文版 图解 Monad Monad 最简介绍 Haskell 与范畴轮 Haskell 趣学指南","tags":[{"name":"programing","slug":"programing","permalink":"http://wudaijun.com/tags/programing/"}]},{"title":"go pprof 性能分析","date":"2018-04-03T16:00:00.000Z","path":"2018/04/go-pprof/","text":"一. pprof 数据采样pprof 采样数据主要有三种获取方式: runtime/pprof: 手动调用runtime.StartCPUProfile或者runtime.StopCPUProfile等 API来生成和写入采样文件，灵活性高 net/http/pprof: 通过 http 服务获取Profile采样文件，简单易用，适用于对应用程序的整体监控。通过 runtime/pprof 实现 go test: 通过 go test -bench . -cpuprofile prof.cpu生成采样文件 适用对函数进行针对性测试 1.1 net/http/pprof在应用程序中导入import _ &quot;net/http/pprof&quot;，并启动 http server即可: // net/http/pprof 已经在 init()函数中通过 import 副作用完成默认 Handler 的注册 go func() { log.Println(http.ListenAndServe(&quot;localhost:6060&quot;, nil)) }() 之后可通过 http://localhost:6060/debug/pprof/CMD 获取对应的采样数据。支持的 CMD 有: goroutine: 获取程序当前所有 goroutine 的堆栈信息。 heap: 包含每个 goroutine 分配大小，分配堆栈等。每分配 runtime.MemProfileRate(默认为512K) 个字节进行一次数据采样。 threadcreate: 获取导致创建 OS 线程的 goroutine 堆栈 block: 获取导致阻塞的 goroutine 堆栈(如 channel, mutex 等)，使用前需要先调用 runtime.SetBlockProfileRate mutex: 获取导致 mutex 争用的 goroutine 堆栈，使用前需要先调用 runtime.SetMutexProfileFraction 以上五个 CMD 都通过runtime/pprof Profile 结构体统一管理，以 Lookup 提供统一查询接口，有相似的返回值(goroutine 堆栈)，它们都支持一个 debug URL参数，默认为0，此时返回的采样数据是不可人为解读的函数地址列表，需要结合 pprof 工具才能还原函数名字。 debug=1时，会将函数地址转换为函数名，即脱离 pprof 在浏览器中直接查看。对 goroutine CMD来说，还支持 debug=2，此时将以 unrecovered panic 的格式打印堆栈，可读性更高。如启用net/http/pprof后，http://localhost:6060/debug/pprof/goroutine?debug=2 的响应格式为: goroutine 18 [chan receive, 8 minutes]: ngs/core/glog.logWorker(0x18b548a, 0x4, 0x7fff5fbffb0e, 0x0, 0x3, 0xc4200e31a0, 0xc4203627c4) /Users/wudaijun/go/src/ngs/core/glog/worker.go:43 +0x19c created by ngs/core/glog.newLogger /Users/wudaijun/go/src/ngs/core/glog/glog.go:51 +0xe4 goroutine 6 [syscall, 8 minutes]: os/signal.signal_recv(0x0) /usr/local/Cellar/go/1.9.1/libexec/src/runtime/sigqueue.go:131 +0xa7 os/signal.loop() /usr/local/Cellar/go/1.9.1/libexec/src/os/signal/signal_unix.go:22 +0x22 created by os/signal.init.0 /usr/local/Cellar/go/1.9.1/libexec/src/os/signal/signal_unix.go:28 +0x41 goroutine 50 [select, 8 minutes]: context.propagateCancel.func1(0x1cfcee0, 0xc42017a1e0, 0x1cf3820, 0xc42005b480) /usr/local/Cellar/go/1.9.1/libexec/src/context/context.go:260 +0x113 created by context.propagateCancel /usr/local/Cellar/go/1.9.1/libexec/src/context/context.go:259 +0x1da ... 以上几种 Profile 可在 http://localhost:6060/debug/pprof/ 中看到，除此之外，go pprof 的 CMD 还包括: cmdline: 获取程序的命令行启动参数 profile: 获取指定时间内(从请求时开始)的cpuprof，倒计时结束后自动返回。参数: seconds, 默认值为30。cpuprofile 每秒钟采样100次，收集当前运行的 goroutine 堆栈信息。 symbol: 用于将地址列表转换为函数名列表，地址通过’+’分隔，如 URL/debug/pprof?0x18d067f+0x17933e7 trace: 对应用程序进行执行追踪，参数: seconds, 默认值1s 这几个 CMD 因为各种原因没有整合到 Profile 结构中去，但就使用上而言，是没有区别的，URL格式是一致的，因此可以看做一个整体，从各个角度对系统进行数据采样和分析。 1.2 runtime/pprofruntime/pprof提供各种相对底层的 API 用于生成采样数据，一般应用程序更推荐使用net/http/pprof，runtime/pprof 的 API 参考runtime/pprof或 http pprof 实现。 1.3 go test通常用net/http/pprof或runtime/pprof对应用进行整体分析，找出热点后，再用go test进行基准测试，进一步确定热点加以优化并对比测试。 # 生成 test 二进制文件， pprof 工具需要用到 ▶ go test -c -o tmp.test # 执行基准测试 BenchAbc，并忽略任何单元测试，test flag前面需要加上&#39;test.&#39;前缀 ▶ tmp.test -test.bench BenchAbc -test.run XXX test.cpuprofile cpu.prof # 与上面两条命令等价，只不过没有保留 test 二进制文件 ▶ go test -bench BenchAbc -run XXX -cpuprofile=cpu.prof . go test可以直接加-cpuprofile -mutexprofilefraction等参数实现prof数据的采样和生成，更多相关参数参考 go test -h。 二. pprof 数据分析虽然 net/http/pprof提供的数据分析可以通过设置参数后直接在浏览器查看，但 pprof 采样数据主要是用于 pprof 工具的，特别针对 cpuprof, memprof, blockprof等来说，我们需要直观地得到整个调用关系链以及每次调用的详细信息，这是需要通过go tool pprof命令来分析: go tool pprof [binary] [binary.prof] # 如果使用的 net/http/pprof 可以直接接 URL go tool pprof http://localhost:6060/debug/pprof/profile go pprof 采样数据是非常丰富的，大部分情况下我们只会用到 CPU 和 内存分析，因此这里介绍下 cpu, heap, block 和 mutex 四种 pprof 数据分析。 2.1 cpuprofile以Profiling Go Programs中的示例代码为例: ▶ go build -o havlak1 havlak1.go ▶ ./havlak1 --cpuprofile=havlak1.prof # of loops: 76000 (including 1 artificial root node) ▶ go tool pprof havlak1 havlak1.prof File: havlak1 Type: cpu Time: Apr 3, 2018 at 3:50pm (CST) Duration: 20.40s, Total samples = 23.30s (114.24%) Entering interactive mode (type &quot;help&quot; for commands, &quot;o&quot; for options) (pprof) top5 Showing nodes accounting for 9.60s, 41.20% of 23.30s total Dropped 112 nodes (cum &lt;= 0.12s) Showing top 5 nodes out of 90 flat flat% sum% cum cum% 2.59s 11.12% 11.12% 2.78s 11.93% runtime.mapaccess1_fast64 /usr/local/Cellar/go/1.9.1/libexec/src/runtime/hashmap_fast.go 2.26s 9.70% 20.82% 4.97s 21.33% runtime.scanobject /usr/local/Cellar/go/1.9.1/libexec/src/runtime/mgcmark.go 2.06s 8.84% 29.66% 13.79s 59.18% main.FindLoops /Users/wudaijun/Code/goprof/havlak/havlak1.go 1.39s 5.97% 35.62% 1.39s 5.97% runtime.heapBitsForObject /usr/local/Cellar/go/1.9.1/libexec/src/runtime/mbitmap.go 1.30s 5.58% 41.20% 4.14s 17.77% runtime.mapassign_fast64 /usr/local/Cellar/go/1.9.1/libexec/src/runtime/hashmap_fast.go top5用于显示消耗 CPU 前五的函数，每一行代表一个函数，每一列为一项指标: flat: 采样时，该函数正在运行的次数*采样频率(10ms)，即得到估算的函数运行”采样时间”。这里不包括函数等待子函数返回。 flat%: flat / 总采样时间值 sum%: 前面所有行的 flat% 的累加值，如第二行 sum% = 20.82% = 11.12% + 9.70% cum: 采样时，该函数出现在调用堆栈的采样时间，包括函数等待子函数返回。因此 flat &lt;= cum cum%: cum / 总采样时间值 PS: 老的pprof版本貌似显示的是采样次数，比如 flat 为采样时该函数正在运行的次数，这个次数*采样频率即得到采样时间。 go tool pprof 常用命令: topN: 输入 top 命令，默认显示 flat 前10的函数调用，可使用 -cum 以 cum 排序 list Func: 显示函数名以及每行代码的采样分析 web: 生成 svg 热点图片，可在浏览器中打开，可使用 web Func 来过滤指定函数相关调用树 通过top5命令可以看到，mapaccess1_fast64函数占用的CPU 采样时间最多，通过 web mapaccess1_fast64 命令打开调用图谱，查看该函数调用关系，可以看到主要在DFS 和 FindLoops 中调用的，然后再通过 list DFS查看函数代码和关键调用，得到 map 结构是瓶颈点，尝试转换为 slice 优化，整个过程参考Profiling Go Programs。总的思路就是通过top 和web 找出关键函数，再通过list Func 查看函数代码，找到关键代码行并确认优化方案(辅以 go test Benchmark)。 2.2 memprofile▶ go build -o havlak3 havlak3.go ▶ ./havlak3 --memprofile=havlak3.mprof ▶ go tool pprof havlak3 havlak3.mprof File: havlak3 Type: inuse_space Time: Apr 3, 2018 at 3:44pm (CST) Entering interactive mode (type &quot;help&quot; for commands, &quot;o&quot; for options) (pprof) top Showing nodes accounting for 57.39MB, 100% of 57.39MB total flat flat% sum% cum cum% 39.60MB 69.00% 69.00% 39.60MB 69.00% main.FindLoops /Users/wudaijun/Code/goprof/havlak/havlak3.go 11.29MB 19.67% 88.67% 11.29MB 19.67% main.(*CFG).CreateNode /Users/wudaijun/Code/goprof/havlak/havlak3.go 6.50MB 11.33% 100% 17.79MB 31.00% main.NewBasicBlockEdge /Users/wudaijun/Code/goprof/havlak/havlak3.go 0 0% 100% 39.60MB 69.00% main.FindHavlakLoops /Users/wudaijun/Code/goprof/havlak/havlak3.go 0 0% 100% 17.79MB 31.00% main.buildBaseLoop /Users/wudaijun/Code/goprof/havlak/havlak3.go memprofile 也就是 heap 采样数据，go tool pprof 默认显示的是使用的内存的大小，如果想要显示使用的堆对象的个数，则通过go tool pprof --inuse_objects havlak3 havlak3.mprof，其它参数还有--alloc_objects和--alloc_space，分别是分配的堆内存大小和对象个数。在本例中，FindLoops 函数分配了39.60M 堆内存，占到69%，同样，接下来是通过list FindLoops对函数代码进行 review，找出关键数据结构，进行优化。 2.3 blockprofile12345678910111213141516171819202122232425262728293031323334353637383940414243var mutex sync.Mutexfunc main() &#123; // rate = 1 时, 统计所有的 block event, // rate &lt;=0 时，则关闭block profiling // rate &gt; 1 时，为 ns 数，阻塞时间t&gt;rate的event 一定会被统计，小于rate则有t/rate 的几率被统计 // 参考 https://github.com/golang/go/blob/release-branch.go1.9/src/runtime/mprof.go#L397 runtime.SetBlockProfileRate(1 * 1000 * 1000) var wg sync.WaitGroup wg.Add(1) mutex.Lock() go worker(&amp;wg) time.Sleep(2*time.Millisecond) mutex.Unlock() wg.Wait() writeProfTo(\"block\", \"block.bprof\")&#125;func worker(wg *sync.WaitGroup) &#123; defer wg.Done() mutex.Lock() time.Sleep(1*time.Millisecond) mutex.Unlock()&#125;func writeProfTo(name, fn string) &#123; p := pprof.Lookup(name) if p == nil &#123; fmt.Errorf(\"%s prof not found\", name) return &#125; f, err := os.Create(fn) if err != nil &#123; fmt.Errorf(\"%v\", err.Error()) return &#125; defer f.Close() err = p.WriteTo(f, 0) if err != nil &#123; fmt.Errorf(\"%v\", err.Error()) return &#125;&#125; 运行程序并 pprof: ▶ go build -o Temp tmp.go ▶ go tool pprof Temp block.bprof (pprof) top Showing nodes accounting for 3.37ms, 100% of 3.37ms total flat flat% sum% cum cum% 2.04ms 60.52% 60.52% 2.04ms 60.52% sync.(*Mutex).Lock /usr/local/Cellar/go/1.9.1/libexec/src/sync/mutex.go 1.33ms 39.48% 100% 1.33ms 39.48% sync.(*WaitGroup).Wait /usr/local/Cellar/go/1.9.1/libexec/src/sync/waitgroup.go 0 0% 100% 1.33ms 39.48% main.main /Users/wudaijun/go/src/ngs/test/tmp/tmp.go 0 0% 100% 2.04ms 60.52% main.worker /Users/wudaijun/go/src/ngs/test/tmp/tmp.go 0 0% 100% 3.37ms 100% runtime.goexit /usr/local/Cellar/go/1.9.1/libexec/src/runtime/asm_amd64.s 0 0% 100% 1.33ms 39.48% runtime.main /usr/local/Cellar/go/1.9.1/libexec/src/runtime/proc.go 可以看到程序在 mutex.Lock 上阻塞了2.04ms(worker goroutine)， 在 WaitGroup.Wait 上等待了1.33ms(main goroutine)，从更上层来看，在 main 函数中一共阻塞了2.04ms，worker函数中阻塞了1.33ms(cum 列)，通过 web命令生成 svg 图片在浏览器查看: 可以很直观地看到整个阻塞调用链，对于耗时较多的阻塞调用加以优化。 2.4 mutexprofile仍然用2.3中的代码，只需要改两个地方，将 runtime.SetBlockProfileRate(1 * 1000 * 1000) 改为: // 当 rate = 0 时，关闭 mutex prof (默认值) // 当 rate = 1 时，表示记录所有的 mutex event // 当 rate &gt; 1 时，记录 1/rate 的 mutex event(随机) runtime.SetMutexProfileFraction(1) 再将writeProfTo(&quot;block&quot;, &quot;block.bprof&quot;)改为writeProfTo(&quot;mutex&quot;, &quot;mutex.mprof&quot;)即可，编译运行，并打开 pprof 工具: ▶ go tool pprof bin/Temp mutex.mprof (pprof) top Showing nodes accounting for 2.55ms, 100% of 2.55ms total flat flat% sum% cum cum% 2.55ms 100% 100% 2.55ms 100% sync.(*Mutex).Unlock /usr/local/Cellar/go/1.9.1/libexec/src/sync/mutex.go 0 0% 100% 2.55ms 100% main.main /Users/wudaijun/go/src/ngs/test/tmp/tmp.go 0 0% 100% 2.55ms 100% runtime.goexit /usr/local/Cellar/go/1.9.1/libexec/src/runtime/asm_amd64.s 0 0% 100% 2.55ms 100% runtime.main /usr/local/Cellar/go/1.9.1/libexec/src/runtime/proc.go 查看 svg 图: 三. 实践 Tips以下是一些从其它项目借鉴或者自己总结的实践经验，它们只是建议，而不是准则，实际项目中应该以性能分析数据来作为优化的参考，避免过早优化。 对频繁分配的小对象，使用 sync.Pool 对象池避免分配 自动化的 DeepCopy 是非常耗时的，其中涉及到反射，内存分配，容器(如 map)扩展等，大概比手动拷贝慢一个数量级 用 atomic.Load/StoreXXX，atomic.Value, sync.Map 等代替 Mutex。(优先级递减) 使用高效的第三方库，如用fasthttp替代 net/http 在开发环境加上-race编译选项进行竞态检查 在开发环境开启 net/http/pprof，方便实时 pprof 将所有外部IO(网络IO，磁盘IO)做成异步 参考: Profiling Go Programs profiling-and-optimizing-go-web-applications","tags":[{"name":"go","slug":"go","permalink":"http://wudaijun.com/tags/go/"}]},{"title":"Docker 容器管理","date":"2018-03-24T16:00:00.000Z","path":"2018/03/docker-container-ops/","text":"一. 容器资源限制Docker资源限制主要靠Linux cgroups技术实现，简单说，cgroups是一个个的进程组(实际上是进程树)，这些进程树通过挂接 subsystem(事实上是挂接到 cgroup 上层的hierarchy)来实现对各种资源的限制和追踪，subsystem是内核附加在程序上的一系列钩子（hooks），通过程序运行时对资源的调度触发相应的钩子以达到资源追踪和限制的目的。cgroups 技术的具体介绍和实现参考文末链接。 1. CPU默认情况下，Docker容器对 CPU 资源的访问是无限制的，可使用如下参数控制容器的 CPU 访问: --cpus: 控制容器能够使用的最大 CPU 核数，参数为一个精度为两位小数的浮点数(默认值为0，即不限制 CPU)，不能超出物理机的 CPU 核数。 # 通过 stress 开启三个 worker 跑满 CPU 的 worker，并设置容器能访问的 cpus 为1.5 &gt; docker run --rm -it --cpus 1.5 progrium/stress --cpu 3 stress: info: [1] dispatching hogs: 3 cpu, 0 io, 0 vm, 0 hdd stress: dbug: [1] using backoff sleep of 9000us stress: dbug: [1] --&gt; hogcpu worker 3 [7] forked stress: dbug: [1] using backoff sleep of 6000us stress: dbug: [1] --&gt; hogcpu worker 2 [8] forked stress: dbug: [1] using backoff sleep of 3000us stress: dbug: [1] --&gt; hogcpu worker 1 [9] forked # 开启另一个窗口查看 CPU 占用情况 top # ... PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 4296 root 20 0 7316 100 0 R 51.8 0.0 0:07.04 stress 4294 root 20 0 7316 100 0 R 51.5 0.0 0:07.02 stress 4295 root 20 0 7316 100 0 R 46.5 0.0 0:06.42 stress 三个 worker 进程各自占用了50%的 CPU，共计150%，符合--cpus指定的1.5核约束。 --cpu-shares: 通过权重来控制同一物理机上的各容器的 CPU 占用，默认值为1024(该值应该是起源于 Linux2.6+中 CFS 调度算法的默认进程优先级)，它是一个软限制，仅在物理机 CPU 不够用时生效，当 CPU 够用时，容器总是尽可能多地占用 CPU。 # 开启8个 cpu worker 跑满所有核 默认 cpu-shares 为1024 &gt; docker run --rm -it progrium/stress --cpu 8 # 开新窗口查看 CPU 状态 &gt; top # ... PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 4477 root 20 0 7316 96 0 R 100.0 0.0 0:08.51 stress 4481 root 20 0 7316 96 0 R 100.0 0.0 0:08.52 stress 4474 root 20 0 7316 96 0 R 99.7 0.0 0:08.50 stress 4476 root 20 0 7316 96 0 R 99.7 0.0 0:08.50 stress 4478 root 20 0 7316 96 0 R 99.7 0.0 0:08.50 stress 4479 root 20 0 7316 96 0 R 99.7 0.0 0:08.50 stress 4480 root 20 0 7316 96 0 R 99.7 0.0 0:08.50 stress 4475 root 20 0 7316 96 0 R 99.3 0.0 0:08.48 stress # 再开8个 cpu worker，设置 cpu-shares 为 512 docker run --rm -it --cpu-shares 512 progrium/stress --cpu 8 # 再次查看 CPU 占用 &gt; top # ... PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 4815 root 20 0 7316 96 0 R 67.0 0.0 0:28.56 stress 4816 root 20 0 7316 96 0 R 67.0 0.0 0:28.30 stress 4820 root 20 0 7316 96 0 R 67.0 0.0 0:28.13 stress 4821 root 20 0 7316 96 0 R 67.0 0.0 0:28.31 stress 4817 root 20 0 7316 96 0 R 66.7 0.0 0:28.04 stress 4818 root 20 0 7316 96 0 R 66.7 0.0 0:28.42 stress 4819 root 20 0 7316 96 0 R 66.7 0.0 0:28.24 stress 4822 root 20 0 7316 96 0 R 66.7 0.0 0:28.38 stress 4961 root 20 0 7316 96 0 R 33.3 0.0 0:03.93 stress 4962 root 20 0 7316 96 0 R 33.3 0.0 0:03.96 stress 4965 root 20 0 7316 96 0 R 33.3 0.0 0:03.95 stress 4966 root 20 0 7316 96 0 R 33.3 0.0 0:04.02 stress 4968 root 20 0 7316 96 0 R 33.3 0.0 0:03.90 stress 4963 root 20 0 7316 96 0 R 33.0 0.0 0:04.01 stress 4964 root 20 0 7316 96 0 R 33.0 0.0 0:03.97 stress 4967 root 20 0 7316 96 0 R 33.0 0.0 0:03.94 stress 可以看到最开始的8个 worker CPU 占用由100%降到67%左右，而新启动的 worker CPU 占用为32%左右，大致满足2/3和1/3的权重占比。 除此之外，Docker还可以通过--cpuset-cpus参数限制容器运行在某些核上，但环境依赖太强(需要知道主机上有几个CPU核)，有违容器初衷，并且通常都不需要这样做。在 Docker1.13之后，还支持容器的实时调度配置(realtime scheduler)，就应用层而言，基本用不到这项配置，参考: https://docs.docker.com/config/containers/resource_constraints/#configure-the-realtime-scheduler。 2. 内存同 CPU 一样，默认情况下，Docker没有对容器内存进行限制。内存相关的几个概念: memory: 即容器可用的物理内存(RES)，包含 kernel-memory 和 user-memory，即内核内存和用户内存。kernel-memory: 内核内存，每个进程都会占用一部分内核内存，和user-memory 的最大区别是不能被换入换出，因此进程的内核内存占用过大可能导致阻塞系统服务。swap: 容器可用的交换区大小，会swap+memory限制着进程最大能够分配的虚拟页，也是进程理论上能够使用的最大”内存”(虚拟内存)。 以下大部分配置的参数为正数，加上内存单位，如”4m”, “128k”。 -m or --memory: 容器可以使用的最大内存限制，最小为4m --memory-swap: 容器使用的内存和交换区的总大小 --memory-swappiness: 默认情况下，主机可以把容器使用的匿名页(anonymous page) swap 出来，这个参数可以配置可被swap的比例(0-100) --memory-reservation: 内存软限制，每次系统内存回收时，都会尝试将进程的内存占用降到该限制以下(尽可能换出)。该参数的主要作用是避免容器长时间占用大量内存。 --kernel-memory: 内核内存的大小 --memory-swappiness: 设置容器可被置换的匿名页的百分比，值为[0,100]，为0则关闭匿名页交换，容器的工作集都在内存中活跃，默认值从父进程继承 --oom-kill-disable: 当发生内存不够用(OOM) 时，内核默认会向容器中的进程发送 kill 信号，添加该参数将避免发送 kill 信号。该参数一般与-m 一起使用，因为如果没有限制内存，而又启用了 oom-kill-disable，OS 将尝试 kill 其它系统进程。(PS: 该参数我在 Ubuntu 16.04 LTS/Docker17.09.0-ce环境下，没有测试成功，仍然会直接 kill) --oom-score-adj: 当发生 OOM 时，进程被 kill 掉的优先级，取值[-1000,1000]，值越大，越可能被 kill 掉 --memory和--memory-swap: 1. 当 memory-swap &gt; memory &gt; 0: 此时容器可使用的 swap 大小为: swap = memory-swap - memory 2. memory-swap == 0 或 &lt; memory: 相当于没有设置(如果&lt; memory, docker 会错误提示)，使用默认值，此时容器可使用的 swap 大小为: swap == memory，即 memory-swap = = 2*memory 3. memory-swap == memory &gt; 0: 容器不能使用交换空间: swap = memory-swap - memory = 0 4. memory-swap == -1: 容器可使用主机上所有可用的 swap 空间，即无限制 在配置--memory-swap 参数时，可能遇到如下提示: WARNING: Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap. 解决方案为: To enable memory and swap on system using GNU GRUB (GNU GRand Unified Bootloader), do the following: 1. Log into Ubuntu as a user with sudo privileges. 2. Edit the /etc/default/grub file. 3. Set the GRUB_CMDLINE_LINUX value as follows: GRUB_CMDLINE_LINUX=&quot;cgroup_enable=memory swapaccount=1&quot; 4. Save and close the file. 5. Update GRUB. $ sudo update-grub Reboot your system. 示例: 我们通过一个 带有 stress 命令的 ubuntu 镜像来进行测试: &gt; cat Dockerfile FROM ubuntu:latest RUN apt-get update &amp;&amp; \\ apt-get install stress &gt; docker build -t ubuntu-stress:latest . # 示例一: # memory 限制为100M，swap 空间无限制，分配1000M 内存 &gt; docker run -it --rm -m 100M --memory-swap -1 ubuntu-stress:latest /bin/bash root@e618f1fc6ff9:/# stress --vm 1 --vm-bytes 1000M # docker stats 查看容器内存占用，此时容器物理内存已经达到100M 限制 &gt; docker stats e618f1fc6ff9 CONTAINER CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS e618f1fc6ff9 15.62% 98.25MiB / 100MiB 98.25% 3.39kB / 0B 22GB / 22.4GB 3 &gt; pgrep stress 27158 27159 # stress worker 子进程 PID # 通过 top 可以看到进程物理内存占用为100M，虚拟内存占用为1000M &gt; top -p 27159 top - 19:30:08 up 31 days, 1:55, 3 users, load average: 1.63, 1.43, 1.03 Tasks: 1 total, 0 running, 1 sleeping, 0 stopped, 0 zombie %Cpu(s): 1.8 us, 4.3 sy, 2.1 ni, 81.2 id, 10.3 wa, 0.0 hi, 0.3 si, 0.0 st KiB Mem : 16361616 total, 840852 free, 3206616 used, 12314148 buff/cache KiB Swap: 16705532 total, 15459856 free, 1245676 used. 12681868 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 27159 root 20 0 1031484 98844 212 D 14.3 0.6 0:53.11 stress # 示例二: # memory 限制为100M，swap 比例为 50% &gt; docker run -it --rm -m 100M --memory-swappiness 50 ubuntu-stress:latest /bin/bash root@e3fdd8b75f1d:/# stress --vm 1 --vm-bytes 190M # 分配190M 内存 # 190M 内存正常分配，因为190M*50%的页面可以被 swap，剩下50%的页面放在内存中 &gt; top -p 29655 # ... PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 29655 root 20 0 202044 98296 212 D 9.7 0.6 0:17.52 stress # 停止 stress，重新尝试分配210M 内存，210M*50%&gt;100M，内存不够，进程被 kill 掉 &gt; root@e3fdd8b75f1d:/# stress --vm 1 --vm-bytes 210M stress: info: [13] dispatching hogs: 0 cpu, 0 io, 1 vm, 0 hdd stress: FAIL: [13] (415) &lt;-- worker 14 got signal 9 stress: WARN: [13] (417) now reaping child worker processes stress: FAIL: [13] (451) failed run completed in 4s # 示例三: # memory 限制为100M, swap 比例为60%, memory-swap 为130M # 可以得到，容器能使用的最大虚拟内存为 min(100/(1-60%), 130) = 130M，现在来简单验证 docker run -it --rm -m 100M --memory-swappiness 50 --memory-swap 30M ubuntu-stress:latest /bin/bash # 分配120M 内存, OK root@b54444b40706:/# stress --vm 1 --vm-bytes 120M stress: info: [11] dispatching hogs: 0 cpu, 0 io, 1 vm, 0 hdd ^C # 分配140M 内存，Error root@b54444b40706:/# stress --vm 1 --vm-bytes 140M stress: info: [13] dispatching hogs: 0 cpu, 0 io, 1 vm, 0 hdd stress: FAIL: [13] (415) &lt;-- worker 14 got signal 9 stress: WARN: [13] (417) now reaping child worker processes stress: FAIL: [13] (451) failed run completed in 1s 二. 容器监控1. docker inspectdocker inspect用于查看容器的静态配置，容器几乎所有的配置信息都在里面: &gt; docker inspect 5c004516ee59 0e9300806926 [ { &quot;Id&quot;: &quot;5c004516ee592b53e3e83cdee69fc93713471d4ce06778e1c6a9f783a576531b&quot;, &quot;Created&quot;: &quot;2018-03-27T16:44:27.521434182Z&quot;, &quot;Path&quot;: &quot;game&quot;, &quot;Args&quot;: [], &quot;State&quot;: { &quot;Status&quot;: &quot;running&quot;, &quot;Running&quot;: true, ... docker inspect接收一个容器 ID 列表，返回一个 json 数组，包含容器的各项参数，可以通过 docker format 过滤输出: # 显示容器 IP &gt; docker inspect --format &#39;{{ .NetworkSettings.IPAddress }}&#39; 5c004516ee59 172.17.0.2 2. docker statsdocker stats可实时地显示容器的资源使用(内存, CPU, 网络等): # 查看指定容器 &gt; docker stats ngs-game-1 CONTAINER CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS ngs-game-1 0.55% 127.4MiB / 15.6GiB 0.80% 0B / 0B 0B / 0B 18 # 以容器名代替容器ID查看所有运行中的容器状态 &gt; docker stats $(docker ps --format={{.Names}}) CONTAINER CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS ngs-game-1 0.74% 127.4MiB / 15.6GiB 0.80% 0B / 0B 0B / 0B 18 ngs-game-4 0.54% 21.99MiB / 15.6GiB 0.14% 0B / 0B 0B / 0B 20 ngs-auth-1 0.01% 11.11MiB / 15.6GiB 0.07% 0B / 0B 0B / 0B 20 3. docker attach将本地的标准输入/输出以及错误输出 attach 到运行中的container 上。 &gt; docker run -d -it --name ubuntu1 ubuntu-stress /bin/bash da01f119000f7370780eea0220a0fbf6e7b6d8d0dac1d635fc5dd480a64e4f68 &gt; docker attach ubuntu1 root@da01f119000f:/# # 开启另一个 terminal，再次 attach，此时两个 terminal 的输入输出会自动同步 &gt; docker attach ubuntu1 由于本地输入完全重定向到容器，因此输入 exit 或CTRL-d会退出容器，要 dettach 会话，输入CTRL-p CTRL-q。 三. 容器停止 docker stop: 分为两个阶段，第一个阶段向容器主进程(Pid==1)发送SIGTERM信号，容器主进程可以捕获这个信号并进入退出处理流程，以便优雅地停止容器。第一阶段是有时间限制的(通过-t参数指明，默认为10s)，如果超过这个时间容器仍然没有停止，则进入第二阶段: 向容器主进程发送SIGKILL信号强行终止容器(SIGKILL无法被忽略或捕获)。 docker kill: 不带参数则相当于直接进入docker stop的第二阶段，可通过-s参数指定要发送的信号(默认是SIGKILL)。 docker stop/kill仅向容器主进程(Pid==1)发送信号，因此对于ENTRYPOINT/CMD的Shell格式来说，可能导致应用无法接收的信号，Docker命令文档也提到了这一点: Note: ENTRYPOINT and CMD in the shell form run as a subcommand of /bin/sh -c, which does not pass signals. This means that the executable is not the container’s PID 1 and does not receive Unix signals. 参考: https://docs.docker.com/config/containers/resource_constraints/ https://docs.docker.com/engine/reference/run/#runtime-constraints-on-resources DOCKER基础技术：LINUX CGROUP Docker背后的内核知识——cgroups资源限制","tags":[{"name":"docker","slug":"docker","permalink":"http://wudaijun.com/tags/docker/"}]},{"title":"Docker容器编排工具","date":"2018-03-07T16:00:00.000Z","path":"2018/03/docker-management/","text":"一. Docker Machine通常我们使用的Docker都是直接在物理机上安装Docker Engine，docker-machine是一个在虚拟机上安装Docker Engine的工具，使用起来很方便: # 创建一个docker machine，命名为abc &gt; docker-machine create abc # 列出当前主机上所有的docker machine &gt; docker-machine ls # 通过ssh连接到abc &gt; docker-machine ssh abc # 现在就已经在abc machine上，可以像使用Docker Engine一样正常使用 docker@abc:~$ docker ps # 退出machine docker@abc:~$ exit docker-machine可以用来在本机部署Docker集群，或者在云上部署Docker。docker-machine支持多种虚拟方案，virtualbox，xhyve，hyperv等等。具体使用比较简单，命令参考附录文档。 二. Docker SwarmDocker Swarm是docker原生的集群管理工具，之前是个独立的项目，于 Docker 1.12 被整合到 Docker Engine 中,作为swarm model存在，因此Docker Swarm实际上有两种：独立的swarm和整合后swarm model。官方显然推荐后者，本文也使用swarm model。相较于kubernetes，Mesos等工具，swarm最大的优势是轻量，原生和易于配置。它使得原本单主机的应用可以方便地部署到集群中。 相关术语 task: 任务，集群的最小单位，对应单容器实例 service: 服务，由一个或多个task构成，可以统一配置，部署，收缩 node: 机器节点，代表一台物理机 相关命令 docker service: 提供了service创建，更新，回滚，task扩展收缩等功能 docker node: 提供对机器节点的管理 docker swarm: 用于配置机器集群，包括管理manager和worker两类机器节点的增删 1. 初始化 swarm[n1-common]&gt; docker swarm init Swarm initialized: current node (b3a3avned864im04d7veyw06t) is now a manager. To add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-4mptgs751hcyh3ddlqwvv2aumo5j5mu1qllva52ciim6bun51d-eausald3qqtae604doj639mck 192.168.65.2:2377 To add a manager to this swarm, run &#39;docker swarm join-token manager&#39; and follow the instructions. 执行该条命令的node将会成为manager node，该命令会生成两个token: manager token和worker token，通过docker swarm join --token TOKEN MANAGER_NODE_IP提供不同的token来将当前node以不同身份加入到集群。 现在我们尝试加入一个worker node，在另一台机器上执行: [moby]&gt; docker swarm join --token SWMTKN-1-2w53lkm9h1l5u6yb4hh0k2t8yayub2zx0sidpvcr9nicqwafzx-9jm5zix2041rhfrf7e07oh4l2 172.20.140.39:2377 This node joined a swarm as a worker. 2. 配置节点通过 docker node ls 可以查看当前swarm集群中的所有节点(只能在manager节点上运行): [n1-common]&gt; sudo docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS yozazaogirhpj8skccfwqtl8f moby Ready Active rx03hnmwx6z9jc9x9velz46if * n1-common Ready Active Leader PS: swarm的service和node管理命令的规范和container管理类似: docker node|service ls: 查看集群中所有的节点(服务) docker node|service ps: 查看指定节点(服务)的容器信息 docker node|service inspect: 查看指定节点(服务)的内部配置和状态信息 docker node|service update: 更新节点(服务)配置信息 docker node|service rm: 从集群中移除节点(服务) 以上命令都只能在manager节点上运行。 在这里，我们通过docker node update为节点设置标签: n1-common:~$ docker node update --label-add type=db moby moby 3. 创建服务服务有两种模式(mode): 复制集模式(—mode replicas): 默认模式，该方式会将指定的(通过—replicas) M个task按照指定方式部署在N个机器节点上(N &lt;= 集群机器节点数)。 全局模式(—mode global): 将服务在每个机器节点上部署一份，因此无需指定任务数量，也不能进行任务扩展和收缩。 我们尝试创建一个名为redis的服务，该服务包含5个任务的复制集: [n1-common]&gt; docker service create \\ --replicas 5 \\ --name redis \\ --constraint &#39;node.labels.type=db&#39; \\ --update-delay 10s \\ --update-parallelism 2 \\ --env MYVAR=foo \\ -p 6379:6379 \\ redis --update-xxx指定了服务更新策略，这里为redis服务指定最多同时更新2个task，并且每批次更新之间间隔10s，在更新失败时，执行回滚操作，回滚到更新前的配置。更新操作通过docker service update命令完成，可以更新docker service create中指定的几乎所有配置，如task数量。docker service create除了更新策略外，还可以为service指定回滚策略(--rollback-xxx)，重启策略(--restart-xxx)等。 --constraint指定服务约束，限制服务的任务能够部署的节点，在这里，redis服务的5个任务只能部署在集群中labels.type==db的节点上。除了constraint参数外，还可以通过--placement-pref更进一步地配置部署优先级，如--placement-pref &#39;spread=node.labels.type&#39;将task平均分配到不同的type上，哪怕各个type的node数量不一致。 --env MYVAR=foo指定服务环境变量，当然，这里并没有实际意义。 关于服务创建的更多选项参考官方文档。运行以上命令后，服务默认将在后台创建(—detach=false)，通过docker service ps redis可查看服务状态，确保服务的任务都以正常启动: [n1-common]&gt; docker service ps redis ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS fegu7p341u58 redis.1 redis:latest moby Running Running 9 seconds ago hoghsnnamv56 redis.2 redis:latest moby Running Running 9 seconds ago 0klozd8zkz0d redis.3 redis:latest moby Running Running 10 seconds ago jpcik7w3hpjx redis.4 redis:latest moby Running Running 10 seconds ago 29jrofbwfi13 redis.5 redis:latest moby Running Running 8 seconds ago 可以看到，由于只有moby节点的labels.type==db，因此所有的task都被部署在moby节点上。现在整个服务已经部署完成，那么如何访问这个服务呢？事实上，我们通过moby或者n1-common两台主机IP:6379均可访问Redis服务，Swarm向用户屏蔽了服务的具体部署位置，让用户使用集群就像使用单主机一样，这也为部署策略，负载均衡以及故障转移提供基础。 4. 平滑更新通过docker service update可以完成对服务的更新，可更新的配置很多，包括docker service create中指定的参数，自定义标签等，服务的更新策略由--update-xxx选项配置，只有部分更新需要重启任务，可通过--force参数强制更新。 现在我们尝试限制redis服务能够使用的cpu个数: [n1-common]&gt; docker service update --limit-cpu 2 redis redis Since --detach=false was not specified, tasks will be updated in the background. In a future release, --detach=false will become the default. [n1-common]&gt; docker service ps redis ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS fegu7p341u58 redis.1 redis:latest moby Running Running 13 minutes ago hoghsnnamv56 redis.2 redis:latest moby Running Running 13 minutes ago mgblj8v97al1 redis.3 redis:latest moby Running Running 9 seconds ago 0klozd8zkz0d \\_ redis.3 redis:latest moby Shutdown Shutdown 11 seconds ago jpcik7w3hpjx redis.4 redis:latest moby Running Running 13 minutes ago 49mvisd0zbtj redis.5 redis:latest moby Running Running 8 seconds ago 29jrofbwfi13 \\_ redis.5 redis:latest moby Shutdown Shutdown 11 seconds ago [n1-common]&gt; docker service ps redis ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS 9396e3x8gp5m redis.1 redis:latest moby Ready Ready 2 seconds ago fegu7p341u58 \\_ redis.1 redis:latest moby Shutdown Running 2 seconds ago msugiubez60a redis.2 redis:latest moby Ready Ready 2 seconds ago hoghsnnamv56 \\_ redis.2 redis:latest moby Shutdown Running 2 seconds ago mgblj8v97al1 redis.3 redis:latest moby Running Running 13 seconds ago 0klozd8zkz0d \\_ redis.3 redis:latest moby Shutdown Shutdown 15 seconds ago jpcik7w3hpjx redis.4 redis:latest moby Running Running 13 minutes ago 49mvisd0zbtj redis.5 redis:latest moby Running Running 12 seconds ago 29jrofbwfi13 \\_ redis.5 redis:latest moby Shutdown Shutdown 15 seconds ago 由于限制服务所使用的CPU数量需要重启任务，通过前后两次的docker service ps可以看到，docker service的更新策略与我们在docker service create中指定的一致: 每两个一组，每组间隔10s，直至更新完成，通过指定--detach=false能同步地看到这个平滑更新过程。这种平滑更新重启使得服务在升级过程中，仍然能够正常对外提供服务。docker swarm会保存每个任务的升级历史及对应的容器ID和容器状态，以便在更新失败时正确回滚(如果指定了更新失败的行为为回滚)，docker service rollback命令可强制将任务回滚到上一个版本。 现在我们通过docker service scale来伸缩服务任务数量，在这里我们使用--detach=false选项: [n1-common]&gt; docker service scale redis=3 redis scaled to 3 overall progress: 3 out of 3 tasks 1/3: running [==================================================&gt;] 2/3: running [==================================================&gt;] 3/3: running [==================================================&gt;] verify: Service converged [n1-common]&gt; docker service ps redis ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS 9396e3x8gp5m redis.1 redis:latest moby Running Running 10 minutes ago fegu7p341u58 \\_ redis.1 redis:latest moby Shutdown Shutdown 10 minutes ago 8urov9089x6c redis.4 redis:latest moby Running Running 9 minutes ago jpcik7w3hpjx \\_ redis.4 redis:latest moby Shutdown Shutdown 9 minutes ago 49mvisd0zbtj redis.5 redis:latest moby Running Running 10 minutes ago 29jrofbwfi13 \\_ redis.5 redis:latest moby Shutdown Shutdown 10 minutes ago 服务的任务规模被收缩，现在只剩下redis.1,redis.4,redis.5三个任务。 5. 故障转移现在我们将redis服务停掉，重新创建一个redis服务: [n1-common]&gt; docker service rm redis redis [n1-common]&gt; docker service create --replicas 5 --name redis -p 6379:6379 redis fvcwpsmbscxhsmg04vf5zhmbf Since --detach=false was not specified, tasks will be created in the background. In a future release, --detach=false will become the default. [n1-common]&gt; docker service ps redis ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS n1dd790efq36 redis.1 redis:latest moby Running Running 2 minutes ago 5fvqbozb7bpr redis.2 redis:latest n1-common Running Running 2 minutes ago ma533n5ce09c redis.3 redis:latest moby Running Running 2 minutes ago j1f18j2yaqhc redis.4 redis:latest n1-common Running Running 2 minutes ago p2kf7ftrexam redis.5 redis:latest moby Running Running 2 minutes ago 由于我们没有指定部署约束，因此redis服务的5个任务将被自动负载到集群节点中，在这里，redis.2,redis.4部署在n1-common上，其余三个部署在moby，现在我们将moby节点退出集群，观察服务任务状态变化: [moby]&gt; docker swarm leave Node left the swarm. [n1-common]&gt; service ps redis ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS 8c5py5p9pcgz redis.1 redis:latest n1-common Ready Accepted less than a second ago n1dd790efq36 \\_ redis.1 redis:latest moby Shutdown Running 12 seconds ago 5fvqbozb7bpr redis.2 redis:latest n1-common Running Running 8 minutes ago ml546ziyey4r redis.3 redis:latest n1-common Ready Accepted less than a second ago ma533n5ce09c \\_ redis.3 redis:latest moby Shutdown Running 8 minutes ago j1f18j2yaqhc redis.4 redis:latest n1-common Running Running 8 minutes ago kfu6jeddkvwu redis.5 redis:latest n1-common Ready Accepted less than a second ago p2kf7ftrexam \\_ redis.5 redis:latest moby Shutdown Running 12 seconds ago [n1-common]&gt; docker service ps redis ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS 8c5py5p9pcgz redis.1 redis:latest n1-common Running Running 3 seconds ago n1dd790efq36 \\_ redis.1 redis:latest moby Shutdown Running 23 seconds ago 5fvqbozb7bpr redis.2 redis:latest n1-common Running Running 8 minutes ago ml546ziyey4r redis.3 redis:latest n1-common Running Running 3 seconds ago ma533n5ce09c \\_ redis.3 redis:latest moby Shutdown Running 8 minutes ago j1f18j2yaqhc redis.4 redis:latest n1-common Running Running 8 minutes ago kfu6jeddkvwu redis.5 redis:latest n1-common Running Running 3 seconds ago p2kf7ftrexam \\_ redis.5 redis:latest moby Shutdown Running 23 seconds ago 故障节点moby上面的1,3,5任务已经被自动重新部署在其它可用节点(当前只有n1-common)上，并记录了每个任务的版本和迁移历史。现在如果尝试再将moby节点加入集群，会发现5个task仍然都在n1-common上，没有立即进行任务转移，而是等下一步重启升级或者扩展服务任务时再进行动态负载均衡。 6. 再看Swarm集群再来回顾一下Docker Swarm，在我们初始化或加入Swarm集群时，通过docker network ls可以看到，Docker做了如下事情: 创建了一个叫ingress的overlay网络，用于Swarm集群容器跨主机通信，在创建服务时，如果没有为其指定网络，将默认接入到ingress网络中 创建一个docker_gwbridge虚拟网桥，用于连接集群各节点(Docker Deamon)的物理网络到到ingress网络 网络细节暂时不谈(也没怎么搞清楚)，总之，Swarm集群构建了一个跨主机的网络，可以允许集群中多个容器自由访问。Swarm集群有如下几个比较重要的特性: 服务的多个任务可以监听同一端口(通过iptables透明转发)。 屏蔽掉服务的具体物理位置，通过任意集群节点IP:Port均能访问服务(无论这个服务是否跑在这个节点上)，Docker会将请求正确路由到运行服务的节点(称为routing mesh)。在routine mesh下，服务运行在虚拟IP环境(virtual IP mode, vip)，即使服务运行在global模式(每个节点都运行有任务)，用户仍然不能假设指定IP:Port节点上的服务会处理请求。 如果不想用Docker Swarm自带的routing mesh负载均衡器，可以在服务创建或更新时使用--endpoint-mode = dnsrr，dnsrr为dns round robin简写，另一种模式即为vip，dnsrr允许应用向Docker通过服务名得到服务IP:Port列表，然后应用负责从其中选择一个地址进行服务访问。 综上，Swarm通过虚拟网桥和NATP等技术，搭建了一个跨主机的虚拟网络，通过Swarm Manager让这个跨主机网络用起来像单主机一样方便，并且集成了服务发现(服务名-&gt;服务地址)，负载均衡(routing mesh)，这些都是Swarm能够透明协调转移任务的根本保障，应用不再关心服务有几个任务，部署在何处，只需要知道服务在这个集群中，端口是多少，然后这个服务就可以动态的扩展，收缩和容灾。当然，Swarm中的服务是理想状态的微服务，亦即是无状态的。 三. Docker Compose &amp; Stackdocker-compose 是一个用于定义和运行多容器应用的工具。使用compose，你可以通过一份docker-compose.yml配置文件，然后运行docker-compose up即可启动整个应用所配置的服务。一个docker-compose.yml文件定义如下: version: &#39;3&#39; # docker-compose.yml格式版本号，版本3为官方推荐版本，支持swarm model和deploy选项 services: # 定义引用所需服务 web: # 服务名字 build: . # 服务基于当前目录的Dockerfile构建 ports: # 服务导出端口配置 - &quot;5000:5000&quot; volumes: # 服务目录挂载配置 - .:/code - logvolume01:/var/log links: # 网络链接 - redis deploy: # 部署配置 和 docker service create中的参数对应 只有版本&gt;3支持 replicas: 5 resources: limits: cpus: &quot;0.1&quot; memory: 50M restart_policy: condition: on-failure redis: # redis 服务 image: redis # 服务基于镜像构建 docker-compose设计之初是单机的，docker-compose中也有服务的概念，但只是相当于一个或多个容器(version&gt;2.2 scale参数)，并且只能部署在单台主机上。版本3的docker-compose.yml开始支持swarm model，可以进行集群部署配置，这里的服务才是swarm model中的服务。但version 3的docker-compose.yml本身已经不能算是docker-compose的配置文件了，因为docker-compose不支持swarm model，用以上配置文件执行docker-compose up将得到警告: WARNING: Some services (web) use the &#39;deploy&#39; key, which will be ignored. Compose does not support &#39;deploy&#39; configuration - use `docker stack deploy` to deploy to a swarm. WARNING: The Docker Engine you&#39;re using is running in swarm mode. Compose does not use swarm mode to deploy services to multiple nodes in a swarm. All containers will be scheduled on the current node. 那么docker stack又是什么？docker stack是基于docker swarm之上的应用构建工具，前面介绍的docker swarm只能以服务为方式构建，而docker-compose虽然能以应用为单位构建，但本身是单机版的，Docker本身并没有基于docker-compose进行改造，而是另起炉灶，创建了docker stack命令，同时又复用了docker-compose.yml配置方案(同时也支持另一种bundle file配置方案)，因此就造成了docker-compose能使用compose配置的version 1, version 2,和部分version 3(不支持swarm model和deploy选项)，而docker stack仅支持version 3的compose配置。 总的来说，如果应用是单机版的，或者说不打算使用docker swarm集群功能，那么就通过docker-compose管理应用构建，否则使用docker stack，毕竟后者才是亲生的。 参考: Docker Machine Docker Swarm Docker Compose Docker Services Docker overlay网络","tags":[{"name":"docker","slug":"docker","permalink":"http://wudaijun.com/tags/docker/"}]},{"title":"Go sync.Map 实现","date":"2018-02-08T16:00:00.000Z","path":"2018/02/go-sync-map-implement/","text":"Go基于CSP模型，提倡”Share memory by communicating; don’t communicate by sharing memory.”，亦即通过channel来实现goroutine之间的数据共享，但很多时候用锁仍然是不可避免的，它可以让流程更直观明了，并且减少内存占用等。通常我们的实践是用channel传递数据的所有权，分配工作和同步异步结果等，而用锁来共享状态和配置等信息。 本文从偏实现的角度学习下Go的atomic.Load/Store，atomic.Value，以及sync.Map。 1. atomic.Load/Store在Go中，对于一个字以内的简单类型(如整数，指针)，可以直接通过atomic.Load/Store/Add/Swap/CompareAndSwap系列API来进行原子读写，以Int32为例: 1234567891011 // AddInt32 atomically adds delta to *addr and returns the new value.func AddInt32(addr *int32, delta int32) (new int32)// LoadInt32 atomically loads *addr.func LoadInt32(addr *int32) (val int32)// StoreInt32 atomically stores val into *addr.func StoreInt32(addr *int32, val int32)// SwapInt32 atomically stores new into *addr and returns the previous *addr value.func SwapInt32(addr *int32, new int32) (old int32)// CompareAndSwapInt32 executes the compare-and-swap operation for an int32 value.func CompareAndSwapInt32(addr *int32, old, new int32) (swapped bool) 一个有意思的问题，在64位平台下，对Int32，Int64的直接读写是原子的吗？以下是一些有意思的讨论: http://preshing.com/20130618/atomic-vs-non-atomic-operations/ https://stackoverflow.com/questions/46556857/is-golang-atomic-loaduint32-necessary https://stackoverflow.com/questions/5258627/atomic-64-bit-writes-with-gcc 总结就是，现代硬件架构基本都保证了内存对齐的word-sized load和store是原子的，这隐含两个条件: 单条MOV, MOVQ等指令是原子的，字段内存对齐(CPU对内存的读取是基于word-size的)。但安全起见，最好还是使用atomic提供的接口，具备更好的跨平台性，并且atomic还提供了一些复合操作(Add/Swap/CAS)。golang也在实现上会对具体平台进行优化： 1234var i int64atomic.StoreInt64(&amp;i, 123)x := atomic.LoadInt64(&amp;i)y := atomic.AddInt64(&amp;i, 1) 在MacOS10.12(X86_64)下，对应汇编代码:123456789101112131415161718192021// var i int64tmp.go:9 0x1093bff 488d051af50000 LEAQ 0xf51a(IP), AX // 加载int64 typetmp.go:9 0x1093c06 48890424 MOVQ AX, 0(SP)tmp.go:9 0x1093c0a e8c1a1f7ff CALL runtime.newobject(SB) // i分配在堆上(逃逸分析,escape analytic))tmp.go:9 0x1093c0f 488b442408 MOVQ 0x8(SP), AXtmp.go:9 0x1093c14 4889442450 MOVQ AX, 0x50(SP) // 0x50(SP) = &amp;itmp.go:9 0x1093c19 48c70000000000 MOVQ $0x0, 0(AX) // 初始化 i = 0// atomic.StoreInt64(&amp;i, 123)tmp.go:10 0x1093c20 488b442450 MOVQ 0x50(SP), AX // 加载&amp;itmp.go:10 0x1093c25 48c7c17b000000 MOVQ $0x7b, CX // 加载立即数 123tmp.go:10 0x1093c2c 488708 XCHGQ CX, 0(AX) // *(&amp;i) = 123 Key Step XCHGQ通过LOCK信号锁住内存总线来确保原子性// x := atomic.LoadInt64(&amp;i)tmp.go:11 0x1093c2f 488b442450 MOVQ 0x50(SP), AXtmp.go:11 0x1093c34 488b00 MOVQ 0(AX), AX // AX = *(&amp;i) Key Step 原子操作tmp.go:11 0x1093c37 4889442430 MOVQ AX, 0x30(SP)// y := atomic.AddInt64(&amp;i, 1)tmp.go:12 0x1093c3c 488b442450 MOVQ 0x50(SP), AXtmp.go:12 0x1093c41 48c7c101000000 MOVQ $0x1, CXtmp.go:12 0x1093c48 f0480fc108 LOCK XADDQ CX, 0(AX) // LOCK会锁住内存总线，直到XADDQ指令完成，完成后CX为i的旧值 0(AX)=*(&amp;i)=i+1tmp.go:12 0x1093c4d 488d4101 LEAQ 0x1(CX), AX // AX = CX+1 再执行一次加法 用于返回值tmp.go:12 0x1093c51 4889442428 MOVQ AX, 0x28(SP) 对XCHG和XADD这类X开头的指令，都会通过LOCK信号锁住内存总线，因此加不加LOCK前缀都是一样的。可以看到，由于硬件架构的支持，atomic.Load/Store和普通读写基本没有什么区别，这种CPU指令级别的锁非常快。因此通常我们将这类CPU指令级别的支持的Lock操作称为原子操作或无锁操作。 2. atomic.Valueatomic.Value于go1.4引入，用于无锁存取任意值(interface{})，它的数据结构很简单: 12345678// sync/atomic/value.gotype Value struct &#123; // 没有实际意义 用于保证结构体在第一次被使用之后，不能被拷贝 // 参考: https://github.com/golang/go/issues/8005#issuecomment-190753527 noCopy noCopy // 实际保存的值 v interface&#123;&#125;&#125; atomic负责v的原子存取操作，我们知道interface{}对应的数据结构为eface，有两个字段: type和data，因此它不能直接通过atomic.Load/Store来存取，atomic.Value实现无锁存取的原理很简单: type字段不变，只允许更改data字段，这样就能通过atomic.LoadPointer来实现对data的存取。从实现来讲，atomic.Value要处理好两点: atomic.Value的初始化，因为在初始化时，需要同时初始化type和data字段，atomic.Value通过CAS自旋锁来实现初始化的原子性。 atomic.Value的拷贝，一是拷贝过程的原子性，二是拷贝方式，浅拷贝会带来更多的并发问题，深拷贝得到两个独立的atomic.Value是没有意义的，因此atomic.Value在初始化完成之后是不能拷贝的。 除此之外，atomic.Value的实现比较简单，结合eface和atomic.LoadPointer()即可理解，不再详述。 3. sync.Mapsync.Map于go1.9引入，为并发map提供一个高效的解决方案。在此之前，通常是通过sync.RWMutex来实现线程安全的Map，后面会有mutexMap和sync.Map的性能对比。先来看看sync.Map的特性: 以空间换效率，通过read和dirty两个map来提高读取效率 优先从read map中读取(无锁)，否则再从dirty map中读取(加锁) 动态调整，当misses次数过多时，将dirty map提升为read map 延迟删除，删除只是为value打一个标记，在dirty map提升时才执行真正的删除 sync.Map的使用很简单: 123var m sync.Mapm.Store(\"key\", 123)v, ok := m.Load(\"key\") 下面看一下sync.Map的定义以及Load, Store, Delete三个方法的实现。 3.1 定义123456789101112131415161718192021222324252627282930313233// sync/map.gotype Map struct &#123; // 当写read map 或读写dirty map时 需要上锁 mu Mutex // read map的 k v(entry) 是不变的，删除只是打标记，插入新key会加锁写到dirty中 // 因此对read map的读取无需加锁 read atomic.Value // 保存readOnly结构体 // dirty map 对dirty map的操作需要持有mu锁 dirty map[interface&#123;&#125;]*entry // 当Load操作在read map中未找到，尝试从dirty中进行加载时(不管是否存在)，misses+1 // 当misses达到diry map len时，dirty被提升为read 并且重新分配dirty misses int&#125;// read map数据结构type readOnly struct &#123; m map[interface&#123;&#125;]*entry // 为true时代表dirty map中含有m中没有的元素 amended bool&#125;type entry struct &#123; // 指向实际的interface&#123;&#125; // p有三种状态: // p == nil: 键值已经被删除，此时，m.dirty==nil 或 m.dirty[k]指向该entry // p == expunged: 键值已经被删除， 此时, m.dirty!=nil 且 m.dirty不存在该键值 // 其它情况代表实际interface&#123;&#125;地址 如果m.dirty!=nil 则 m.read[key] 和 m.dirty[key] 指向同一个entry // 当删除key时，并不实际删除，先CAS entry.p为nil 等到每次dirty map创建时(dirty提升后的第一次新建Key)，会将entry.p由nil CAS为expunged p unsafe.Pointer // *interface&#123;&#125;&#125; 定义很简单，补充以下几点: read和dirty通过entry包装value，这样使得value的变化和map的变化隔离，前者可以用atomic无锁完成 Map的read字段结构体定义为readOnly，这只是针对map[interface{}]*entry而言的，entry内的内容以及amended字段都是可以变的 大部分情况下，对已有key的删除(entry.p置为nil)和更新可以直接通过修改entry.p来完成 3.2 Load1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// 查找对应的Key值 如果不存在 返回nil，falsefunc (m *Map) Load(key interface&#123;&#125;) (value interface&#123;&#125;, ok bool) &#123; // 1. 优先从read map中读取(无锁) read, _ := m.read.Load().(readOnly) e, ok := read.m[key] // 2. 如果不存在，并且ammended字段指明dirty map中有read map中不存在的字段，则加锁尝试从dirty map中加载 if !ok &amp;&amp; read.amended &#123; m.mu.Lock() // double check，避免在加锁的时候dirty map提升为read map read, _ = m.read.Load().(readOnly) e, ok = read.m[key] if !ok &amp;&amp; read.amended &#123; e, ok = m.dirty[key] // 3. 不管dirty中有没有找到 都增加misses计数 该函数可能将dirty map提升为readmap m.missLocked() &#125; m.mu.Unlock() &#125; if !ok &#123; return nil, false &#125; return e.load()&#125;// 从entry中atomic load实际interface&#123;&#125;func (e *entry) load() (value interface&#123;&#125;, ok bool) &#123; p := atomic.LoadPointer(&amp;e.p) if p == nil || p == expunged &#123; return nil, false &#125; return *(*interface&#123;&#125;)(p), true&#125;// 增加misses计数，并在必要的时候提升dirty mapfunc (m *Map) missLocked() &#123; m.misses++ if m.misses &lt; len(m.dirty) &#123; return &#125; // 提升过程很简单，直接将m.dirty赋给m.read.m // 提升完成之后 amended == false m.dirty == nil // m.dirty并不立即创建被拷贝元素，而是延迟创建 m.read.Store(readOnly&#123;m: m.dirty&#125;) m.dirty = nil m.misses = 0&#125; 3.3 Store12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485// Store sets the value for a key.func (m *Map) Store(key, value interface&#123;&#125;) &#123; // 1. 如果read map中存在该key 则尝试直接更改(由于修改的是entry内部的pointer，因此dirty map也可见) read, _ := m.read.Load().(readOnly) if e, ok := read.m[key]; ok &amp;&amp; e.tryStore(&amp;value) &#123; return &#125; m.mu.Lock() read, _ = m.read.Load().(readOnly) if e, ok := read.m[key]; ok &#123; if e.unexpungeLocked() &#123; // 2. 如果read map中存在该key，但p == expunged，则说明m.dirty!=nil并且m.dirty中不存在该key值 此时: // a. 将 p的状态由expunged先更改为nil // b. dirty map新建key // c. 更新entry.p = value (read map和dirty map指向同一个entry) m.dirty[key] = e &#125; // 3. 如果read map中存在该key，且 p != expunged，直接更新该entry (此时m.dirty==nil或m.dirty[key]==e) e.storeLocked(&amp;value) &#125; else if e, ok := m.dirty[key]; ok &#123; // 4. 如果read map中不存在该Key，但dirty map中存在该key，直接写入更新entry(read map中仍然没有) e.storeLocked(&amp;value) &#125; else &#123; // 5. 如果read map和dirty map中都不存在该key，则: // a. 如果dirty map为空，则需要创建dirty map，并从read map中拷贝未删除的元素 // b. 更新amended字段，标识dirty map中存在read map中没有的key // c. 将k v写入dirty map中，read.m不变 if !read.amended &#123; m.dirtyLocked() m.read.Store(readOnly&#123;m: read.m, amended: true&#125;) &#125; m.dirty[key] = newEntry(value) &#125; m.mu.Unlock()&#125;// 尝试直接更新entry 如果p == expunged 返回falsefunc (e *entry) tryStore(i *interface&#123;&#125;) bool &#123; p := atomic.LoadPointer(&amp;e.p) if p == expunged &#123; return false &#125; for &#123; if atomic.CompareAndSwapPointer(&amp;e.p, p, unsafe.Pointer(i)) &#123; return true &#125; p = atomic.LoadPointer(&amp;e.p) if p == expunged &#123; return false &#125; &#125;&#125;func (e *entry) unexpungeLocked() (wasExpunged bool) &#123; return atomic.CompareAndSwapPointer(&amp;e.p, expunged, nil)&#125;// 如果 dirty map为nil，则从read map中拷贝元素到dirty mapfunc (m *Map) dirtyLocked() &#123; if m.dirty != nil &#123; return &#125; read, _ := m.read.Load().(readOnly) m.dirty = make(map[interface&#123;&#125;]*entry, len(read.m)) for k, e := range read.m &#123; // a. 将所有为 nil的 p 置为 expunged // b. 只拷贝不为expunged 的 p if !e.tryExpungeLocked() &#123; m.dirty[k] = e &#125; &#125;&#125;func (e *entry) tryExpungeLocked() (isExpunged bool) &#123; p := atomic.LoadPointer(&amp;e.p) for p == nil &#123; if atomic.CompareAndSwapPointer(&amp;e.p, nil, expunged) &#123; return true &#125; p = atomic.LoadPointer(&amp;e.p) &#125; return p == expunged&#125; 3.4 Delete123456789101112131415161718192021222324252627282930313233// Delete deletes the value for a key.func (m *Map) Delete(key interface&#123;&#125;) &#123; // 1. 从read map中查找，如果存在，则置为nil read, _ := m.read.Load().(readOnly) e, ok := read.m[key] if !ok &amp;&amp; read.amended &#123; // double check m.mu.Lock() read, _ = m.read.Load().(readOnly) e, ok = read.m[key] // 2. 如果read map中不存在，但dirty map中存在，则直接从dirty map删除 if !ok &amp;&amp; read.amended &#123; delete(m.dirty, key) &#125; m.mu.Unlock() &#125; if ok &#123; // 将entry.p 置为 nil e.delete() &#125;&#125;func (e *entry) delete() (hadValue bool) &#123; for &#123; p := atomic.LoadPointer(&amp;e.p) if p == nil || p == expunged &#123; return false &#125; if atomic.CompareAndSwapPointer(&amp;e.p, p, nil) &#123; return true &#125; &#125;&#125; 3.5 总结除了Load/Store/Delete之外，sync.Map还提供了LoadOrStore/Range操作，但没有提供Len()方法，这是因为要统计有效的键值对只能先提升dirty map(dirty map中可能有read map中没有的键值对)，再遍历m.read(由于延迟删除，不是所有的键值对都有效)，这其实就是Range做的事情，因此在不添加新数据结构支持的情况下，sync.Map的长度获取和Range操作是同一复杂度的。这部分只能看官方后续支持。 sync.Map实现上并不是特别复杂，但仍有很多值得借鉴的地方: 通过entry隔离map变更和value变更，并且read map和dirty map指向同一个entry, 这样更新read map已有值无需加锁 double checking 延迟删除key，通过标记避免修改read map，同时极大提升了删除key的效率(删除read map中存在的key是无锁操作) 延迟创建dirty map，并且通过p的nil和expunged，amended字段来加强对dirty map状态的把控，减少对dirty map不必要的使用 sync.Map适用于key值相对固定，读多写少(更新m.read已有key仍然是无锁的)的情况，下面是一份使用RWLock的内建map和sync.Map的并发读写性能对比，代码在这里，代码对随机生成的整数key/value值进行并发的Load/Store/Delete操作，benchmark结果如下: 12345678910go test -bench=.goos: darwingoarch: amd64BenchmarkMutexMapStoreParalell-4 5000000 260 ns/opBenchmarkSyncMapStoreParalell-4 3000000 498 ns/opBenchmarkMutexMapLoadParalell-4 20000000 78.0 ns/opBenchmarkSyncMapLoadParalell-4 30000000 41.1 ns/opBenchmarkMutexMapDeleteParalell-4 10000000 235 ns/opBenchmarkSyncMapDeleteParalell-4 30000000 49.2 ns/opPASS 可以看到，除了并发写稍慢之外(并发写随机1亿以内的整数key/value，因此新建key操作远大于更新key，会导致sync.Map频繁的dirty map提升操作)，Load和Delete操作均快于mutexMap，特别是删除，得益于延迟删除，sync.Map的Delete几乎和Load一样快。 最后附上一份转载的sync.Map操作图解(图片出处):","tags":[{"name":"go","slug":"go","permalink":"http://wudaijun.com/tags/go/"}]},{"title":"Go Interface 实现","date":"2018-01-27T16:00:00.000Z","path":"2018/01/go-interface-implement/","text":"本文从源码的角度学习下Go接口的底层实现，以及接口赋值，反射，断言的实现原理。作为对比，用到了go1.8.6和go1.9.1两个版本。 1. eface空接口通过eface结构体实现，位于runtime/runtime2.go: 123456// src/runtime/runtime2.go// 空接口type eface struct &#123; _type *_type data unsafe.Pointer&#125; 空接口(eface)有两个域，所指向对象的类型信息(_type)和数据指针(data)。先看看_type字段： 123456789101112131415// 所有类型信息结构体的公共部分// src/rumtime/runtime2.gotype _type struct &#123; size uintptr // 类型的大小 ptrdata uintptr // size of memory prefix holding all pointers hash uint32 // 类型的Hash值 tflag tflag // 类型的Tags align uint8 // 结构体内对齐 fieldalign uint8 // 结构体作为field时的对齐 kind uint8 // 类型编号 定义于runtime/typekind.go alg *typeAlg // 类型元方法 存储hash和equal两个操作。map key便使用key的_type.alg.hash(k)获取hash值 gcdata *byte // GC相关信息 str nameOff // 类型名字的偏移 ptrToThis typeOff &#125; _type是go所有类型的公共描述，里面包含GC，反射等需要的细节，它决定data应该如何解释和操作，这也是它和C void*不同之处。各个类型所需要的类型描述是不一样的，比如chan，除了chan本身外，还需要描述其元素类型，而map则需要key类型信息和value类型信息等: 12345678910111213141516171819202122232425// src/runtime/type.go// ptrType represents a pointer type.type ptrType struct &#123; typ _type // 指针类型 elem *_type // 指针所指向的元素类型&#125;type chantype struct &#123; typ _type // channel类型 elem *_type // channel元素类型 dir uintptr&#125;type maptype struct &#123; typ _type key *_type elem *_type bucket *_type // internal type representing a hash bucket hmap *_type // internal type representing a hmap keysize uint8 // size of key slot indirectkey bool // store ptr to key instead of key itself valuesize uint8 // size of value slot indirectvalue bool // store ptr to value instead of value itself bucketsize uint16 // size of bucket reflexivekey bool // true if k==k for all keys needkeyupdate bool // true if we need to update key on an overwrite&#125; 这些类型信息的第一个字段都是_type(类型本身的信息)，接下来是一堆类型需要的其它详细信息(如子类型信息)，这样在进行类型相关操作时，可通过一个字(typ *_type)即可表述所有类型，然后再通过_type.kind可解析出其具体类型，最后通过地址转换即可得到类型完整的”_type树”，参考reflect.Type.Elem()函数: 12345678910111213141516171819202122232425// reflect/type.go// reflect.rtype结构体定义和runtime._type一致 type.kind定义也一致(为了分包而重复定义)// Elem()获取rtype中的元素类型，只针对复合类型(Array, Chan, Map, Ptr, Slice)有效func (t *rtype) Elem() Type &#123; switch t.Kind() &#123; case Array: tt := (*arrayType)(unsafe.Pointer(t)) return toType(tt.elem) case Chan: tt := (*chanType)(unsafe.Pointer(t)) return toType(tt.elem) case Map: // 对Map来讲，Elem()得到的是其Value类型 // 可通过rtype.Key()得到Key类型 tt := (*mapType)(unsafe.Pointer(t)) return toType(tt.elem) case Ptr: tt := (*ptrType)(unsafe.Pointer(t)) return toType(tt.elem) case Slice: tt := (*sliceType)(unsafe.Pointer(t)) return toType(tt.elem) &#125; panic(\"reflect: Elem of invalid type\")&#125; 2. ifaceiface结构体表示非空接口: 123456789101112131415161718192021222324252627282930// runtime/runtime2.go// 非空接口type iface struct &#123; tab *itab data unsafe.Pointer&#125;// 非空接口的类型信息type itab struct &#123; inter *interfacetype // 接口定义的类型信息 _type *_type // 接口实际指向值的类型信息 link *itab bad int32 inhash int32 fun [1]uintptr // 接口方法实现列表，即函数地址列表，按字典序排序&#125;// runtime/type.go// 非空接口类型，接口定义，包路径等。type interfacetype struct &#123; typ _type pkgpath name mhdr []imethod // 接口方法声明列表，按字典序排序&#125;// 接口的方法声明 type imethod struct &#123; name nameOff // 方法名 ityp typeOff // 描述方法参数返回值等细节&#125; 非空接口(iface)本身除了可以容纳满足其接口的对象之外，还需要保存其接口的方法，因此除了data字段，iface通过tab字段描述非空接口的细节，包括接口方法定义，接口方法实现地址，接口所指类型等。iface是非空接口的实现，而不是类型定义，iface的真正类型为interfacetype，其第一个字段仍然为描述其自身类型的_type字段。 为了提高查找效率，runtime中实现(interface_type, concrete_type) -&gt; itab(包含具体方法实现地址等信息)的hash表: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110// runtime/iface.goconst ( hashSize = 1009)var ( ifaceLock mutex // lock for accessing hash hash [hashSize]*itab)// 简单的Hash算法func itabhash(inter *interfacetype, typ *_type) uint32 &#123; h := inter.typ.hash h += 17 * typ.hash return h % hashSize&#125;// 根据interface_type和concrete_type获取或生成itab信息func getitab(inter *interfacetype, typ *_type, canfail bool) *itab &#123; ... // 算出hash key h := itabhash(inter, typ) var m *itab ... // 遍历hash slot链表 for m = (*itab)(atomic.Loadp(unsafe.Pointer(&amp;hash[h]))); m != nil; m = m.link &#123; // 如果在hash表中找到则返回 if m.inter == inter &amp;&amp; m._type == typ &#123; if m.bad &#123; if !canfail &#123; additab(m, locked != 0, false) &#125; m = nil &#125; ... return m &#125; &#125; &#125; // 如果没有找到，则尝试生成itab(会检查是否满足接口) m = (*itab)(persistentalloc(unsafe.Sizeof(itab&#123;&#125;)+uintptr(len(inter.mhdr)-1)*sys.PtrSize, 0, &amp;memstats.other_sys)) m.inter = inter m._type = typ additab(m, true, canfail) if m.bad &#123; return nil &#125; return m&#125;// 检查concrete_type是否符合interface_type 并且创建对应的itab结构体 将其放到hash表中func additab(m *itab, locked, canfail bool) &#123; inter := m.inter typ := m._type x := typ.uncommon() ni := len(inter.mhdr) nt := int(x.mcount) xmhdr := (*[1 &lt;&lt; 16]method)(add(unsafe.Pointer(x), uintptr(x.moff)))[:nt:nt] j := 0 for k := 0; k &lt; ni; k++ &#123; i := &amp;inter.mhdr[k] itype := inter.typ.typeOff(i.ityp) name := inter.typ.nameOff(i.name) iname := name.name() ipkg := name.pkgPath() if ipkg == \"\" &#123; ipkg = inter.pkgpath.name() &#125; for ; j &lt; nt; j++ &#123; t := &amp;xmhdr[j] tname := typ.nameOff(t.name) // 检查方法名字是否一致 if typ.typeOff(t.mtyp) == itype &amp;&amp; tname.name() == iname &#123; pkgPath := tname.pkgPath() if pkgPath == \"\" &#123; pkgPath = typ.nameOff(x.pkgpath).name() &#125; // 是否导出或在同一个包 if tname.isExported() || pkgPath == ipkg &#123; if m != nil &#123; // 获取函数地址，并加入到itab.fun数组中 ifn := typ.textOff(t.ifn) *(*unsafe.Pointer)(add(unsafe.Pointer(&amp;m.fun[0]), uintptr(k)*sys.PtrSize)) = ifn &#125; goto nextimethod &#125; &#125; &#125; // didn't find method if !canfail &#123; if locked &#123; unlock(&amp;ifaceLock) &#125; panic(&amp;TypeAssertionError&#123;\"\", typ.string(), inter.typ.string(), iname&#125;) &#125; m.bad = true break nextimethod: &#125; if !locked &#123; throw(\"invalid itab locking\") &#125; // 加到Hash Slot链表中 h := itabhash(inter, typ) m.link = hash[h] m.inhash = true atomicstorep(unsafe.Pointer(&amp;hash[h]), unsafe.Pointer(m))&#125; 可以看到，并不是每次接口赋值都要去检查一次对象是否符合接口要求，而是只在第一次生成itab信息，之后通过hash表即可找到itab信息。 3. 接口赋值123456789101112131415161718type MyInterface interface &#123; Print()&#125;type MyStruct struct&#123;&#125;func (ms MyStruct) Print() &#123;&#125;func main() &#123; a := 1 b := \"str\" c := MyStruct&#123;&#125; var i1 interface&#123;&#125; = a var i2 interface&#123;&#125; = b var i3 MyInterface = c var i4 interface&#123;&#125; = i3 var i5 = i4.(MyInterface) fmt.Println(i1, i2, i3, i4, i5)&#125; 用go1.8编译并反汇编: $GO1.8PATH/bin/go build -gcflags &#39;-N -l&#39; -o tmp tmp.go $GO1.8PATH/bin/go tool objdump -s &quot;main\\.main&quot; tmp 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758// var i1 interface&#123;&#125; = atest.go:16 0x1087146 488b442430 MOVQ 0x30(SP), AXtest.go:16 0x108714b 4889442438 MOVQ AX, 0x38(SP)test.go:16 0x1087150 488d05a9e10000 LEAQ 0xe1a9(IP), AX // 加载a的类型信息(int)test.go:16 0x1087157 48890424 MOVQ AX, 0(SP)test.go:16 0x108715b 488d442438 LEAQ 0x38(SP), AX // 加载a的地址test.go:16 0x1087160 4889442408 MOVQ AX, 0x8(SP)test.go:16 0x1087165 e84645f8ff CALL runtime.convT2E(SB)test.go:16 0x108716a 488b442410 MOVQ 0x10(SP), AX // 填充i1的type和datatest.go:16 0x108716f 488b4c2418 MOVQ 0x18(SP), CX test.go:16 0x1087174 48898424a0000000 MOVQ AX, 0xa0(SP)test.go:16 0x108717c 48898c24a8000000 MOVQ CX, 0xa8(SP)// var i2 interface&#123;&#125; = b// 与i1类似 加载类型信息 调用convT2E...test.go:17 0x10871bc e8ef44f8ff CALL runtime.convT2E(SB)test.go:17 0x10871c1 488b442410 MOVQ 0x10(SP), AXtest.go:17 0x10871c6 488b4c2418 MOVQ 0x18(SP), CXtest.go:17 0x10871cb 4889842490000000 MOVQ AX, 0x90(SP)test.go:17 0x10871d3 48898c2498000000 MOVQ CX, 0x98(SP)// var i3 MyInterface = ctest.go:18 0x10871db 488d051e000800 LEAQ 0x8001e(IP), AX // 加载c的类型信息(MyStruct)test.go:18 0x10871e2 48890424 MOVQ AX, 0(SP)test.go:18 0x10871e6 488d442430 LEAQ 0x30(SP), AXtest.go:18 0x10871eb 4889442408 MOVQ AX, 0x8(SP)test.go:18 0x10871f0 e86b45f8ff CALL runtime.convT2I(SB)test.go:18 0x10871f5 488b442410 MOVQ 0x10(SP), AXtest.go:18 0x10871fa 488b4c2418 MOVQ 0x18(SP), CXtest.go:18 0x10871ff 4889842480000000 MOVQ AX, 0x80(SP)test.go:18 0x1087207 48898c2488000000 MOVQ CX, 0x88(SP)// var i4 interface&#123;&#125; = i3test.go:19 0x108720f 488b842488000000 MOVQ 0x88(SP), AXtest.go:19 0x1087217 488b8c2480000000 MOVQ 0x80(SP), CX // CX = i3.itabtest.go:19 0x108721f 48898c24e0000000 MOVQ CX, 0xe0(SP) test.go:19 0x1087227 48898424e8000000 MOVQ AX, 0xe8(SP) // 0xe8(SP) = i3.datatest.go:19 0x108722f 48894c2448 MOVQ CX, 0x48(SP) test.go:19 0x1087234 4885c9 TESTQ CX, CXtest.go:19 0x1087237 7505 JNE 0x108723etest.go:19 0x1087239 e915020000 JMP 0x1087453test.go:19 0x108723e 8401 TESTB AL, 0(CX)test.go:19 0x1087240 488b4108 MOVQ 0x8(CX), AX // (i3.itab+8) 得到 &amp;i3.itab.typ，因此AX=i3.itab.typ 即iface指向对象的具体类型信息，这里是MyStructtest.go:19 0x1087244 4889442448 MOVQ AX, 0x48(SP) // 0x48(SP) = i3.itab.typtest.go:19 0x1087249 eb00 JMP 0x108724btest.go:19 0x108724b 488b8424e8000000 MOVQ 0xe8(SP), AX // AX = i3.datatest.go:19 0x1087253 488b4c2448 MOVQ 0x48(SP), CX // CX = i3.itab.typtest.go:19 0x1087258 48894c2470 MOVQ CX, 0x70(SP) // i4.typ = i3.itab.typtest.go:19 0x108725d 4889442478 MOVQ AX, 0x78(SP) // i4.data = i3.data// var i5 = i4.(MyInterface)test.go:20 0x1087262 48c78424f000000000000000 MOVQ $0x0, 0xf0(SP)test.go:20 0x108726e 48c78424f800000000000000 MOVQ $0x0, 0xf8(SP)test.go:20 0x108727a 488b442478 MOVQ 0x78(SP), AXtest.go:20 0x108727f 488b4c2470 MOVQ 0x70(SP), CXtest.go:21 0x1087284 488d1535530100 LEAQ 0x15335(IP), DXtest.go:20 0x108728b 48891424 MOVQ DX, 0(SP) // 压入 MyInterface 的 interfacetypetest.go:20 0x108728f 48894c2408 MOVQ CX, 0x8(SP) // 压入 i4.typetest.go:20 0x1087294 4889442410 MOVQ AX, 0x10(SP) // 压入 i4.datatest.go:20 0x1087299 e87245f8ff CALL runtime.assertE2I(SB) // func assertE2I(inter *interfacetype, e eface) (r iface)... 可以看到编译器通过convT2E和convT2I将编译器已知的类型赋给接口(其中E代表eface，I代表iface，T代表编译器已知类型，即静态类型)，编译器知晓itab的布局，会在编译期检查接口是否适配，并且生成itab信息，因此编译器生成的convT2X调用是必然成功的。 对于接口间的赋值，将iface赋给eface比较简单，直接提取eface的interfacetype和data赋给iface即可。而反过来，则需要使用接口断言，接口断言通过assertE2I, assertI2I等函数来完成，这类assert函数根据使用方调用方式有两个版本: 12i5 := i4.(MyInterface) // call conv.assertE2Ii5, ok := i4.(MyInterface) // call conv.AssertE2I2 下面看一下几个常用的conv和assert函数实现: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950// go1.8/src/runtime/iface.gofunc convT2E(t *_type, elem unsafe.Pointer) (e eface) &#123; if raceenabled &#123; raceReadObjectPC(t, elem, getcallerpc(unsafe.Pointer(&amp;t)), funcPC(convT2E)) &#125; if msanenabled &#123; msanread(elem, t.size) &#125; if isDirectIface(t) &#123; // This case is implemented directly by the compiler. throw(\"direct convT2E\") &#125; x := newobject(t) // TODO: We allocate a zeroed object only to overwrite it with // actual data. Figure out how to avoid zeroing. Also below in convT2I. typedmemmove(t, x, elem) e._type = t e.data = x return&#125;func convT2I(tab *itab, elem unsafe.Pointer) (i iface) &#123; t := tab._type if raceenabled &#123; raceReadObjectPC(t, elem, getcallerpc(unsafe.Pointer(&amp;tab)), funcPC(convT2I)) &#125; if msanenabled &#123; msanread(elem, t.size) &#125; if isDirectIface(t) &#123; // This case is implemented directly by the compiler. throw(\"direct convT2I\") &#125; x := newobject(t) typedmemmove(t, x, elem) i.tab = tab i.data = x return&#125;func assertE2I(inter *interfacetype, e eface) (r iface) &#123; t := e._type if t == nil &#123; // explicit conversions require non-nil interface value. panic(&amp;TypeAssertionError&#123;\"\", \"\", inter.typ.string(), \"\"&#125;) &#125; r.tab = getitab(inter, t, false) r.data = e.data return&#125; 在assertE2I中，我们看到了getitab函数，即i5=i4.(MyInterface)中，会去判断i4的concretetype(MyStruct)是否满足MyInterface的interfacetype，由于前面我们执行过var i3 MyInterface = c，因此hash[itabhash(MyInterface, MyStruct)]已经存在itab，所以无需再次检查接口是否满足，从hash表中取出itab即可(里面针对接口的各个方法实现地址都已经初始化完成)。 而在go1.9中，有一些优化: 1.对convT2x针对简单类型(如int32,string,slice)进行特例化优化(避免typedmemmove): 123456convT2E16, convT2I16convT2E32, convT2I32convT2E64, convT2I64convT2Estring, convT2IstringconvT2Eslice, convT2IsliceconvT2Enoptr, convT2Inoptr 据统计，在编译make.bash的时候，有93%的convT2x调用都可通过以上特例化优化。参考这里。 2.优化了剩余对convT2I的调用 由于itab由编译器生成(参考上面go1.8生成的汇编代码和convT2I函数)，可以直接由编译器将itab和elem直接赋给iface的tab和data字段，避免函数调用和typedmemmove。关于此优化可参考1和2。 具体汇编代码不再列出，感兴趣的同学可以自己尝试。 对接口的构造和转换本质上是对object的type和data两个字段的操作，对空接口eface来说，只需将type和data提取并填入即可，而对于非空接口iface构造和断言，需要判断object或eface是否满足接口定义，并生成对应的itab(包含接口类型，object类型，object接口实现方法地址等信息)，每个已初始化的iface都有itab字段，该字段的生成是通过hash表优化的，以及对于每个interfacetype &lt;-&gt; concrettype对，只需要生成一次itab，之后从hash表中取就可以了。由于编译器知晓itab的内存布局，因此在将iface赋给eface的时候可以避免函数调用，直接将iface.itab.typ赋给eface.typ。 4. 类型反射4.1 类型&amp;值解析类型和值解析无非就是将eface{}的_type和data字段取出进行解析，针对TypeOf的实现很简单: 12345678910// 代码位于relect/type.go// reflect.Type接口的实现为: reflect.rtype// reflect.rtype结构体定义和runtime._type一样，只是实现了reflect.Type接口，实现了一些诸如Elem()，Name()之类的方法:func TypeOf(i interface&#123;&#125;) Type &#123; // emptyInterface结构体定义与eface一样，都是两个word(type和data) eface := *(*emptyInterface)(unsafe.Pointer(&amp;i)) return toType(eface.typ) // 将eface.typ赋给reflect.Type接口，供外部使用&#125; 要知道，对于复合类型，如Ptr, Slice, Chan, Map等，它们的type信息中包含其子类型的信息，如Slice元素类型，而其元素类型也可能是复合类型，因此type实际上是一颗”类型树”，可通过reflect.Elem()和reflect.Key()等API来获取这些子类型信息，但如果如果type不匹配(比如reflect.TypeOf([]int{1,2}).Key())，会panic。 reflect.ValueOf()则要复杂一些，因为它需要根据type来决定数据应该如何被解释: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091type Value struct &#123; // 值的类型 typ *rtype // 立即数或指向数据的指针 ptr unsafe.Pointer // type flag uintptr // 指明值的类型，是否只读，ptr字段是否是指针等 flag&#125;func ValueOf(i interface&#123;&#125;) Value &#123; if i == nil &#123; return Value&#123;&#125; &#125; escapes(i) return unpackEface(i)&#125;// 将数据从interface&#123;&#125;解包为reflec.Valuefunc unpackEface(i interface&#123;&#125;) Value &#123; e := (*emptyInterface)(unsafe.Pointer(&amp;i)) // NOTE: don&apos;t read e.word until we know whether it is really a pointer or not. t := e.typ if t == nil &#123; return Value&#123;&#125; &#125; f := flag(t.Kind()) if ifaceIndir(t) &#123; f |= flagIndir &#125; return Value&#123;t, e.word, f&#125;&#125; // 将数据由reflect.Value打包为interface&#123;&#125;func packEface(v Value) interface&#123;&#125; &#123; t := v.typ var i interface&#123;&#125; e := (*emptyInterface)(unsafe.Pointer(&amp;i)) // First, fill in the data portion of the interface. switch &#123; case ifaceIndir(t): if v.flag&amp;flagIndir == 0 &#123; panic(&quot;bad indir&quot;) &#125; ptr := v.ptr if v.flag&amp;flagAddr != 0 &#123; c := unsafe_New(t) typedmemmove(t, c, ptr) ptr = c &#125; e.word = ptr case v.flag&amp;flagIndir != 0: e.word = *(*unsafe.Pointer)(v.ptr) default: e.word = v.ptr &#125; e.typ = t return i&#125; // 将reflect.Value转换为interface&#123;&#125;，相当于reflect.ValueOf的逆操作// 等价于: var i interface&#123;&#125; = (v&apos;s underlying value)func (v Value) Interface() (i interface&#123;&#125;) &#123; return valueInterface(v, true)&#125;func valueInterface(v Value, safe bool) interface&#123;&#125; &#123; if v.flag == 0 &#123; panic(&amp;ValueError&#123;&quot;reflect.Value.Interface&quot;, 0&#125;) &#125; if safe &amp;&amp; v.flag&amp;flagRO != 0 &#123; panic(&quot;reflect.Value.Interface: cannot return value obtained from unexported field or method&quot;) &#125; if v.flag&amp;flagMethod != 0 &#123; v = makeMethodValue(&quot;Interface&quot;, v) &#125; // 当interface&#123;&#125;作为子类型时，会产生类型为Interface的Value // 如 reflect.TypeOf(m).Elem().Kind() == Interface if v.kind() == Interface &#123; if v.NumMethod() == 0 &#123; return *(*interface&#123;&#125;)(v.ptr) &#125; return *(*interface &#123; M() &#125;)(v.ptr) &#125; return packEface(v)&#125; 和reflect.Type.Elem()一样，reflect.Value也提供一系列的方法进行值解析，如Elem()可以得到Interface或Ptr指向的值，Index()可以得到Array, Slice或String对应下标的元素等。但在使用这些API前要先通过reflect.Type.Kind()确认类型匹配，否则会panic。 4.2 类型反射类型&amp;值解析实际上对将interface{}的type和data提出来，以reflect.Type和reflect.Value接口暴露给用户使用，而类型反射是指提供一个reflect.Type，我们可以创建一个对应类型的对象，这可以通过reflect.New()来完成： 12345678910111213141516171819202122// reflect/value.go// New returns a Value representing a pointer to a new zero value// for the specified type. That is, the returned Value's Type is PtrTo(typ).func New(typ Type) Value &#123; if typ == nil &#123; panic(\"reflect: New(nil)\") &#125; ptr := unsafe_New(typ.(*rtype)) fl := flag(Ptr) return Value&#123;typ.common().ptrTo(), ptr, fl&#125;&#125; // runtime/malloc.gofunc newobject(typ *_type) unsafe.Pointer &#123; return mallocgc(typ.size, typ, true)&#125;//go:linkname reflect_unsafe_New reflect.unsafe_Newfunc reflect_unsafe_New(typ *_type) unsafe.Pointer &#123; return newobject(typ)&#125; PS: Go的包管理看来还是不够好用，为了达成reflect包和runtime包的”解耦”，先后使用和copy struct define和link method “黑科技”。 reflect.New()创建对应Type的对象并返回其指针，以下是一个简单的示例: 1234567891011121314151617181920212223type User struct &#123; UserId int Name string&#125;func main() &#123; x := User&#123;UserId: 111&#125; typ := reflect.TypeOf(x) // reflect.New返回的是*User 而不是User y := reflect.New(typ).Elem() for i:=0; i&lt;typ.NumField(); i++ &#123; // 根据每个struct field的type 设置其值 fieldT := typ.Field(i) fieldV := y.Field(i) kind := fieldT.Type.Kind() if kind == reflect.Int&#123; fieldV.SetInt(123) &#125; else if kind == reflect.String&#123; fieldV.SetString(&quot;wudaijun&quot;) &#125; &#125; fmt.Println(y.Interface())&#125; 以上代码稍改一下，即可实现简单CSV解析：根据提供的struct原型，分析其字段，并一一映射到csv每一列，将csv读出的string转换为对应的struct field type，对于简单类型使用strconv即可完成，对于复合数据结构如Map, Slice，可使用json库来定义和解析。 reflect.New()和reflect.Zero()可用于创建Type对应的对象，除此之外，reflect包还提供了reflect.MapOf(), reflect.SliceOf()等方法用于基于现有类型创建复合类型。具体源码不再列出，参考reflect/type.go和reflect/value.go。 reflect提供的反射能力不可谓不强大，但在实际使用中仍然不够好用，一个因为Go本质上是静态类型语言，要提供”动态类型”的部分语义是比较复杂和不易用的，这有点像C++提供泛型编程，虽然强大，但也是把双刃剑。 参考: Golang汇编快速指南 Go Interface源码剖析","tags":[{"name":"go","slug":"go","permalink":"http://wudaijun.com/tags/go/"}]},{"title":"Go 调度模型","date":"2018-01-16T16:00:00.000Z","path":"2018/01/go-scheduler/","text":"G P M 模型定义于src/runtime/runtime2.go: G: Gourtines, 每个Goroutine对应一个G结构体，G保存Goroutine的运行堆栈，即并发任务状态。G并非执行体，每个G需要绑定到P才能被调度执行。 P: Processors, 对G来说，P相当于CPU核，G只有绑定到P(在P的local runq中)才能被调度。对M来说，P提供了相关的执行环境(Context)，如内存分配状态(mcache)，任务队列(G)等 M: Machine, OS线程抽象，负责调度任务，和某个P绑定，从P的runq中不断取出G，切换堆栈并执行，M本身不具备执行状态，在需要任务切换时，M将堆栈状态写回G，任何其它M都能据此恢复执行。 Go1.1之前只有G-M模型，没有P，Dmitry Vyukov在Scalable Go Scheduler Design Doc提出该模型在并发伸缩性方面的问题，并通过加入P(Processors)来改进该问题。 G-P-M模型示意图: 补充说明: P的个数由GOMAXPROCS指定，是固定的，因此限制最大并发数 M的个数是不定的，由Go Runtime调整，默认最大限制为10000个 调度流程在M与P绑定后，M会不断从P的Local队列(runq)中取出G(无锁操作)，切换到G的堆栈并执行，当P的Local队列中没有G时，再从Global队列中返回一个G(有锁操作，因此实际还会从Global队列批量转移一批G到P Local队列)，当Global队列中也没有待运行的G时，则尝试从其它的P窃取(steal)部分G来执行，源代码如下: // go1.9.1 src/runtime/proc.go // 省略了GC检查等其它细节，只保留了主要流程 // g: G结构体定义 // sched: Global队列 // 获取一个待执行的G func findrunnable() (gp *g, inheritTime bool) { // 获取当前的G对象 _g_ := getg() top: // 获取当前P对象 _p_ := _g_.m.p.ptr() // 1. 尝试从P的Local队列中取得G 优先_p_.runnext 然后再从Local队列中取 if gp, inheritTime := runqget(_p_); gp != nil { return gp, inheritTime } // 2. 尝试从Global队列中取得G if sched.runqsize != 0 { lock(&amp;sched.lock) // globrunqget从Global队列中获取G 并转移一批G到_p_的Local队列 gp := globrunqget(_p_, 0) unlock(&amp;sched.lock) if gp != nil { return gp, false } } // 3. 检查netpoll任务 if netpollinited() &amp;&amp; sched.lastpoll != 0 { if gp := netpoll(false); gp != nil { // non-blocking // netpoll返回的是G链表，将其它G放回Global队列 injectglist(gp.schedlink.ptr()) casgstatus(gp, _Gwaiting, _Grunnable) if trace.enabled { traceGoUnpark(gp, 0) } return gp, false } } // 4. 尝试从其它P窃取任务 procs := uint32(gomaxprocs) if atomic.Load(&amp;sched.npidle) == procs-1 { goto stop } if !_g_.m.spinning { _g_.m.spinning = true atomic.Xadd(&amp;sched.nmspinning, 1) } for i := 0; i &lt; 4; i++ { // 随机P的遍历顺序 for enum := stealOrder.start(fastrand()); !enum.done(); enum.next() { if sched.gcwaiting != 0 { goto top } stealRunNextG := i &gt; 2 // first look for ready queues with more than 1 g // runqsteal执行实际的steal工作，从目标P的Local队列转移一般的G过来 // stealRunNextG指是否steal目标P的p.runnext G if gp := runqsteal(_p_, allp[enum.position()], stealRunNextG); gp != nil { return gp, false } } } ... } 当没有G可被执行时，M会与P解绑，然后进入休眠(idle)状态。 用户态阻塞/唤醒当Goroutine因为Channel操作而阻塞(通过gopark)时，对应的G会被放置到某个wait队列(如channel的waitq)，该G的状态由_Gruning变为_Gwaitting，而M会跳过该G尝试获取并执行下一个G。 当阻塞的G被G2唤醒(通过goready)时(比如channel可读/写)，G会尝试加入G2所在P的runnext，然后再是P Local队列和Global队列。 系统调用当G被阻塞在某个系统调用上时，此时G会阻塞在_Gsyscall状态，M也处于block on syscall状态，此时仍然可被抢占调度: 执行该G的M会与P解绑，而P则尝试与其它idle的M绑定，继续执行其它G。如果没有其它idle的M，但队列中仍然有G需要执行，则创建一个新的M。 当系统调用完成后，G会重新尝试获取一个idle的P，并恢复执行，如果没有idle的P，G将加入到Global队列。 系统调用能被调度的关键有两点: runtime/syscall包中，将系统调用分为SysCall和RawSysCall，前者和后者的区别是前者会在系统调用前后分别调用entersyscall和exitsyscall(位于src/runtime/proc.go)，做一些现场保存和恢复操作，这样才能使P安全地与M解绑，并在其它M上继续执行其它G。某些系统调用本身可以确定会长时间阻塞(比如锁)，会调用entersyscallblock在发起系统调用前直接让P和M解绑(handoffp)。 另一个关键点是sysmon，它负责检查所有系统调用的执行时间，判断是否需要handoffp。 sysmonsysmon是一个由runtime启动的M，也叫监控线程，它无需P也可以运行，它每20us~10ms唤醒一次，主要执行: 释放闲置超过5分钟的span物理内存； 如果超过2分钟没有垃圾回收，强制执行； 将长时间未处理的netpoll结果添加到任务队列； 向长时间运行的G任务发出抢占调度； 收回因syscall长时间阻塞的P； 入口在src/runtime/proc.go:sysmon函数，它通过retake实现对syscall和长时间运行的G进行调度: func retake(now int64) uint32 { n := 0 for i := int32(0); i &lt; gomaxprocs; i++ { _p_ := allp[i] if _p_ == nil { continue } pd := &amp;_p_.sysmontick s := _p_.status if s == _Psyscall { // Retake P from syscall if it&#39;s there for more than 1 sysmon tick (at least 20us). t := int64(_p_.syscalltick) if int64(pd.syscalltick) != t { pd.syscalltick = uint32(t) pd.syscallwhen = now continue } // 如果当前P Local队列没有其它G，当前有其它P处于Idle状态，并且syscall执行事件不超过10ms，则不用解绑当前P(handoffp) if runqempty(_p_) &amp;&amp; atomic.Load(&amp;sched.nmspinning)+atomic.Load(&amp;sched.npidle) &gt; 0 &amp;&amp; pd.syscallwhen+10*1000*1000 &gt; now { continue } // handoffp incidlelocked(-1) if atomic.Cas(&amp;_p_.status, s, _Pidle) { if trace.enabled { traceGoSysBlock(_p_) traceProcStop(_p_) } n++ _p_.syscalltick++ handoffp(_p_) } incidlelocked(1) } else if s == _Prunning { // Preempt G if it&#39;s running for too long. t := int64(_p_.schedtick) if int64(pd.schedtick) != t { pd.schedtick = uint32(t) pd.schedwhen = now continue } // 如果当前G执行时间超过10ms，则抢占(preemptone) if pd.schedwhen+forcePreemptNS &gt; now { continue } // 执行抢占 preemptone(_p_) } } return uint32(n) } 抢占式调度当某个goroutine执行超过10ms，sysmon会向其发起抢占调度请求，由于Go调度不像OS调度那样有时间片的概念，因此实际抢占机制要弱很多: Go中的抢占实际上是为G设置抢占标记(g.stackguard0)，当G调用某函数时(更确切说，在通过newstack分配函数栈时)，被编译器安插的指令会检查这个标记，并且将当前G以runtime.Goched的方式暂停，并加入到全局队列。源代码如下: // src/runtime/stack.go // Called from runtime·morestack when more stack is needed. // Allocate larger stack and relocate to new stack. // Stack growth is multiplicative, for constant amortized cost. func newstack(ctxt unsafe.Pointer) { ... // gp为当前G preempt := atomic.Loaduintptr(&amp;gp.stackguard0) == stackPreempt if preempt { ... // Act like goroutine called runtime.Gosched. // G状态由_Gwaiting变为 _Grunning 这是为了能以Gosched的方式暂停Go casgstatus(gp, _Gwaiting, _Grunning) gopreempt_m(gp) // never return } } // 以goched的方式将G重新放入 func goschedImpl(gp *g) { status := readgstatus(gp) // 由Running变为Runnable casgstatus(gp, _Grunning, _Grunnable) // 与M解除绑定 dropg() lock(&amp;sched.lock) // 将G放入Global队列 globrunqput(gp) unlock(&amp;sched.lock) // 重新调度 schedule() } func gopreempt_m(gp *g) { if trace.enabled { traceGoPreempt() } goschedImpl(gp) } netpoll前面的findrunnable，G的获取除了p.runnext，p.runq和sched.runq外，还有一中G从netpoll中获取，netpoll是Go针对网络IO的一种优化，本质上为了避免网络IO陷入系统调用之中，这样使得即便G发起网络I/O操作也不会导致M被阻塞（仅阻塞G），从而不会导致大量M被创建出来。 G创建流程G结构体会复用，对可复用的G管理类似于待运行的G管理，也有Local队列(p.gfree)和Global队列(sched.gfree)之分，获取算法差不多，优先从p.gfree中获取(无锁操作)，否则从sched.gfree中获取并批量转移一部分(有锁操作)，源代码参考src/runtime/proc.go:gfget函数。 从Goroutine的角度来看，通过go func()创建时，会从当前闲置的G队列取得可复用的G，如果没有则通过malg新建一个G，然后: 尝试将G添加到当前P的runnext中，作为下一个执行的G 否则放到Local队列runq中(无锁) 如果以上操作都失败，则添加到Global队列sched.runq中(有锁操作，因此也会顺便将当P.runq中一半的G转移到sched.runq) G的几种暂停方式: goched: 将当前的G暂停，保存堆栈状态，以Runnable状态放入Global队列中，让当前M继续执行其它任务。无需对G进行唤醒操作，因为总会有M从Global队列取得并执行该M。抢占调度即使用该方式。 gopark: 与goched的最大区别在于gopark没有将G放回执行队列，而是位于某个等待队列中(如channel的waitq，此时G状态为_Gwaitting)，因此G必须被手动唤醒(通过goready)，否则会丢失任务。应用层阻塞通常使用这种方式。 notesleep: 既不让出M，也不让G重回任务队列，直接让线程休眠直到被唤醒，该方式更快，通常用于gcMark，stopm这类自旋场景 goexit: 立即终止G任务，不管其处于调用堆栈的哪个层次，在终止前，确保所有defer正确执行。 Go调度器的查看方法示例程序，对比cgo sleep和time.sleep系统调用情况: // #include &lt;unistd.h&gt; import &quot;C&quot; func main() { var wg sync.WaitGroup wg.Add(1000) for i := 0; i &lt; 1000; i++ { go func() { C.sleep(1) // 测试1 // time.Sleep(time.Second) // 测试2 wg.Done() }() } wg.Wait() println(&quot;done!&quot;) time.Sleep(time.Second * 3) } 通过GODEBUG运行时环境变量的schedtrace=1000参数，可以每隔1000ms查看一次调度器状态: $ GODEBUG=schedtrace=1000 ./test // 测试1输出结果 SCHED 0ms: gomaxprocs=4 idleprocs=2 threads=1003 spinningthreads=2 idlethreads=32 runqueue=0 [0 0 0 0] done! SCHED 1001ms: gomaxprocs=4 idleprocs=4 threads=1003 spinningthreads=0 idlethreads=1000 runqueue=0 [0 0 0 0] SCHED 2001ms: gomaxprocs=4 idleprocs=4 threads=1003 spinningthreads=0 idlethreads=1000 runqueue=0 [0 0 0 0] SCHED 3010ms: gomaxprocs=4 idleprocs=4 threads=1003 spinningthreads=0 idlethreads=1000 runqueue=0 [0 0 0 0] // 测试2输出结果 SCHED 0ms: gomaxprocs=4 idleprocs=2 threads=6 spinningthreads=1 idlethreads=2 runqueue=129 [0 128 0 0] done! SCHED 1009ms: gomaxprocs=4 idleprocs=4 threads=6 spinningthreads=0 idlethreads=3 runqueue=0 [0 0 0 0] SCHED 2010ms: gomaxprocs=4 idleprocs=4 threads=6 spinningthreads=0 idlethreads=3 runqueue=0 [0 0 0 0] SCHED 3019ms: gomaxprocs=4 idleprocs=4 threads=6 spinningthreads=0 idlethreads=3 runqueue=0 [0 0 0 0] 其中schedtrace日志每一行的字段意义: SCHED：调试信息输出标志字符串，代表本行是goroutine scheduler的输出； 1001ms：即从程序启动到输出这行日志的时间； gomaxprocs: P的数量； idleprocs: 处于idle状态的P的数量；通过gomaxprocs和idleprocs的差值，我们就可知道执行go代码的P的数量； threads: os threads的数量，包含scheduler使用的m数量，加上runtime自用的类似sysmon这样的thread的数量； spinningthreads: 处于自旋状态的os thread数量； idlethread: 处于idle状态的os thread的数量； runqueue： go scheduler全局队列中G的数量； [0 0 0 0]: 分别为4个P的local queue中的G的数量。 可以看出，time.Sleep并没有使用系统调用，而是进行了类似netpoll类似的优化，使得仅仅是G阻塞，M不会阻塞，而在使用cgo sleep的情况下，可以看到大量的闲置M。 通过运行时环境变量GODEBUG的schedtrace参数可定时查看调度器状态: // 每1000ms打印一次 $GODEBUG=schedtrace=1000 godoc -http=:6060 SCHED 0ms: gomaxprocs=4 idleprocs=3 threads=3 spinningthreads=0 idlethreads=0 runqueue=0 [0 0 0 0] SCHED 1001ms: gomaxprocs=4 idleprocs=0 threads=9 spinningthreads=0 idlethreads=3 runqueue=2 [8 14 5 2] SCHED 2006ms: gomaxprocs=4 idleprocs=0 threads=25 spinningthreads=0 idlethreads=19 runqueue=12 [0 0 4 0] SCHED 3006ms: gomaxprocs=4 idleprocs=0 threads=26 spinningthreads=0 idlethreads=8 runqueue=2 [0 1 1 0] ... GODEBUG还可使用GODEBUG=&quot;schedtrace=1000,scheddetail=1&quot;选项来查看每个G,P,M的调度状态，打出的信息非常详尽复杂，平时应该是用不到。关于Go调试可参考Dmitry Vyukov大牛的Debugging performance issues in Go programs。 参考资料: scheduler-tracing-in-go 也谈goroutine调度器—TonyBai Go学习笔记—雨痕","tags":[{"name":"go","slug":"go","permalink":"http://wudaijun.com/tags/go/"}]},{"title":"常见GC算法","date":"2017-12-02T16:00:00.000Z","path":"2017/12/gc-study/","text":"先来看看GC(自动垃圾回收)的主要问题: 额外的开销(内存/CPU) 执行GC的时机无法预测，在实时性要求高的场景或事务处理来说可能是不可容忍的 部分GC算法会Stop-the-world 各语言运行时在选取GC算法时，都要从这几个方面进行衡量与取舍，下面是一些常见的GC算法。 引用计数(Reference counting):为每个对象维护一个计数，保存其它对象指向它的引用数量。当一个引用被覆盖或销毁，该引用对象的引用计数-1，当一个引用被建立或拷贝，引用对象的引用计数+1，如果对象的引用计数为0，则表明该对象不再被访问(inaccessible)，将被回收。引用计数有如下优缺点: 优点: GC开销将被均摊到程序运行期，不会有长时间的回收周期。 每个对象的生命周期被明确定义，可用于某些编译器的runtime优化。 算法简单，易于实现。 即时回收，不会等内存状态到达某个阀值再执行回收。 缺点: 引用计数会频繁更新，带来效率开销 原生的引用计数算法无法回收循环引用的对象链(如C++ shared_ptr引用链) 针对第一个频繁更新的缺点，可以使用延迟更新和合并更新等技术，这通常能够很好优化局部频繁的引用更新(如for循环)，虽然这也增加了算法实现复杂度。 针对循环引用的问题，一种解决方案是弱引用(weak reference)，弱引用不影响GC，通常的实践是owner持有child的强引用，child持有owner的弱引用，在事件注册器或其它容器中，如果你只希望保存这个引用，但不希望这个引用影响GC时，也可弱引用。弱引用在使用时，需要先判断对象是否还存在，如C++的weak_ptr需要先转换为shared_ptr。但这不能完全避免无意的循环墙引用，一些GC算法可以检测循环引用，例如以追踪式GC的思路，从根出发，回收那些不可达的对象。 标记-清扫(Mark-and-Sweep):标记-清扫算法为每个对象预留一个Flag位，分为两个阶段，标记阶段会从Root向下递归遍历所有对象，并将所有可达对象的Flag位设为”正在使用”。第二阶段，清扫阶段，遍历所有内存，回收那些所有未被标记为”正在使用”的对象。整个算法的思路很简单，也基本上避免了引用计数法的缺点，但最大的缺点在于回收期间整个系统必须暂停(Stop-the-world)。 三色标记法(Tri-color marking):针对原生标记-清扫算法标记过程会STW的缺点，三色标记法改进了标记方案。三色标记法将所有对象分为三类: 白色: GC的候选对象集合(待处理) 灰色: 可从根访问，并且还未扫描对白色集合对象的引用(处理中,不会被GC,但引用待确认) 黑色: 可从根访问，并且不存在对白色集合的引用(处理完成) 步骤如下: 初始化，所有对象都是白色 从根遍历，所有可达对象标记为灰色 从灰色对象队列中取出对象，将其引用的对象标记为灰色，并将自己标记为黑色 重复第三步，直到灰色队列为空，此时白色对象即为孤儿对象，进行回收 三色标记法有个重要的不变量: 黑色对象不会引用任何白色对象，因此白色对象可以在灰色对象处理完成之后立即回收。此算法最大的特点在于将标记过程拆分和量化，使得用户程序和标记过程可并行执行(需要其它技术追踪标记过程中的对象引用变更)，不用Stop-the-world，算法可按照各个集合的大小阶段性执行GC，并且不用遍历整个内存空间。 半空间回收器(semi-space collector)半空间收集器将内存分为两半，分别叫from space和to space，初始时，所有的对象都在to space中分配直到空间用完，触发一次回收周期，此时to space和from space互换，然后将所有根可访问的对象从from space拷贝到to space，之后程序可以继续执行。新的对象继续在新的to space中分配，直到再次空间用完触发回收。该算法的优点是所有存活的数据结构都紧凑排列在to space，内存分配也可通过简单的分配指针自增来实现，缺点是浪费了一半的内存空间。这种GC方案也叫stop-and-copy。 三色标记法的一些变形moving or non-moving三色标记法执行标记流程后(灰色队列为空)，所有的白色对象可被回收，那么这些白色对象是直接被回收，其它不变还是执行内存拷贝(non-moving)，将黑色对象移动并覆盖不再使用的白色对象内存(moving)。相当于执行内存块调整(compact)，可以让内存结构更有序，下次分配更快。这部分算法独立于三色标记，可以由GC算法在运行时选择。 mark and non-sweep基于半空间收集器的copy思路，可以运用到三色标记法中，通过颜色互换来模拟space互换，该算法对三色标记的颜色定义有所不同，步骤如下: 对象只有黑色与白色两种颜色，并且黑色与白色是可以互换的(可通过修改黑白的位映射来实现，无需修改对象) 所有可被访问的对象都是黑色，所有可被回收的对象为白色 对象从白色对象空间分配，被分配后即标记为黑色 当内存空间不足(不再有白色对象)，触发GC，此时所有黑色对象变为白色对象，从根遍历所有可访问的对象，将其由白色变为黑色，此时剩下的白色即为可被回收对象，程序可继续运行 程序继续从白色空间分配，直到白色空间用完，再次触发GC 分代GC(Generational GC)前面的各种标记扫描算法，都有一个缺点，每次需要遍历标记所有可达对象，包括一些长期存活的对象，或者说，GC也具有局部性: 最近被分配的对象越容易不再使用。分代GC即基于这一启发，它将内存空间按”代(Generation)”分为几个部分(通常是两代，即Young Generation和Old Generation)，并尽可能频繁地在年轻的一代执行GC，当年轻一代的内存空间不够时，将可达对象全部移到上一代，此时年轻代的内存全部闲置，可用于分配新对象，这样更快并且通常也更有效率。当老一代GC不够用时，才执行Full Sweep。 通常大部分语言的运行时都会混合多种GC算法，比如Erlang的GC(参考1,2)就混合了分代GC和引用计数(高效)，在进程堆内使用分代GC，对全局数据使用引用计数(即时释放内存)。 参考: Reference counting - wikipedia Tracing garbage collection - wikipedia","tags":[{"name":"gc","slug":"gc","permalink":"http://wudaijun.com/tags/gc/"}]},{"title":"Hexo使用mathjax渲染公式","date":"2017-12-01T16:00:00.000Z","path":"2017/12/hexo-with-mathjax/","text":"最近有在博客中嵌入公式的需求，目前主要有两个数学公式渲染引擎mathjax和KaTeX，前者应用更广泛，支持的语法更全面，因此这里简述将mathjax整合到hexo。 1. 替换Markdown渲染器npm uninstall hexo-renderer-marked --save npm install hexo-renderer-kramed --save hexo-renderer-karmed渲染器fork自hexo-renderer-marked，对mathjax的支持更友好，特别是下划线处理(marked会优先将_之间的内容斜体转义) 2. 挂载mathjax脚本在主题layout/_partial/目录下添加mathjax.ejs: &lt;!-- mathjax config similar to math.stackexchange --&gt; &lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ tex2jax: { inlineMath: [ [&#39;$&#39;,&#39;$&#39;], [&quot;\\\\(&quot;,&quot;\\\\)&quot;] ], processEscapes: true } }); &lt;/script&gt; &lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Config({ tex2jax: { skipTags: [&#39;script&#39;, &#39;noscript&#39;, &#39;style&#39;, &#39;textarea&#39;, &#39;pre&#39;, &#39;code&#39;] } }); &lt;/script&gt; &lt;script type=&quot;text/x-mathjax-config&quot;&gt; MathJax.Hub.Queue(function() { var all = MathJax.Hub.getAllJax(), i; for(i=0; i &lt; all.length; i += 1) { all[i].SourceElement().parentNode.className += &#39; has-jax&#39;; } }); &lt;/script&gt; &lt;script type=&quot;text/javascript&quot; src=theme.cdn.mathjax + &quot;/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt; &lt;/script&gt; 如果用的是jade模板，则添加mathjax.jade: //mathjax config similar to math.stackexchange script(type=&quot;text/x-mathjax-config&quot;). MathJax.Hub.Config({ tex2jax: { inlineMath: [ [&#39;$&#39;,&#39;$&#39;], [&quot;\\\\(&quot;,&quot;\\\\)&quot;] ], displayMath: [ [&#39;$$&#39;,&#39;$$&#39;], [&quot;\\\\[&quot;,&quot;\\\\]&quot;] ], processEscapes: true } }); script(type=&quot;text/x-mathjax-config&quot;). MathJax.Hub.Config({ tex2jax: { skipTags: [&#39;script&#39;, &#39;noscript&#39;, &#39;style&#39;, &#39;textarea&#39;, &#39;pre&#39;, &#39;code&#39;] } }); script(async, type=&quot;text/javascript&quot;, src=theme.cdn.mathjax + &#39;/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&#39;) 在_partial/after_footer.ejs中添加: &lt;% if (page.mathjax){ %&gt; &lt;%- partial(&#39;mathjax&#39;) %&gt; &lt;% } %&gt; 如果是jade模板，则在_partial/after_footer.jade中添加: if page.mathjax == true include mathjax 3. 配置在主题_config.yml中配置mathjax cdn: cdn: mathjax: https://cdn.mathjax.org 当需要用到mathjax渲染器时，在文章头部添加mathjax:true: layout: post mathjax: true ... 只有添加该选项的文章才会加载mathjax渲染器。 4. 支持mathjax的Markdown编辑器: Qute 原生支持mathjax，界面有点Geek。 Macdown: Macdown原生不支持mathjax，在md文件中添加(注意https，Macdown为了安全，只会加载https的远程脚本): &lt;script type=&quot;text/javascript&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML&quot;&gt; MathJax.Hub.Config({ tex2jax: { inlineMath: [ [&#39;$&#39;,&#39;$&#39;], [&quot;\\\\(&quot;,&quot;\\\\)&quot;] ], displayMath: [ [&#39;$$&#39;,&#39;$$&#39;], [&quot;\\\\[&quot;,&quot;\\\\]&quot;] ],}, TeX: {equationNumbers: { autoNumber: &quot;AMS&quot; },Augment: { Definitions: { macros: { overbracket: [&#39;UnderOver&#39;,&#39;23B4&#39;,1], underbracket: [&#39;UnderOver&#39;,&#39;23B5&#39;,1], } }}}, }); &lt;/script&gt; 5. 示例:行内公式: $$ a+b=c $$ 行间公式: $$ \\left( \\begin{array}{ccc} a &amp; b &amp; c \\\\ d &amp; e &amp; f \\\\ g &amp; h &amp; i \\end{array} \\right) $$ 得到: 行内公式: $ a+b=c $ 行间公式: \\left( \\begin{array}{ccc} a & b & c \\\\ d & e & f \\\\ g & h & i \\end{array} \\right)具体mathjax语法，这里有一篇不错的博客。","tags":[{"name":"hexo","slug":"hexo","permalink":"http://wudaijun.com/tags/hexo/"}]},{"title":"docker 网络模式","date":"2017-11-20T16:00:00.000Z","path":"2017/11/docker-network/","text":"以Docker为平台部署服务器时，最应该理解透彻的便是网络配置。离上次学习，Docker网络又更新了不少内容，重新温习一下。 通过docker run的--network选项可配置容器的网络模式，Docker提供了多种网络工作模式，通过docker network ls可查看默认提供的三种网络模式: none, host, bridge none 模式不为Docker容器进行任何网络配置，容器将不能访问任何外部的路由(容器内部仍然有loopback接口)，需要手动为其配置网卡，指定IP等，否则与外部只能通过文件IO和标准输入输出交互，或通过docker attach 容器ID进入容器。 host 模式与宿主机共用网络栈，IP和端口，容器本身看起来就像是个普通的进程，它暴露的端口可直接通过宿主机访问。相比于bridge模式，host模式有显著的性能优势(因为走的是宿主机的网络栈，而不是docker deamon为容器虚拟的网络栈)。 bridge 模式默认网络模式，此模式下，容器有自己的独立的Network Namespace。简单来说，Docker在宿主机上虚拟了一个子网络，宿主机上所有容器均在这个子网络中获取IP，这个子网通过网桥挂在宿主机网络上。Docker通过NAT技术确保容器可与宿主机外部网络交互。 Docker服务默认会创建一个docker0网桥，并为网桥指定IP和子网掩码(通常为172.17.0.1/16)。当启动bridge模式的容器时，Docker Daemon利用veth pair技术，在宿主机上创建两个虚拟网络接口设备。veth pair技术保证无论哪一个veth收到报文，都将转发给另一方。veth pair的一端默认挂在docker0网桥上，另一端添加到容器的namespace下，并重命名为eth0，保证容器独享eth0，做到网络隔离。连接在同一个Docker网桥上的容器可以通过IP相互访问。如此实现了宿主机到容器，容器与容器之间的联通性。 关于网桥: 网桥(Bridges): 工作在数据链路层，连接多个端口，负责转发数据帧。网桥知道它的各个端口的数据链路协议(目前几乎都是以太网)，将来自一个端口的数据帧转发到其它所有端口。有多个端口的网桥又叫做交换机，目前这两个概念没有本质区别。 网桥可以用来连接不同的局域网(LAN)，按照最简单的方法，网桥会将某个端口收到的数据无脑转发给其它所有端口，这种泛洪(Flooding)算法效率过低，网桥依靠转发表来转发数据帧，通过自学习算法，记录各个Mac地址在对应哪个端口(转发表数据库)，辅之超时遗忘(Aging)和无环拓扑算法(Loop-Free Topology，典型地如Spanning Tree Protocol, STP)。 Linux网桥: Linux下网桥是一个虚拟设备，你可以通过命令创建它，并且为其挂载设备(物理或虚拟网卡)。可通过brctl命令来创建和Linux网桥。管理Linux bridge的具体用法参考: https://wiki.linuxfoundation.org/networking/bridge。 Docker网桥: Docker网桥通过Linux网桥实现，加上NAT, veth pair, 网络命名空间等技术，实现网络隔离和容器互联。可通过sudo docker network inspect bridge查看Docker网桥配置以及状态。 当容器需要和宿主机外部网络交互时，会在宿主机上分配一个可用端口，通过这个端口做SNAT转换(将容器IP:Port换为宿主机IP:Port)，再向外部网络发出请求。当外部响应到达时，Docker再根据这一层端口映射关系，将响应路由给容器IP:Port。 外部网络要访问容器Port0，需要先将Port0与宿主机Port1绑定(外部网络无法直接访问宿主机二级网络)，将宿主机IP:Port1暴露给外部网络，外部网络请求到达宿主机时，会进行DNAT转换(将宿主机IP:Port1换为容器IP:Port0)。 从实现上来讲，Docker的这种NAT(实际上是NATP，包含IP,Port的转换)规则，是Docker Daemon通过修改ipatables规则来实现的，ubuntu下可通过sudo ipatbles -t nat -L来查看和NAT相关的规则。 总之，Docker容器在bridge模式下不具有一个公有IP，即和宿主机的eth0不处于同一个网段。导致的结果是宿主机以外的世界不能直接和容器进行通信。虽然NAT模式经过中间处理实现了这一点，但是NAT模式仍然存在问题与不便，如：容器均需要在宿主机上竞争端口，容器内部服务的访问者需要使用服务发现获知服务的外部端口等。另外NAT模式会一定程度的影响网络传输效率。 默认设置下，Docker允许容器之间的互联，可通过--icc=false关闭容器互联(通过iptables DROP实现)，此时容器间相互访问只能通过--link选项链接容器来实现容器访问。—link 选项实际在链接容器的/etc/hosts中添加了一行被链接容器名和其IP的映射，并且会在被链接容器重启后更新该行(这样即使IP有变动也可以通过容器名正确连接)，此外还会添加一条针对两个容器允许连接的iptables规则。但Docker官方文档说--link已经是遗留的选项，更推荐自定义网络模式。 自定义模式自定义bridge网络即创建一个新的bridge网络，它的行为和默认的bridge网络类似，可通过docker network创建一个docker网桥: # 创建一个Docker网桥 命名为my_bridge docker network create --driver bridge my_bridge # 查看当前Docker支持的所有网络模式 (会多出一个bridge网络模式，名为my_bridge) docker network ls # 将容器挂在my_bridge网桥上 docker run --network=my_bridge -it --rm ubuntu # 查看 my_bridge网桥配置和状态 docker network inspect my_bridge # 移除 my_bridge网桥 docker network rm my_bridge 自定义网桥通常用于创建一个小的容器网络，在自定义网桥中，--link选项不被支持。 自定义overlay网络前面提到的网络模式，主要解决同一个主机上容器与容器，容器与主机，容器与外界的连接方案，如果要实现跨主机的容器与容器之间的通信方案，可以: 端口映射 将物理网卡挂在Docker网桥上，将容器和宿主机配置在同一网段下，见参考3 使用OpenvSwich网桥，如通过配置工具pipework，见参考4 在Docker1.9之后，可以使用原生解决方案Docker overlay 图片出处 overlay网络可以实现跨主机的容器VLAN，具体使用可以参考理解Docker跨多主机容器网络。 其它在使用Docker时，要注意平台之间实现的差异性，如[Docker For Mac]的实现和标准Docker规范有区别，Docker For Mac的Docker Daemon是运行于虚拟机(xhyve)中的(而不是像Linux上那样作为进程运行于宿主机)，因此Docker For Mac没有docker0网桥，不能实现host网络模式，host模式会使Container复用Daemon的网络栈(在xhyve虚拟机中)，而不是与Host主机网络栈，这样虽然其它容器仍然可通过xhyve网络栈进行交互，但却不是用的Host上的端口(在Host上无法访问)。bridge网络模式 -p 参数不受此影响，它能正常打开Host上的端口并映射到Container的对应Port。文档在这一点上并没有充分说明，容易踩坑。参考Docker文档 和 这篇帖子 参考bridge模式的详细实现可参考[Docker源码分析(七)：Docker Container网络(上)][] Docker源码分析(七)：Docker Container网络(上) Docker networking 理解Docker跨多主机容器网络 Docker网络详解及pipework源码解读与实践","tags":[{"name":"docker","slug":"docker","permalink":"http://wudaijun.com/tags/docker/"}]},{"title":"探讨服务端回合制战斗系统","date":"2017-09-25T16:00:00.000Z","path":"2017/09/ngs-battle/","text":"本文记录最近做战斗系统的一些心得和思考，由于我们的战斗系统是回合制的，与大部分回合制游戏一样，需要服务器计算战斗，客户端以战报的方式回放。这里探讨一下服务端战斗系统的设计思路，实现一个灵活，可配置，扩展性强的战斗系统。 战斗流程战斗地图是一个X*Y的矩阵，每个参与者(Fighter)初始位于其中一个格子上。战斗开始后，按照回合迭代，达到胜负条件或最大回合数则战斗结束。回合内，英雄按照出手顺序先后行动(Action)，英雄的Action包括移动，释放技能和普攻。 战斗流程是比较简明易懂的，整个战斗系统的难点在于多样的技能实现。每个英雄有N个技能，每个英雄可通过学习其它技能来实现不同的战斗效果，技能的效果和作用比较繁杂，例如： SkillA: 英雄每回合前3次伤害有50%机率使伤害翻倍(最多生效2次) SkillB: 诅咒一片区域（以一个敌方为中心的3*3格子）的敌人，使其攻击距离减1，持续两回合 SkillC: 分裂箭，英雄普攻可对多个敌人造成伤害 … 以下我们主要围绕灵活的技能系统为主要需求，讨论如何实现一个稳定，可扩展的战斗系统。 Event系统Event事件管理是实现复杂技能效果的基石，通过Event可以将复杂，易变的技能效果和核心流程解耦。对回合制游戏，典型地Event有回合开始/结束，英雄移动/普攻/受击/死亡等，EventMgr管理这些Event和它们的Handler，主要提供如下接口: // Go Code type EventArgs map[string]interface{} type EventHandler func(EventArgs) // 触发Event， 由战斗流程调用 如回合开始，英雄移动等 FireEvent(EventType, EventId, EventArgs) // 监听Event， ListenerId通常为Buff的唯一ID AddEventListener(EventType, EventId, ListenerId, EventHandler) // 注销ListenerId监听的所有Event，通常在Buff结束时调用 DelEventListener(ListenerId) EventMgr实际上是一个订阅者模式，战斗流程通过FireEvent发布事件，Buff订阅关心的事件并更新自己状态，在Buff结束时注销所有相关的事件监听。两者通过订阅者模式完成解耦，便于扩展。 技能系统技能分为主动技能(概率触发)和被动技能(相当于战斗开始立即触发)。技能的效果分为瞬时性和持续性两种，前者即像普通一样立即造成伤害(其实普攻也可以看做技能的一种)，后者指技能效果包含状态性，有自己的生命周期和状态更新，如Dot伤害，无法移动，沉默等，这个状态通常叫做Buff，关于技能和Buff的区别我的理解是，技能是Buff的容器，是静态的，Buff是技能触发后的实现效果，是动态的。瞬时伤害的技能也可以通过Buff实现，只不过这个Buff生命周期很短，在造成伤害后就消失了。关于Buff的详细实现我们放到后面，我们先看技能系统本身。 考虑到技能以后的扩展性和可维护性，对其尽可能做抽象是有必要的，抽象出公共的流程，将可变量配置化，可以提升系统稳定性和扩展性，也方便后期做测试。技能本身包含几个阶段：技能触发(概率触发，战斗开始触发)，目标选取(敌军/友军，一个/多个)，技能作用(造成伤害，挂接Buff)，前两个是可抽离到配置的，通过通用的技能触发器和目标选取脚本得到技能所需要的信息传给技能作用模块，由于技能作用效果的多样性，目前我们没有对技能作用进行抽象，是通过脚本各个实现的。 Buff系统技能的各种复杂效果都通过BUFF实现，每个Buff都挂于战场某个参与者(Fighter)上，当Fighter阵亡，其上所有的Buff都会被移除(包括Event关联)。BUFF系统是由一个基于战场事件(Event)的回调系统驱动，整个战场在战斗流程中不断抛出各种Event(如回合开始/结束，Fighter普攻/受击/释放技能，伤害结算等)，BUFF注册这些Event并更新自己Owner(Fighter)的状态，来实现灵活强大的技能效果。 1.Buff抽象 Start():Buff开始，即Buff启动脚本，负责初始化状态，注册BUFF的生命周期和相关Event等。 Update():Buff状态更新，实现Buff作用并更新Buff的状态，对于次数性BUFF(如前N次免伤)，可能调用N次Update，复杂的技能也可能有多个Update函数(关心不同的Event) Finish():Buff的正常结束，当BUff结束条件满足(比如Update了N次，或者持续了N回合)调用 Cancel():Buff被冲突(中断)时的处理 以上阐述的是Buff的行为抽象，而不是具体实现，在设计Buff时从这四个触法点思考，加上EventMgr注册回调机制，基本可以实现绝大部分各式的Buff效果。例如最开始提到的SkillA: 英雄每回合前3次伤害有50%机率使伤害翻倍(最多生效2次)，这是一个持续整场战斗的Buff(BuffA)，它注册两个Update Event: BEFORE_DAMAGE(英雄攻击伤害结算前): Update1 中判断如果本回合已触发次数小于2并且满足触发条件(50%概率)，则更新自己的状态(计数器+1)，并产生伤害翻倍子Buff(后面讨论)。 ROUND_START(每回合开始): Update2 中重置Buff状态(计数器)。 目前我们所讨论的Buff都是独立的，有自己生命周期的个体，通过Event注册回调与战斗主流程解耦。但实际上Buff之间是有相互关联的，主要分为三种：Buff属性作用，Buff冲突免疫关系和Buff生命周期，下面分别介绍。 2. Buff属性作用仍然是我们前面的SkillA技能，我们现在来看如何实现伤害翻倍这个Buff，该Buff是SkillA对应的Buff生成的子Buff，它应该被设计为可公用的伤害增加Buff，这个Buff的作用是影响伤害结算流程，按照我们之前的事件注册回调思路，我们可以注册伤害结算这个Event，接收当前算出的伤害，然后*200%并返回新伤害值。如BuffA提升10%伤害，BuffB增加20点真实伤害，BuffC降低20%的伤害，那么最终得到伤害为: (基础伤害*110%+20)*80%，这种方案默认公式计算的顺序与Buff挂载顺序一致，，而正确的伤害值应该为(基础伤害+20)*(1+10%-20%)，如果要处理这种优先级关系，需要遍历所有注册伤害结算Event的Handler，按照类型排序，再依次处理，如果一旦有同类Buff添加或移除，又要重新计算。这样公式与事件管理做到了一起，是不稳定的。 BuffA,BuffB,BuffC之所以会有复杂的公式计算，一个原因在于它们作用于同一属性的不同维度，BuffA,BuffC作用于伤害值的比例增加这一维度，而BuffB作用于伤害值的绝对值增长这一维度，我们可以将它们分开，作为Fighter两个独虚拟属性ATTR_DAMAGE_ADD, ATTR_DAMAGE_MUL来维护，允许Buff对其修改，但此时的修改是只有加减关系的，避免了优先级的问题，在伤害结算时，通过公式计算: (基础伤害+ATTR_DMAGE_ADD)*ATTR_DAMAGE_MUL即可。 整个交互流程为: 战斗流程抛出事件 -&gt; 事件系统 -&gt;Buff系统 -&gt; Fighter属性维度 战斗流程获取属性 -&gt; 属性系统 -&gt; 公式计算 -&gt; Fighter属性维度 通过属性系统和事件系统，将战斗流程和Buff系统解耦，将组件职责降到最小，方便测试和扩展。 比如沉默，眩晕等效果，如果没有虚拟属性，沉默Buff会注册EVENT_BEFORE_SKILL(Fighter释放技能前)这个Event，并且返回false来告知战斗系统它当前不能释放技能。同样，眩晕Buff会注册Fighter EVENT_BEFORE_MOVE, EVENT_BEFORE_ATTACK, EVENT_BEFORE_SKILL三个Event来实现眩晕效果，一来整个战斗流程每次都要合并各类Event的各种返回值(并且EventHandler得不到统一的接口抽象)，效率低下，二来战斗流程不应该依赖外部EventHandler的实现，它只关心值本身(能否移动，能否施法等)，因此虚拟属性本身实际上起一个依赖倒置的作用。如果使用虚拟属性，那么沉默Buff会在ATTR_FORBIDEN_SKILL这个属性上+1，眩晕同理，这样战斗流程在Fighter尝试施放技能时，获取Fighter的ATTR_FORBIDEN_SKILL属性，如果&gt;0，则不能施法。 Buff通过属性来影响战斗流程，一方面解耦了战斗流程和Buff之间的依赖，另一方面也减轻了Buff与Buff之间的依赖，并且通过属性来固化战斗系统不易变的部分，可以减轻系统复杂度，易于调试。 3. Buff作用链不是所有的Buff都可以通过属性来完成解耦，比如护盾效果，A Buff 加200护盾，B Buff 加300护盾，那么 Fighter 护盾属性为500，此时战斗伤害结算流程扣除400护盾值，那么这个值应该从 A 减还是 B 减，战斗流程并不知晓，需要护盾Buff自己来维护(并且在护盾为0时结束Buff)，并且护盾还有各种类型(物理护盾，魔法护盾等)。属性系统在这里并不适用，因此，我们需要一个Buff作用链来完成护盾的结算流程。比如受击400点魔法伤害，战斗系统抛出 EVENT_FLOW_SHIELD 事件，传入400以及伤害类型，A Buff 收到后判断伤害类型，减免200点伤害(并结束自身)，返回400-200=200，B Buff 伤害类型不满足，返回200，战斗流程得到返回值200，继续后续处理。 到这里，可能你会问：为什么不将沉默，眩晕这些效果也通过 Buff作用链来完成？实现上是可以的，每次 Fighter移动/攻击/释放技能前，抛出对应事件，检查其返回值，决定是否执行操移动/攻击等。但就我的设计理念而言，能够通过虚拟属性固化的效果尽量固化，对系统复杂度和效率都有好处。 Buff 作用链和 Buff 事件处理的机制是一样的，只不过前者关心返回值，后者不关心返回值。可公用EventMgr来处理，通过引用类型的EventArgs来更新和返回。 4.Buff相互关系-Buff 冲突: 即该BUFF生效时，已有的哪些Buff会失效，如一些清除负面状态的Buff-Buff 免疫: 即该BUFF生效时，后面来的哪些BUFF不能生效，如BKB免疫眩晕-Buff 叠加: 两种同类增益或减益BUFF同时生效时，按照某个规则进行BUFF效果重新计算生成 Buff的冲突免疫关系实际上是Buff作用效果的一部分，但是一个可抽象和配置化的流程，在挂载Buff时统一处理。至于Buff叠加，在Buff B的Start节点中，判断是否有指定Buff A存在，如果存在，修正BuffB的效果(或移除已有BuffA)，是个特例流程，做到Buff脚本里面就行了。 至此，我们通过属性维度和公式计算来避免了作用于同一个属性的Buff的顺序依赖，通过公用流程来处理Buff的免疫和冲突，仅针对Buff叠加这类少见的特殊作用进行特例化处理，这样最大程度的提升了Buff的扩展性，Buff可以独立实现，Buff的关系可通过配置表配置，新加一个Buff无需修改已有的Buff系统，对其它模块的影响也降到最小。 5.Buff生命周期为了达成复用，我们会通过子Buff的概念来实现一些复杂的Buff，SkillA的BuffA，它维护自己的状态，并在条件满足时，产生伤害翻倍这个子Buff，父子Buff的生命周期关系大致有三种: 完全独立，创建完成之后即不再相互引用 父Buff结束时，子Buff随之结束 父Buff通过内部状态控制子Buff的生命周期 对战斗系统来说，理想情况下，每个Buff应该自己管理自己的生命周期，这样状态内聚在Buff本身，更好地满足正交性和复用性。并且Buff的生命周期耦合容易引发状态错误，如子Buff由于Buff冲突被移除时，父Buff可能并不知晓，当父Buff结束子Buff时会再次触发Buff Finish或Cancel操作。 因此我们应该尽可能将父子Buff关系弱化(向第一种关系靠齐)，将Buff生命周期独立: 将Buff的生命周期作为创建子Buff的参数传入，如一个持续两回合的属性Buff，则将”持续两回合”这个周期以事件类型(回合结束), 事件ID(0), 触发次数(2)传入 用子Buff监听”父Buff移除”事件的方式来将关系2转换为关系1 用唯一事件ID来完成父子Buff的特例的事件交互，将部分关系3转换为关系1 由于更复杂的状态控制，比如Buff的结束机制可能不止一种，所以想要完全只保留关系1的父子Buff是比较难的。对于这类少数父子Buff，可考虑特例化实现这个子Buff，比如吸血Buff独立实现可能会比复用恢复Buff更好，如果以上方案都不能很好解决，最后再考虑将其生命周期完全交由父Buff控制(子Buff本身无Event状态)。 属性系统属性系统针对Fighter的各种属性进行管理，属性系统包括K-V Map和公式计算两部分，前面我们讲到通过虚拟属性来完成Buff与战斗流程间的解耦，那么K-V Map的Key有如下几种: 固定属性: 当前不受Buff影响的属性，无需公式计算直接获取即可。如Fighter当前血量，位置信息等 基础属性: 受Buff影响的属性的基础值，如Fighter进入战斗时的初始攻击力，防御力等，基础属性在战斗过程中不变 Buff属性: 基础属性的可变维度，由各类Buff修改，如攻击力增加值(绝对值)，防御力加成(百分比) 虚拟属性: 如禁足，沉默，伤害加成等，这些属性原本Fighter上面没有，属于战斗系统需要，也由Buff修改 我将属性管理器K-V Map保存的”属性”称为属性维度，它们是Buff操作属性的最小粒度，每个属性维度都是纯加减运算，不受Buff先后顺序的影响。对最终属性的计算，由公式计算系统，比如: 最终攻击力 = (攻击力基础值+攻击力增加值)*(1+攻击力加成值)，战斗流程关心Fighter最终攻击力，Buff系统关心其影响的某个属性维度(如攻击力增加值或攻击力加成值)，中间的这一块就是公式计算，将公式计算抽象出来的好处是公式系统可独立变化，甚至可以将公式配置化。属性的计算过程对战斗流程来说是透明的，这给属性维度和公式计算的变更带来的很大的灵活度。 配置框架评估一套配置框架好坏不能简单从可配置性这一点来看，一个完全可配置技能效果，做到通过配置即可添加一些简单技能的配置框架不是不能实现，但开发和维护成本过高，对策划的要求，出错的可能性也更高。因此在设计配置框架时，要结合项目需求，在开发效率，可维护性，可扩展性等方面作出权衡。 简单提一下我们目前用的配置方案: 技能基础表: 技能ID 技能类型 技能距离 技能目标选取器 目标数量 技能BuffIds 技能描述 1 主动 同攻击距离 敌人 1 [1001] 对目标造成Args[0]的伤害，并眩晕Args[1]回合(Buff[0]) 技能成长表，即技能的Args表: 技能ID 技能等级 技能参数 1 1 [0.8,1] 1 2 [0.9,1] Buff表: BuffId 描述 所属Tags 冲突Tags 免疫Tags 冲突自身 免疫自身 1001 眩晕 [控制] [] [] false false 技能根据技能类型，距离和目标选取器以及目标数量，通过通用目标选取流程得到目标，然后传入技能作用脚本，技能作用脚本由程序维护，通过技能描述使用技能参数(来自技能成长表)和技能BuffIds(传入Buff作用脚本)，由于多个Buff可能由同一个Buff脚本实现(如攻击力提高，防御力降低)，因此Buff脚本需要外部传入BuffId来获取冲突免疫关系，对程序来说，BuffId是透明的，它只代表一类冲突免疫关系，对策划来讲，Buff脚本是透明的，他只关心Buff相互关系(如A技能的攻击力提升与B技能的攻击力提升不能同时存在)，至于技能和技能参数以及BuffID的关联本身，是不常变的，因此直接硬编码映射。 Buff的配置方案有两种，一是通过BuffID来配置冲突免疫关系，这种方案灵活性高，但扩展性和维护性差。另一种方案是通过BuffTag，每个Buff可配置自己的Tag(如增益，减益，控制)，根据这些Tag来控制冲突免疫关系，免疫负面状态的Buff对应的配置中免疫Tag为[减益，控制]。这种方案与BuffId无关(免疫自身和冲突自身需单独配置)，维护性和扩展性更高。 战报系统战斗跑在服务器，客户端需要通过战报进行战斗回放，那么战报就要包含整个战斗的详细过程，每回合那个英雄放了什么技能，攻击了谁，等等，客户端根据这些信息”拼凑”出战斗动画。战报的每一”桢”应该为一个最小粒度的事件，如A对B发起了普通攻击应该是: 1. A对B发起普攻，2. B损失了50HP，中间还可能穿插反击和其它Buff效果，客户端在”按桢表现”的时候，还需要一些关联，如B是因为A的普攻而掉血，因此实际上更好的格式为: B由于A的普攻损失了50HP，为了协议扩展性，我们会将这些事件格式统筹起来: 事件类型 FighterId 事件参数 解释 回合开始 0 [1] 第一回合开始 发起普攻 1 [2] Fighter1向Fighter2发起普攻 受到伤害 2 [1,0,50] Fighter2由于Fighter1的普攻(BuffId0)损失了50HP 每个事件都有自己的参数意义，这部分和客户端约定即可。这里的事件类型和之前提到的战斗系统的EventMgr很相似，很多触发点也一样，只是是针对各种Buff，一个是针对客户端表现。 至于事件参数的类型，最常见的可能有整型，浮点数，字符串，在protobuf协议里面可以直接通过复合结构定义: message Elem { sint32 type = 1; // 1: intv 2: fltv 3: strv sint32 intv = 2; float fltv = 3; string strv = 4; } 这种方案很丑陋，但在protobuf3中，由于optional字段默认值不发送和sint32的变长编码，实际发送一个type=1,intv=20的Elem只会占用四个字节(两个字段的内容和编号各占 一个字节)，因此还是比较实用的。参考过protobuf的oneof，不是很好用，对repeat和map等复合结构的支持不好。 技能效果扩展1. 召唤物SkillB: 诅咒一片区域（以一个敌方为中心的3*3格子）的敌人，使其攻击距离减1持续两回合 该技能可用前面介绍的已有机制实现: 技能目标选取规则中，配置攻击范围内一个敌人，技能作用脚本中，获取到该目标周围九宫格所有的敌人，对它们施加持续两回合的属性子Buff(攻击距离-1，由子Buff自身管理生命周期)。 现在考虑SkillB的诅咒区域如果有状态和AI(如存在两回合，每回合跟随施法者移动)，则实现上更为复杂: 监控所有敌人的移动，当其进入区域时，添加Buff，出去时，移除Buff 当自己移动时，根据前后状态更新敌人身上的Buff 两回合后，结束自身 那如果是召唤一个宠物，并且宠物有血量，可移动，攻击和被攻击呢，是的，答案是以”Fighter”来实现召唤物，这里的Fighter是一个更广泛的概念，它只是一系列接口，如移动/攻击/属性变更等，这样所有能够通过Fighter实现的，技能都可以实现，也算是终极方案了。 2. 行为属性SkillC: 分裂箭，英雄普攻可对多个敌人造成伤害 这类技能的特性是会影响已有技能或其它行为，比如改变普攻流程，移动方式等，这种通常很难用属性系统去做，解决方案是将Fighter的行为(普攻/技能/移动等)抽离为可插拨模块(也可理解为行为属性)，初始每个英雄的行为属性被赋默认值，技能可以更改这些行为(如分裂箭可更换普攻行为)，实现更高级别的抽象，战斗流程根据行为类型和次序(如移动/技能/普攻)取出并执行这些行为，行为属性也是召唤物Fighter实现的基础。 3. 全局BuffSkillD: 腐蚀一片区域，进入区域的敌人受到持续伤害，区域存在2回合，并且不会随施法者死亡消失 由于其简单，用Fighter实现过于重度，由于其独立的生命周期，不能以普通Buff的形式存在(会随Fighter死亡消失)，那么可以考虑用全局Buff，全局Buff挂在战场上，介于Fighter和普通Buff之间，适合实现一些简单，全局的效果，如天气效果。 总结到这里，战斗系统的主要组件就已经介绍得差不多了，总结起来，核心思路是将相对稳定的核心战斗流程和相对动态的技能Buff扩展隔离开来，战斗流程通过事件系统来解耦外部Buff脚本，Buff通过属性系统(包括行为属性)来反馈到战斗流程。在构建的过程中，还要时刻关注到哪些是易变的，比如Buff 冲突免疫关系，伤害计算公式等，将其单独抽出来，封装成模块甚至抽离到配置，尽量将功能做到模块化，离线化，方便模块的扩展和测试。 将战斗流程”固化”下来，不要交给Buff系统去任意递归迭代，这种思路适用于回合制这类战斗流程相对固定的情形。另一种思路，是”去流程化”，将流程做到Buff中，比如将移动作为一个Buff，那么”禁足”的效果可以直接通过Buff免疫来实现: 如果Fighter已经有禁足Buff，则移动Buff不能挂上去，达成不能移动的效果。沉默和缴械效果也类似。这种思路更为灵活，但相对更复杂和难以调试。通常我们将通用/固定的行为作为流程，特例/易变的流程作为Buff。 在战斗系统设计中，很多方案都不是绝对的左或者是右，比如普攻是否应该当做特殊技能处理(这样能很方便实现特殊的普攻效果，如分裂箭)？哪些属于流程，哪些属于Buff？哪些效果以Buff实现，哪些效果以Fighter实现？哪些可抽到配置文件，哪些直接写在代码里等等，在实际决策中，往往都是根据实际情况(开发效率，GD需求，扩展性，维护性等)在中间选一个合适点，并且尽可能在细节上封装解耦，以便之后能根据变化进行调整。","tags":[{"name":"gameserver","slug":"gameserver","permalink":"http://wudaijun.com/tags/gameserver/"}]},{"title":"用context库规范化Go的异步调用","date":"2017-08-18T16:00:00.000Z","path":"2017/08/go-conetxt-usage/","text":"常见并发模型之前对比过Go和Erlang的并发模型，提到了Go的优势在于流控，下面列举几种常见的流控: Ping-Pong这通常针对于两个goroutine之间进行简单的数据交互和协作，我们常用的RPC也属于此类，通过channel的类型可以灵活实现交互方式: 同步单工: 单个双向非缓冲channel 同步双工: 多个单向非缓冲channel 异步双工: 多个单向缓冲channel 流水线流水线如其词语，goroutine是”流水线工人”，channel则为”流水线”，衔接不同的goroutine的输入输出，每个goroutine有一个输入(inbound)channel和输出(outbound)channel: // 以下定义一个流水线工人 用于将inbound channel中数字求平方并放入outbound channel func sq(in &lt;-chan int) &lt;-chan int { out := make(chan int) go func() { for n := range in { out &lt;- n * n } close(out) }() return out } 流水线goroutine有一些特质：它负责创建并关闭channel(在完成自己的工作后)，这样外部调用无需关心channel的创建和关闭，当channel被关闭，它的下游goroutine会读出零值的数据。我们还可以用链式调用来组装流水线： sq(sq(sq(ch))) 在实际应用中，如DB读写，网络读写等外部阻塞操作通常都放到单独的流水线去做，下游主goroutine可以灵活处理IO结果(如通过select完成IO复用)。 扇入扇出流水线工作通常是一对一的”工作对接”，通过select可以达成IO复用，比如GS同时处理网络消息，RPC调用，Timer消息等，这其实就是简单的扇入模型，扇出模型也比较常见，比如在对一些无状态的任务做分发时，可以让多个goroutine处理一个channel任务队列上的数据，最大程度地提升处理效率。 上面三个模式是应用最常用到的，因此不再举例具体说明，Go并发可视化这篇文章很好地归纳和总结了这些模型，推荐一读。 交互规范上面只所以提出这三种模型主要是为了导出接下来的问题，当用到多个goroutine时，如何协调它们的工作： 如何正确关闭其它goroutine这类问题的通常情形是：当某个goroutine遇到异常或错误，需要退出时，如何通知其它goroutine，或者当服务器需要停止时，如何正常终止整个并发结构，为了简化处理问题模型，以流水线模型为例，在正常情况下，它们会按照正常的流程结束并关闭channel(上游关闭channel，下游range停止迭代，如此反复)，但当某个下游的goroutine遇到错误需要退出，上游是不知道的，它会将channel写满阻塞，channel内存和函数栈内存将导致内存泄露，在常规处理方案中，我们会使用一个done channel来灵活地通知和协调其它goroutine，通过向done channel写入数据(需要知道要关闭多少个goroutine)或关闭channel(所有的读取者都会收到零值，range会停止迭代)。 如何处理请求超时至于超时和请求放弃，通常我们可以通过select来实现单次请求的超时，比如 A -&gt; B -&gt; C 的Ping-Pong异步调用链，我们可以在A中select设置超时，然后在B调用C时也设置超时，这种机制存在如下问题: 每次请求链中的单次调用都要启一个timer goroutine 调用链中的某个环节，并不知道上层设置的超时还有多少，比如B调用C时，如果发现A设置的超时剩余时间不足1ms，可以放弃调用C，直接返回 A-&gt;B的超时可能先于B-&gt;C的超时发生，从而导致其它问题 如何安全放弃异步请求这个问题可以理解为如何提前结束某次异步调用，接上面提到的A-&gt;B-&gt;C调用链，如果A此时遇到了其它问题，需要提前结束整个调用链(如)，B是不知道的，A和B之间数据交互channel和done channel，没有针对某个请求的取消channel，尽管大部分时候不会遇到这种需求，但针对某个请求的协同机制是缺失的，还需要另行设计。 如何保存异步调用上下文异步调用通常会有上下文，这个上下文不只指调用参数，还包括回调处理参数(非处理结果)，请求相关上下文(如当前时间)等，这类数据从设计上可以通过包含在请求中，或者extern local value，或者每次请求的session mgr来解决，但并不通用，需要开发者自行维护。 使用context以上几个问题并不限于Go，而是异步交互会遇到的普遍问题，只是在Go应用和各类库会大量用到goroutine，所以这类问题比较突出。针对这些问题，Go的内部库(尤其是net,grpc等内部有流水线操作的库)作者开发了context(golang.org/x/net/context)包，用于简化单个请求在多个goroutie的请求域(request-scoped)数据，它提供了: 请求的超时机制 请求的取消机制 请求的上下文存取接口 goroutine并发访问安全性 context以组件的方式提供超时(WithTimeout/WithDeadline)，取消(WithCancel)和K-V(WithValue)存取功能，每次调用WithXXX都将基于当前的context(Background为根Context)继承一个Context,一旦父Context被取消，其子Context都会被取消，应用可通过&lt;-context.Done()和context.Err()来判断当前context是否结束和结束的原因(超时/取消)。 比如针对我们前面的”sq流水线工人”，我们可以通过context让它知道当前流水线的状态，并及时终止: 1234567891011121314func sq(ctx context.Context, in &lt;-chan int) out &lt;-chan int&#123; out := make(chan int) go func() &#123; for n := range in &#123; select&#123; case &lt;-ctx.Done(): // 当前流水线被终止 close(out) return ctx.Err() // 终止原因: DeadlineExceeded or Canceled case out &lt;- n * n: &#125; close(out) &#125;() return out&#125; 我们可以将context在goroutine之间传递，并且针对当前调用通过WithXXX创建子context，设置新的超时，请求上下文等，一旦请求链被取消或超时，context的done channel会被关闭，当前context的所有&lt;-ctx.Done()操作都会返回，并且所有当前context的子context会以相同原因终止。 比如在A-&gt;B-&gt;C中，B基于A的context通过WithTimeout或WithValue创建子context，子Context的超时和上下文都可以独立于父context(但如果子context设置超时大于父context剩余时间，将不会创建timer)，通过context库内部的继承体系来完成对应用层调用链的记录，并执行链式的超时和取消。 关于context的进一步了解可参考Go语言并发模型：使用 context，也可直接阅读源码，实现也比较简单，单文件不到300行代码，但本身的意义却是重大的，go的很多异步库(如net,grpc,etcd等)都用到了这个模块，context正在逐渐成为异步库的API规范，我们也可以从context这个库中得到一些启发，适当地用在自己的项目中。","tags":[{"name":"go","slug":"go","permalink":"http://wudaijun.com/tags/go/"}]},{"title":"Go vs Erlang","date":"2017-05-30T16:00:00.000Z","path":"2017/05/go-vs-erlang/","text":"源于从Erlang到Go的一些思维碰撞，就像当初从C++到Erlang一样，整理下来记于此。 ActorActor模型，又叫参与者模型，其”一切皆参与者(actor)”的理念与面向对象编程的“一切皆是对象”类似，但是面向对象编程中对象的交互通常是顺序执行的(占用的是调用方的时间片，是否并发由调用方决定)，而Actor模型中actor的交互是并行执行的(不占用调用方的时间片，是否并发由自己决定)。 在Actor模型中，actor执行体是第一类对象，每个actor都有自己的ID(类比人的身份证)，可以被传递。actor的交互通过发送消息来完成，每个actor都有一个通信信箱(mailbox，本质上是FIFO消息队列)，用于保存已经收到但尚未被处理的消息。actorA要向actorB发消息，只需持有actorB ID，发送的消息将被立即Push到actorB的消息信箱尾部，然后返回。因此Actor的通信原语是异步的。 从actor自身来说，它的行为模式可简化为: 发送消息给其它的actor 接收并处理消息，更新自己的状态 创建其它的actor 一个好的Actor模型实现的设计目标: 调度器: 实现actor的公平调度 容错性: 具备良好的容错性和完善错误处理机制 扩展性: 屏蔽actor通信细节，统一本地actor和远程actor的通信方式，进而提供分布式支持 热更新? (还没弄清楚热更新和Actor模型，函数式范式的关联性) 在Actor模型上，Erlang已经耕耘三十余载，以上提到的各个方面都有非常出色的表现，其OTP整合了在Actor模型上的最佳实践，是Actor模型的标杆。 CSP顺序通信进程(Communicating sequential processes，CSP)和Actor模型一样，都由独立的，并发的执行实体(process)构成，执行实体间通过消息进行通信。但CSP模型并不关注实体本身，而关注发送消息使用的通道(channel)，在CSP中，channel是第一类对象，process只管向channel写入或读取消息，并不知道也不关心channel的另一端是谁在处理。channel和process是解耦的，可以单独创建和读写，一个process可以读写(订阅)个channel，同样一个channel也可被多个process读写(订阅)。 对每个process来说： 从命名channel取出并处理消息 向命名channel写入消息 创建新的process Go语言并没有完全实现CSP理论(参见知乎讨论)，只提取了CSP的process和channel的概念为并发提供理论支持。目前Go已经是CSP的代表性语言。 CSP vs Actor 相同的宗旨：”不要通过共享内存来通信，而应该通过通信来共享内存” 两者都有独立的，并发执行的通信实体 Actor第一类对象为执行实体(actor)，CSP第一类对象为通信介质(channel) Actor中实体和通信介质是紧耦合的，一个Actor持有一个Mailbox，而CSP中process和channel是解耦的，没有从属关系。从这一层来说，CSP更加灵活 Actor模型中actor是主体，mailbox是匿名的，CSP模型中channel是主体，process是匿名的。从这一层来说，由于Actor不关心通信介质，底层通信对应用层是透明的。因此在分布式和容错方面更有优势 Go vs Erlang 以上 CSP vs Actor 均实现了语言级的coroutine，在阻塞时能自动让出调度资源，在可执行时重新接受调度 go的channel是有容量限制的，因此只能一定程度地异步(本质上仍然是同步的)，erlang的mailbox是无限制的(也带来了消息队列膨胀的风险)，并且erlang并不保证消息是否能到达和被正确处理(但保证消息顺序)，是纯粹的异步语义，actor之间做到完全解耦，奠定其在分布式和容错方面的基础 erlang/otp在actor上扩展了分布式(支持异质节点)，热更和高容错，go在这些方面还有一段路要走(受限于channel，想要在语言级别支持分布式是比较困难的) go在消息流控上要做得更好，因为channel的两个特性: 有容量限制并独立于goroutine存在。前者可以控制消息流量并反馈消息处理进度，后者让goroutine本身有更高的处理灵活性。典型的应用场景是扇入扇出，Boss-Worker等。相比go，erlang进程总是被动低处理消息，如果要做流控，需要自己做消息进度反馈和队列控制，灵活性要差很多。另外一个例子就是erlang的receive操作需要遍历消息队列(参考)，而如果用go做同步调用，通过单独的channel来做则更优雅高效 Actor in Go在用Go写GS框架时，不自觉地会将goroutine封装为actor来使用: GS的执行实体(如玩家，公会)的逻辑具备强状态和功能聚合性，不易拆分，因此通常是一个实体一个goroutine 实体接收的逻辑消息具备弱优先级，高顺序性的特点，因此通常实体只会暴露一个Channel与其它实体交互(结合go的interface{}很容易统一channel类型)，这个channel称为RPC channel，它就像这个goroutine的ID，几乎所有逻辑goroutine之间通过它进行交互 除此之外，实体还有一些特殊的channel，如定时器，外部命令等。实体goroutine对这些channel执行select操作，读出消息进行处理 加上goroutine的状态数据之后，此时的goroutine的行为与actor相似：接收消息(多个消息源)，处理消息，更新状态数据，向其它goroutine发送消息(通过RPC channel) 到目前为止，goroutine和channel解耦的优势并未体现出来，我认为主要的原因仍然是GS执行实体的强状态性和对异步交互流程的顺序性导致的。 在研究这个问题的过程中，发现已经有人已经用go实现了Actor模型: https://github.com/AsynkronIT/protoactor-go。 支持分布式，甚至supervisor，整体思想和用法和erlang非常像，真是有种他山逢知音的感觉。:) 参考： http://jolestar.com/parallel-programming-model-thread-goroutine-actor/ https://www.zhihu.com/question/26192499","tags":[{"name":"erlang","slug":"erlang","permalink":"http://wudaijun.com/tags/erlang/"},{"name":"go","slug":"go","permalink":"http://wudaijun.com/tags/go/"}]},{"title":"Erlang+Lua的一次重构","date":"2017-03-21T16:00:00.000Z","path":"2017/03/erlang-lua-reconstruction/","text":"目前所在的项目基于erlang cluster搭建框架，再接入lua用于写逻辑。由于之前有一些erlang+lua的开发经验，因此着手项目的重构和优化，过程中一些体会，记录于此。 先简述一下项目架构，erlang做集群，网络层，节点交互，DB交互等，lua层只写逻辑。一个erlang的Actor持有一个luastate，为了加速erlang和lua之间的交互效率： 将逻辑数据置于lua中而不是erlang中，在落地时，以二进制格式丢给erlang进行DB操作 以lual_ref和msgid等方式，尽量用整数代理字符串 erlang和lua异步运行，lua跑在原生线程池中，这在这篇博文中介绍过 除了这些，还需要注意lua的沙盒环境管理，错误处理，热更新等，这里不再详述。就目前这种结构而言，还有一些缺陷： 原子线程池忙碌可能导致的erlang虚拟机假死，需要保证原生线程池最多占用的核数不超过erlang虚拟机能使用的核数 lua state本身带来的不稳定性，特别是内存，在Actor过多时将会非常明显 第二点，也是目前我们遇到的最棘手的问题，我们知道，在lua中，模块，函数，均是一个闭包，闭包包含函数和外部环境(UpValue，ENV等)，因此在lua中，每个lua state都完整包含加载的所有模块和函数，并且很难共享。我们项目通过一个share lua state完成了对配置表这类静态数据的共享(跨系统进程级的共享可参考云风blog)，但本身逻辑代码占用内存仍然很大，随着逻辑和功能模块的增加，基本一个lua state加载完模块什么也不做，会占用6-7M内存。意味着如果一个玩家一个lua state，那么一台16G内存的服务器，基本只能容纳2000个玩家，内存吃紧，而CPU过剩。因此本次重构也只要针对这个问题。 之前项目组曾针对玩家进行了优化，将主城位于一个岛的玩家归位一组，再将岛按照%M的方式放到M个lua state容器上，这样得到一个复杂的，三层逻辑的lua state。针对玩家这一块的内存占用确实大大减少了，但调试难度也提升了，并且扩展性不好，不能将这种容器扩展到其它service(如Union)上。 按照系统本身的理想设计，一个service(player, union)对应一个lua state，由一个erlang process代理这个lua state，并且通过cluster注册/共享这个service的状态信息。但由于lua state的内存占用，不能再奢侈地将service和lua state 1:1调配，多service在逻辑代码中共用一个lua state已经无可避免，我们可以简单将整个系统分为几个层级， service lua state erlang process cluster N 1 ? ? 因此有以下几种可能的方案： N 1 N N：每个service对应一个erlang process，多个erlang process将代理同一个lua state，这就需要lua state可以”被并发”，也就是同一个lua state只能绑定一个原生线程池上执行，这一点是可以实现的。这种方案在erlang层会获得更好的并发性能，并且cluster层语义不变。 N 1 1 N：一个erlang process作为container的概念代理一个lua state，容纳N个service，并且将service和erlang process的映射关系写入cluster，cluster层对外提供的语义不变，但service的actor属性被弱化，service的一致性状态是个问题。 N 1 1 1：与上种方案类似，只不过将service到container的映射通过算法算出来，而不写入cluster，container本身被编号（编号时，可考虑将serverid编入，这样开新服有一定的扩展性，PS: 一致性哈希方案不适用于游戏这类强状态逻辑），某个service将始终分配在指定container上。这种方案减少了cluster负担，并且减少了service不一致性的BUG。但由于container有状态，在每次系统启动后，service和container的映射关系就确定了，因此整个集群的可伸缩性降低了。 经过几番讨论，我们最终选择了第三个方案，虽然个人认为这类固定分配的方案，与分布式的理念是相悖的，但目前稳定性和一致性才是首要目标。由于采用计算而不是通过mnesia保存映射关系，mnesia的性能和系统一致性得到了提升。本次重构在某些方面与我上一个项目针对cluster的优化有点相似，一个对系统服务进行横向切割，另一个则纵向切割，前者的初衷是为了更好地交互效率，后者则是处于对lua state资源的复用，两者都降低了系统的可伸缩性，得到了”一个更大粒度”的service。 整个重构过程中，有几点感触： erlang和lua结合本身不是一种好的解决方案，或者说，erlang接入其它语言写逻辑都不合适，异质化的系统会打乱erlang本身的调度(不管通过nif还是线程池)，并且给整个系统带来不稳定性(CPU，内存)。另外，接入其它语言可能破坏erlang的原子语义和并发性。拿lua来说，原生线程池会和erlang调度线程抢占CPU并且很难管控，加之lua有自己的GC，因此在内存和CPU这两块关键资源上，erlang失去了控制权，给系统带来不稳定性。再加之lua state的内存占用以及lua state不支持并发，你可能要花更多的时间来调整系统结构，最终得到一个相对稳定的系统。如果处理得不好，用erlang做底层的可靠性和并发性将荡然无存。 系统设计，是一个不断根据当前情况取舍的过程，想要一步到位是不可能的。简单，可控，开发效率高才是主要指标，才能最大程度地适应各种变化，快速响应需求。","tags":[{"name":"lua","slug":"lua","permalink":"http://wudaijun.com/tags/lua/"},{"name":"erlang","slug":"erlang","permalink":"http://wudaijun.com/tags/erlang/"}]},{"title":"Lua 闭包 环境 包管理","date":"2017-02-22T16:00:00.000Z","path":"2017/02/lua-notes/","text":"Variables 访问一个不存在的全局变量得到nil 释放一个全局变量只需将其赋值为nil，效果与未定义该变量一样 Lua 中的变量全是全局变量，那怕是语句块或是函数里，除非用 local 显式声明为局部变量 局部变量比全局变量访问更快 Functions1. 基本特性 多参数/返回值匹配：多余忽略，缺少用nil补足 可变参数：arg，table.pack，table.unpack 命名参数：参数的非顺序填充方式 正确处理尾调用：Lua能够高效正确处理尾调用，而不会导致栈溢出 2. 第一类函数函数是第一类值，函数可以像其它值（string, number）样用于赋给变量，作为函数参数或返回值。函数定义实际上是一个赋值语句，将类型为function的变量赋给一个变量。 123function foo.bar (x) return 2*x end-- 等价于foo.bar = function (x) return 2*x end 从这个角度来看，自然，与变量一样，Lua有全局函数和局部函数之分。 3. 词法闭包词法闭包是指当在一个函数内部嵌套定义另一个函数时，内部函数体可以访问到外部函数的局部变量。 12345678910111213141516171819202122function newCounter() local i = 0 return function() -- anonymous function i = i + 1 return i endendc1 = newCounter()print(c1()) --&gt; 1print(c1()) --&gt; 2c2 = newCounter()print(c2()) --&gt; 1-- 打印c1所有的upvalue 输出: ilocal i=1local up = debug.getupvalue(c1, i)while(up ~= nil) do print(up, \" \") i = i+1 up = debug.getupvalue(c1, i)endprint(c1, c2) -- function: 0x7f8df1d02100 function: 0x7f8df1d02160 这种情况下，我们称i为匿名函数的外部局部变量(external local variable)或upvalue。在这里，newCounter函数返回了一个闭包(closure)。闭包是指一个函数和它的upvalues，闭包机制保证了即使upvalue已经超出了其作用域(newCounter返回)，仍然能正确被闭包函数引用而不会释放(由Lua GC管理)。在上例中，我们说c1和c2是建立在同一个函数上，但作用于同一个局部变量(i)不同实例的两个不同的闭包。 通过打印的upvalues可以看到，只有被闭包函数引用的外部局部变量，才算作该闭包函数的upvalue，Lua会按照闭包函数引用的顺序为upvalue编号，该编号与upvalue定义顺序无关。 最后一点是，闭包函数都是动态生成的，这和Go中的闭包有所不同，Go的闭包函数是在编译时生成的，不同的闭包可以共享闭包函数(同一个函数地址)。Lua的闭包函数动态生成会一定程度地影响运行效率和内存占用。 Lua闭包除了用于高级函数，回调函数，迭代器等上下文环境中以外，在完全不同的上下文环境，可用于重定义或预定义函数，通过这种方法，可以为代码创建一个安全的执行环境(也叫沙箱，sandbox)。 Lua还提供了对C闭包的支持，每当你在Lua中创建一个新的C函数，你可以将这个函数与任意多个upvalues联系起来，每一个upvalue 可以持有一个单独的Lua值。当函数被调用的时候，可以通过假索引(lua_upvalueindex)自由的访问任何一个upvalues。 12345678910111213static int counter (lua_State *L) &#123; double val = lua_tonumber(L, lua_upvalueindex(1)); lua_pushnumber(L, ++val); /* new value */ lua_pushvalue(L, -1); /* duplicate it */ lua_replace(L, lua_upvalueindex(1)); /* update upvalue */ return 1; /* return new value */&#125;int newCounter (lua_State *L) &#123; lua_pushnumber(L, 0); lua_pushcclosure(L, &amp;counter, 1); return 1;&#125; C闭包与Lua闭包在概念上很相似，但有两点不同： C函数的upvalues是显示push到栈中的，而Lua则可通过闭包函数引用确定哪些是upvalues C闭包不能共享upvalues，每个闭包在栈中都有独立的变量集，但你可以通过将upvalues指向同一个table来实现共享 ChunkChunk是一系列语句，Lua执行的每一块语句，比如一个文件或者交互模式下的每一行都是一个Chunk。 当我们执行loadfile(“test.lua”)时，便将test.lua的内容编译后的Chunk作为一个函数返回，如果出现编译错误，则返回nil和错误信息。而dofile相当于: 1234function dofile (filename) local f = assert(loadfile(filename)) return f()end loadstring和dostring的关系类似，只是接收字符串而不是文件名为参数。 再看require，require和dofile完成同样的功能，但主要有几点不同： require会搜索Lua环境目录来加载文件 require会判断文件是否已经加载而避免重复加载统一文件 require可以用于加载C .so库，功能类似loadlib，参考这里 一个lua模块编译后的Chunk被作为匿名函数被执行，那么定义于模块中函数对模块局部变量的引用就形成了闭包，所以说Lua中的闭包真是无处不在。 EnviromentLua中的环境用table来表示，这简化了环境处理也带来了不少灵活性。 在Lua5.1及之前，Lua将环境本身存储在一个全局变量_G中，其中包含了全局变量，内置函数，内置模块等。我们在使用任何符号x时，如果在当前函数的局部变量和upvalues无法找到符号定义(PS: Lua查找变量定义的规则为：局部变量 -&gt; 外部局部变量(upvalue) -&gt; 全局变量)，则会返回_G.x的值。由于_G是一个table，因此我们可以用它实现一些有意思的功能： 通过动态名字访问全局变量： _G[varname] 通过_G的metatable改变对未定义全局变量的读(__index)和写(__newindex)行为 通过setfenv改变指定函数的_G环境，制造函数执行的沙盒环境 现在再回头来看闭包，实际上，Lua闭包除了函数和upvalues，还包括函数环境，这三者组成了一个完整的执行沙盒。 在Lua5.2及之后，Lua取消了setfenv函数，用_ENV方案替代了_G方案： 12345678910111213141516-- before Lua 5.1function f() setfenv(1, &#123;&#125;) -- code hereend-- after Lua 5.2function f() local _ENV = &#123;&#125; -- code hereendorfunction f() local _ENV = &#123;&#125; return function() ... endend _ENV有三个特性： 对全局变量x的引用，将转换为_ENV.x 每个编译后的Chunk，都有一个_ENV upvalue(哪怕并未使用)，作为Chunk环境，并作用于其内定义的函数 在初始化时，_ENV=_G 除了以上三点外，_ENV和普通变量并无区别。因此我们可以直接通过local _ENV = {}来覆盖接下来的代码的环境。将环境(_ENV)作为一个普通的upvalue来处理，这样做的好处是简化了闭包的概念，闭包等于函数加upvalues(没有了全局变量_G)，为闭包优化(如合并相同upvalues的闭包)提供更好的支持，同时也减少了setfenv(f, env)带来的不确定性和不安全性(函数的_ENV upvalue在闭包返回时就已经确定了)。 有_ENV还是一个table，因此对全局变量的访问控制等trick，仍然很容易实现。Lua目前仍然保留_G，但理解它们的别是比较重要的： 我们都知道Lua有一个全局注册表(Registry)，其中包含整个Lua虚拟机的信息，在Registry的LUA_RIDX_GLOBALS索引中，保存了Globals(也就是_G)，在创建Globals时，会生成_G._G=_G的自引用。在引入_ENV后，初始时，_ENV=_G，一旦编译器将_ENV放入Chunk的upvalue后，_ENV将作为普通upvalue被看待，因此我们可以对其重新赋值： 123456789101112i = 1 -- 此时 _ENV.i == _G.i == 1function f() local _ENV=&#123;i=2, print=print, _G=_G&#125; print(i, _ENV.i, _G.i)endfunction g() print(i, _ENV.i, _G.i)endf() -- 2 2 1g() -- 1 1 1 因此，_ENV除了在创建时和_G都指向Registry[LUA_RIDX_GLOBALS]之外，和_G并没有直接联系(_G={}不会影响函数环境，_G.x=1仍然会影响注册表中的Globals)，Lua5.2及之后的环境都由_ENV指定，_G出于历史原因保留，但实际上Lua并不在内部再使用： Lua keeps a distinguished environment called the global environment. This value is kept at a special index in the C registry (see §4.5). In Lua, the global variable _G is initialized with this same value. (_G is never used internally.) 参考_ENV vs _G，setfenv in Lua5.2 Packages在Lua中，有闭包，灵活的table和环境管理，想要实现包管理有非常多的方法： 1. 基本方法最简单的方法就是直接使用table和第一类函数特性： 12345complex = &#123;&#125;function complex.new(r,i) ... endfunction complex.add(c1,c2) ... end...return complex 执行这个Chunk后，便可以通过complex.xxx()使用complex中定义的API了。这种方案主要的缺点是包内包外的调用都必须加上前缀，并且不能很好地隐藏私有成员。 2. 局部函数通过局部函数再导出的方式，我们可以解决包内调用前缀和隐藏私有成员(不导出即可)的问题。12345local function new(r,i) ... endlocal function add(c1,c2) ... end...complex = &#123;new = new, add = add&#125;return complex 但这样容易忘了local，造成全局命名空间污染。 3. 独立环境1234567complex = &#123;&#125;-- before Lua5.1: setfenv(1, complex)local _ENV = complexfunction new(r, i) ... endfunction new(c1, c2) ... endreturn complex 现在，包内所有全局符号new, add都会被转换为complex.new, complex.add，并且我们为包创建了一个独立沙盒环境，如果要在包内访问全局符号，也有多种方法: 1234567-- 方案1: 保存老的全局环境 之后访问全局符号需要加上 _G.前缀local _G = _G-- 方案2: 通过metatable 效率低一些，并且外部可通过complex.print访问_G.printsetmetatable(complex, &#123;__index = _G&#125;)-- 方案3: 只导出要使用的函数 这种方法隔离型更好，并且更快local sqrt = math.sqrtlocal print = print","tags":[{"name":"lua","slug":"lua","permalink":"http://wudaijun.com/tags/lua/"}]},{"title":"Mac OS X下的资源限制","date":"2017-02-06T16:00:00.000Z","path":"2017/02/max-osx-ulimit/","text":"系统的资源是有限的(如CPU，内存，内核所能打开的最大文件数等)，资源限制对针对进程能使用的系统资源设定上限。防止恶意进程无限制地占用系统资源。 资源限制分为两种，硬限制(Hard Limit)和软限制(Soft Limit)，软限制作用于实际进程并且可以修改，但不能超过硬限制，硬限制只有Root权限才能修改。 相关命令在Mac OS X下，有如下三个命令与系统资源有关。 launchctllaunchctl管理OS X的启动脚本，控制启动计算机时需要开启的服务(通过后台进程launchd)。也可以设置定时执行特定任务的脚本，类似Linux cron。 例如，开机时自动启动Apache服务器： $ sudo launchctl load -w /System/Library/LaunchDaemons/org.apache.httpd.plist 关于launchctl的plist格式和用法参考: launchctl man page launchd plist man page mac-os-x-launchd-is-cool creating launchd jobs 简单来说，plist文件用类似XML格式定义了一个命令(及启动参数)和该命令的执行方式(定时执行，系统启动执行，用户登录执行等)。我们这里不着重讨论，我们关心launchctl中如何查看/更改系统资源限制。 # Usage: launchctl limit [&lt;limit-name&gt; [&lt;both-limits&gt; | &lt;soft-limit&gt; &lt;hard-limit&gt;] # 查看文件描述符限制 launchctl limit maxfiles maxfiles 256 unlimited # 修改软限制为512 系统重启失效 sudo launchctl limit maxfiles 512 unlimited # 可将launchctl子命令写入/etc/launchd.conf中 # 在launchd启动时 会执行该文件中的命令 limit maxfiles 512 unlimited 通过将更改命令写入plist文件，并在启动时执行，也可永久更改资源限制： 新建Library/LaunchDaemons/limit.maxfiles.plist文件，写入 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;!DOCTYPE plist PUBLIC &quot;-//Apple//DTD PLIST 1.0//EN&quot; &quot;http://www.apple.com/DTDs/PropertyList-1.0.dtd&quot;&gt; &lt;plist version=&quot;1.0&quot;&gt; &lt;dict&gt; &lt;key&gt;Label&lt;/key&gt; &lt;string&gt;limit.maxfiles&lt;/string&gt; &lt;key&gt;ProgramArguments&lt;/key&gt; &lt;array&gt; &lt;string&gt;launchctl&lt;/string&gt; &lt;string&gt;limit&lt;/string&gt; &lt;string&gt;maxfiles&lt;/string&gt; &lt;string&gt;64000&lt;/string&gt; &lt;string&gt;524288&lt;/string&gt; &lt;/array&gt; &lt;key&gt;RunAtLoad&lt;/key&gt; &lt;true/&gt; &lt;key&gt;ServiceIPC&lt;/key&gt; &lt;false/&gt; &lt;/dict&gt; &lt;/plist&gt; 修改文件权限 sudo chown root:wheel /Library/LaunchDaemons/limit.maxfiles.plist sudo chmod 644 /Library/LaunchDaemons/limit.maxfiles.plist 加载plist文件(或重启系统后生效 launchd在启动时会自动加载该目录的plist) sudo launchctl load -w /Library/LaunchDaemons/limit.maxfiles.plist 确认更改后的限制 launchctl limit maxfiles sysctl大多数类Unix系统都通过(Linux/*BSD/OS X)都提供该命令来更改资源限制和内核配置： # 查看当前内核和进程能打开的文件描述符限制 $ sysctl -A | grep kern.maxfiles kern.maxfiles: 12288 # 系统级的限制 kern.maxfilesperproc: 10240 # 内核级的限制 # 通过sysctl命令热更改 系统重启后失效 $ sysctl -w kern.maxfilesperproc=20480 # 通过配置文件永久更改 重启生效 # 在/etc/sysctl.conf中写入 kern.maxfiles=20480 kern.maxfilesperproc=24576 ulimitulimit是shell的内置命令，用于查看/更改当前shell及其创建的子进程的资源限制。使用比较简单： # 查看当前shell(及其子进程)的所有限制 ulimit -a # 改变进程能打开的最大文件描述符数软限制 当shell关闭后失效 # 将其写入对应shell的startup文件(如~/.bashrc, ~/.zshrc)，可保留更改 ulimit -S -n 1024 区别联系这三个命令的关系在Mac OS X各版本中尤其混乱，先说说本人的一些试验(Mac OS X 10.10.3)： 在默认配置下(不配置plist和sysctl.conf)，launchctl的maxfiles默认值为(256, unlimited)，sysctl的maxfiles默认值为(12288, 10240)，而ulimit -n得到的值为4864。 当不定义plist而定义sysctl.conf，那么重启后launchctl和ulimit看到的上限仍为默认值，sysctl看到的上限与sysctl.conf定义的一致。 当同时在/etc/sysctl.conf和/Library/LaunchDaemons/limit.maxfiles.plist中定义maxfiles时，plist文件中的配置会覆盖sysctl.conf中的配置。如果通过系统重启应用plist，三个命令看到的上限均为plist配置。如果通过launchctl load加载plist，则会同步影响sysctl看到的上限，而不会影响shell下的ulimit上限。 如果通过launchctl配置的软上限和硬上限分别为S和H(非unlimited)，那么通过launchctl应用配置后最终得到软上限和硬上限都为S。如果设定的上限为S和unlimited，实际上应用的参数为S和10240(sysctl中kern.maxfilesperproc默认值)，当S&gt;10240时，会设置失败，S&lt;10240时，会得到(S, 10240) ulimit -H -n 1000 降低硬上限无需Root权限，升高则需要 趁着头大，还可以看看这几篇文章: open files limit does not work as before in osx yosemite maximum files in mac os x how to persist ulimit settings in osx mavericks open files limit in max os x increase the maximum number of open file descriptors in snow leopard 网上对Mac OS X各版本的解决方案各不相同，并且对这三个命令(特别是launchctl和sysctl)在资源限制上的联系与区别也没有清晰的解释。 按照我的理解和折腾出来的经验： ulimit只影响当前Shell下的进程，并且受限于kern.maxfilesperproc 如果配置了plist，那么重启后，ulimit和sysctl均会继承plist中的值 热修改sysctl上限值不会影响launchctl，而反之，launchctl会影响sysctl上限值 综上，在Mac OS X 10.10(我的版本，没试过之前的)之后，使用plist是最合理的方案(但launchctl貌似只能设定一样的软限制和硬限制，如果将硬限制设为ulimited，则会使用kern.maxfilesperproc值)。在系统重启后，kern.maxfilesperproc和ulimit -n都会继承plist maxfiles的值。","tags":[{"name":"system","slug":"system","permalink":"http://wudaijun.com/tags/system/"},{"name":"macosx","slug":"macosx","permalink":"http://wudaijun.com/tags/macosx/"}]},{"title":"Go 常用命令","date":"2017-01-19T16:00:00.000Z","path":"2017/01/go-command-notes/","text":"环境管理 Go版本管理: gvm(go version manager) GOPATH管理: gvp(go version package) 依赖版本管理: gpm(go package manager) go build用于编译指定的源码文件或代码包以及它们的依赖包。 import导入路径中的最后一个元素是路径名而不是包名，路径名可以和包名不一样，但同一个目录只能定义一个包(包对应的_test测试包除外) 编译包: # 当前路径方式 cd src/foo &amp;&amp; go build # 包导入路径方式 go build foo bar # 本地代码包路径方式 go build ./src/foo go build 在编译只包含库源码文件的代码包时，只做检查性的编译，不会输出任何结果文件。如果编译的是main包，则会将编译结果放到执行命令的目录下。 编译源码文件: # 指定源码文件使用文件路径 # 指定的多个源码文件必须属于同一个目录(包) go build src/foo/foo1.go src/foo/foo2.go 当执行以上编译时，编译命令在分析参数的时候如果发现第一个参数是Go源码文件而不是代码包时，会在内部生成一个名为“command-line-arguments”的虚拟代码包。也就是当前的foo1.go foo2.go属于”command-line-arguments”包，而不是foo包，因此除了指定的源码文件和它们所依赖的包，其它文件(如foo3.go)不会被编译。 同样，对于库源码文件，build不会输出任何结果文件。对于main包的源文件，go build要求有且只能有一个main函数声明，并将生成结果(与指定的第一个源码文件同名)放在执行该命令的当前目录下。 构建与go build之上的其它命令(如go run，go install)，在编译包或源码文件时，过程和特性是一样的。 常用选项: 选项 描述 -v 打印出那些被编译的代码包的名字。 -n 打印编译期间所用到的其它命令，但是并不真正执行它们。 -x 打印编译期间所用到的其它命令。注意它与-n标记的区别。 -a 强行对所有涉及到的代码包（包含标准库中的代码包）进行重新构建，即使它们已经是最新的了。 -work 打印出编译时生成的临时工作目录的路径，并在编译结束时保留它。在默认情况下，编译结束时会删除该目录。 go rungo run编译(通过go build)并运行命令源码文件(main package)，查看过程: go run -x -work src/main/main.go # build 临时目录 WORK=/var/folders/n5/j8y6skrx1xn3_ls64gl1lrsmmp53rv/T/go-build979313546 # main.go依赖foo包 先编译foo包 mkdir -p $WORK/foo/_obj/ mkdir -p $WORK/ cd /Users/wudaijun/Work/test/src/foo /usr/local/Cellar/go/1.7/libexec/pkg/tool/darwin_amd64/compile -o $WORK/foo.a -trimpath $WORK -p foo -complete -buildid cd61b5a9f3c8eba0f3088adca894fc9bf695826b -D _/Users/wudaijun/Work/test/src/foo -I $WORK -pack ./foo.go # 在虚拟包 command-line-arguments 中编译 main.go mkdir -p $WORK/command-line-arguments/_obj/ mkdir -p $WORK/command-line-arguments/_obj/exe/ cd /Users/wudaijun/Work/test/src/main /usr/local/Cellar/go/1.7/libexec/pkg/tool/darwin_amd64/compile -o $WORK/command-line-arguments.a -trimpath $WORK -p main -complete -buildid 9131b7dd9f64a85bb423da7f8a7d408c089a23e8 -D _/Users/wudaijun/Work/test/src/main -I $WORK -I /Users/wudaijun/Work/test/pkg/darwin_amd64 -pack ./main.go # 链接 cd . /usr/local/Cellar/go/1.7/libexec/pkg/tool/darwin_amd64/link -o $WORK/command-line-arguments/_obj/exe/main -L $WORK -L /Users/wudaijun/Work/test/pkg/darwin_amd64 -w -extld=clang -buildmode=exe -buildid=9131b7dd9f64a85bb423da7f8a7d408c089a23e8 $WORK/command-line-arguments.a # 从临时目录运行可执行文件 $WORK/command-line-arguments/_obj/exe/main Call Foo() 可看到go run的执行结果都在WORK临时目录中完成，由于使用了-work选项，因此WORK目录会在go run执行完成后保留。go run只接受命令源文件而不接收包路径作为参数，并且不会在当前目录生成任何文件。 go installgo install只比go build多干一件事：安装编译后的结果文件到指定目录。 go test1. 单元测试go test编译指定包或源文件，并执行所在包对应的测试用例。一个符合规范的测试文件指： 文件名必须是_test.go结尾的，这样在执行go test的时候才会执行到相应的代码 你必须import testing这个包 所有的测试用例函数必须是Test开头 测试用例会按照源代码中写的顺序依次执行 测试函数TestXxx()的参数是testing.T，我们可以使用该类型来记录错误或者是测试状态 测试格式：func TestXxx (t *testing.T),Xxx部分可以为任意的字母数字的组合，但是- - 首字母不能是小写字母[a-z]，例如Testingdiv是错误的函数名 函数中通过调用testing.T的Error, Errorf, FailNow, Fatal, FatalIf方法，说明测试不通过，调用Log方法用来记录测试的信息 测试分为包内测试和包外测试，即测试源码文件可于被测试源码文件位于同一个包(目录)，或者测试源码文件声明的包名可以是被测试包名+”_test”后缀。 2. 基准测试基准测试也就是跑分测试，写法和单元测试差不多，只不过函数签名为BenchmarkXxx(b *testing.B)，函数内通过 b.N 作为迭代次数，go test会自动调整这个值，得到合适的测试次数，然后算出每次迭代消耗的时间。 3. 执行测试# 执行当前目录所在包的单元测试 go test . # 执行当前目录所在包的单元测试和基准测试(-bench 后面接正则匹配，&#39;.&#39;通配所有 Benchmark) go test -bench . . # 执行当前目录所在包的 TestAbc 单元测试以及 BenchmarkAbc 基准测试 go test -run TestAbc -bench BenchmarkAbc . # 在当前目录生成 battle.test 二进制文件而不执行，支持 go build 的所有参数。如可通过 -o 参数指定输出文件 go test -c ngs/battle # 直接执行 test 二进制时，test flag 需要加上 &#39;test.&#39; 前缀 ./battle.test -test.bench . # 以下两条命令等价 并且实际上，编译器也是分为这两步来做的 go test -bench . -cpuprofile cpu.prof . go test -c -o my.test . &amp;&amp; my.test -test.bench . -test.cpuprofile cpu.prof # 如果要在 go test 时传入无需编译器加 &#39;test.&#39; 前缀的 flag，可将 flag 放在 -args 选项后: go test -v -args -x -v # 等价于: pkg.test -test.v -x -v","tags":[{"name":"go","slug":"go","permalink":"http://wudaijun.com/tags/go/"}]},{"title":"CSS 笔记","date":"2017-01-08T16:00:00.000Z","path":"2017/01/css-notes/","text":"一. 选择器1. 普通选择器 类别 例子 解释 标签选择器 div 以HTML 标签类型来选择元素,又叫类型选择器 类选择器 .span1 以class属性值来选择元素,可在页面中出现多个 ID选择器 #inst 以id属性值来选择元素,在页面中只能出现一次 2. 并列选择器 类别 例子 解释 并列选择器 div1,span1 同时定义多个样式,即该CSS有多个名称,简化CSS书写 3. 层级选择器 类别 例子 解释 后代选择器 body .span1 选择指定祖先元素内的后代元素 直接子元素选择器 body &gt; .span1 选择指定父元素内的直接子元素 例子: /* body .span1 影响元素E1 E2 */ /* body &gt; .span1 只影响元素E2 */ &lt;body&gt; &lt;span class=&quot;span1&quot;&gt; E1 &lt;/span&gt; &lt;div&gt; &lt;span class=&quot;span1&quot;&gt; E2 &lt;/span&gt; &lt;/div&gt; &lt;/body&gt; 4. 兄弟选择器 类别 例子 解释 普通兄弟选择器 div ~ p 选择第一个元素后的兄弟元素,两者拥有相同的父元素 相邻兄弟选择器 div + p 选择第一个元素后紧跟的元素,两者拥有相同的父元素 例子: /* div + p 影响 Three Six */ /* div ~ p 影响 Three Six Seven */ &lt;div&gt; &lt;p&gt;One&lt;/p&gt; &lt;div&gt;Two&lt;/div&gt; &lt;p&gt;Three&lt;/p&gt; &lt;/div&gt; &lt;div&gt; &lt;div&gt;Four&lt;/div&gt; &lt;div&gt;&lt;p&gt;Five&lt;/p&gt;&lt;/div&gt; &lt;p&gt;Six&lt;/p&gt; &lt;p&gt;Seven&lt;/p&gt; &lt;/div&gt; 5. 伪类/伪元素 伪类: 用于定义同一样式的不同状态 伪元素: 用来添加一些选择器的特殊效果 /* 常见伪类 */ a:link {color:#FF0000;} /* 未访问的链接 */ a:visited {color:#00FF00;} /* 已访问的链接 */ a:hover {color:#FF00FF;} /* 鼠标划过链接 */ a:active {color:#0000FF;} /* 已选中的链接 */ p:first-child{color:blue;} /* 改变当p作为父元素第一个子元素时的样式*/ /* 常见伪元素 */ h1:before{content:url(smiley.gif);} /* 在元素内容之前插入图片 */ h1:after{content:url(smiley.gif);} /* 在元素内容之后插入图片 */ p:first-line {color:#ff0000;} /* 为文本的首行设置特殊样式 */ p:first-letter {color:#ff0000;} /* 为文本的首字母设置特殊样式 */ 6. 优先级!import &gt; 元素内嵌样式 &gt; ID选择器 &gt; Class选择器 &gt; 类型选择器 &gt; 父元素继承值，如果一个选择器应用的多个样式重复定义了某一属性，则样式在CSS中定义顺序越后面优先级越高。 12345678910111213&lt;style&gt; ... h1 &#123;color: red; &#125; .pink-text &#123; color: pink; &#125; .blue-text &#123; color: blue; &#125;&lt;/style&gt;&lt;body&gt;&lt;h1 id=\"orange-text\" class=\"blue-text pink-text\" style=\"color: white\"&gt;Hello World!&lt;/h1&gt;&lt;/body&gt;应用color属性的优先级为: style=\"color:white\" &gt; .orange-text &gt; .pink-text &gt; blue-text &gt; h1类型选择器 &gt; 从body继承color值如果对pink-text的color属性应用了!important，那么应用important的属性优先级将始终最高! 二. 内外边距1. 元素结构 2. padding内边距，定义元素边框和元素内容之间的留白 可填充背景 相邻元素的内边距会叠加(15px + 20px=35px) 3. margin外边距，元素周围生成额外的空白区。“空白区”通常是指其他元素不能出现且父元素背景可见的区域。 不可填充背景 边界是完全透明的(父元素背景可见) 相邻元素的边界会被折叠15px + 20px=20px) margin可为负数，当static元素的margin-top/margin-left被赋予负值时，元素将被拉进指定的方向。例如： /* 元素向上移10px*/ #mydiv1 {margin-top:-10px;} 但如果你设置margin-bottom/right为负数，元素并不会如你所想的那样向下/右移动，而是将后续的元素拖拉进来，覆盖本来的元素。 /* #mydiv1后续元素向上移10px, #mydiv1 本身不移动 */ #mydiv1 {margin-bottom:-10px;} 关于负margin的更多用法: https://www.w3cplus.com/css/the-definitive-guide-to-using-negative-margins.html 4. 外边距合并外边距合并指的是，当两个垂直外边距相遇时，它们将形成一个外边距。合并后的外边距的高度等于两个发生合并的外边距的高度中的较大者。 垂直外边距合并问题常见于第一个子元素的margin-top会顶开父元素与父元素相邻元素的间距。如: &lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot;&gt; &lt;head&gt; &lt;title&gt;垂直外边距合并&lt;/title&gt; &lt;style&gt; .top{width:160px; height:50px; background:#ccf;} .middle{width:160px; background:#cfc;} .middle .firstChild{margin-top:20px;} &lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;div class=&quot;top&quot;&gt;&lt;/div&gt; &lt;div class=&quot;middle&quot;&gt; &lt;div class=&quot;firstChild&quot;&gt;我其实只是想和我的父元素隔开点距离。&lt;/div&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt; 原因：根据CSS规范，当一个元素包含在另一个元素中时（假设没有内边距或边框把外边距分隔开），它们的上和/或下外边距会发生合并。因此firstChild和其父元素middle的上外边距重叠，并顶开了middle和top间的外边距。解决方案是为middle定义边框或者内边距。参考CSS外边距合并，padding or margin。","tags":[{"name":"web","slug":"web","permalink":"http://wudaijun.com/tags/web/"}]},{"title":"Erlang 内存问题诊断","date":"2016-12-25T16:00:00.000Z","path":"2016/12/erlang-memory-debug/","text":"通过erlang:memory()查看节点内存占用总览，需要通过静态和动态两个维度对内存进行考核： 静态: 各类内存占用比例，是否有某种类的内存占用了节点总内存的绝大部分 动态: 各类内存增长特性，如增长速度，或是否长期增长而不回收(atom除外) 找出有疑似内存泄露的种类后，再进行下一步分析 atomatom不会被GC，这意味着我们应该对atom内存增长更加重视而不是忽略。在编写代码时，尽量避免动态生成atom，因为一旦你的输入源不可靠或受到攻击(特别针对网络消息)，atom内存增长可能导致节点crash。可以考虑将atom生成函数替换为更安全的版本： list_to_atom/1 -&gt; list_to_existing_atom/1 binary_to_atom/2 -&gt; binary_to_existing_atom/2 binary_to_term(Bin) -&gt; binary_to_term(Bin,[safe]) etsets内存占用通常是由于表过大，通过ets:i().查看ets表条目数，大小，占用内存等。 process进程内存占用过高可能有两方面原因，进程数量过大和进程占用内存过高。针对于前者，首先找出那些没有被链接或监控的”孤儿进程”： [P || P&lt;-processes(), [{_,Ls},{_,Ms}] &lt;- [process_info(P,[links,monitors])], []==Ls,[]==Ms]. 或通过supervisor:count_children/1查看sup下进程数量和状态。 而如果是进程所占内存过高，则可将内存占用最高的几个进程找出来进行检查: recon:proc_count(memory, 10). % 打印占用内存最高的10个进程 recon:proc_count(message_queue_len, 10). % 打印消息队列最长的10个进程 binaryerlang binary大致上分为两种，heap binary(&lt;=64字节)和refc binary(&gt;64字节)，分别位于进程堆和全局堆上，进程通过ProBin持有refc binary的引用，当refc binary引用计数为0时，被GC。关于binary的详细实现，参考Erlang常用数据结构实现。 recon提供的关于binary问题检测的函数有： % 打印出引用的refc binary内存最高的N个进程 recon:proc_count(binary_memory, N) % 对所有进程执行GC 打印出GC前后ProcBin个数减少数量最多的N个进程 recon:bin_leak(N) 以上两个函数，通常可以找出有问题的进程，然后针对进程的业务逻辑和上下文进行优化。通常来说，针对于refc binary，有如下思路： 每过一段时间手动GC(高效，不优雅) 如果只持有大binary中的一小段，用binary:copy/1-2(减少refc binary引用) 将涉及大binary的工作移到临时一次性进程中，做完工作就死亡(变相的手动GC) 对非活动进程使用hibernate调用(该调用将进程挂起，执行GC并清空调用栈，在收到消息时再唤醒) 一种典型地binary泄露情形发生在当一个生命周期很长的中间件当作控制和传递大型refc binary消息的请求控制器或消息路由器时，因为ProcBin仅仅只是个引用，因此它们成本很低而且在中间件进程中需要花很长的时间去触发GC，所以即使除了中间件其他所有进程都已经GC了某个refc binary对应的ProcBin，该refc binary也需要保留在共享堆里。因此中间件进程成为了主要的泄漏源。 针对这种情况，有如下解决方案： 避免中间件接触到refc binary，由中间件进程返回目标进程的Pid，由原始调用者来进行binary转发 调整中间件进程的GC频率(fullsweep_after) driver/nif另一部分非Erlang虚拟机管制的内存通常来自于第三方Driver或NIF，要确认是否是这部分内存出了问题，可通过recon_alloc:memory(allocated).和OS所报告的内存占用进行对比，可以大概得到C Driver或NIF分配的内存，再根据该部分内存的增长情况和占用比例来判断是否出现问题。 如果是纯C，那么内存使用应该是相对稳定并且可预估的，如果还挂接了Lua这类动态语言，调试起来要麻烦一些，在我们的服务器中，Lua部分是无状态的，可以直接重新加载Lua虚拟机。其它的调试手段，则要透过Lua层面的GC机制去解决问题了。","tags":[{"name":"erlang","slug":"erlang","permalink":"http://wudaijun.com/tags/erlang/"}]},{"title":"Erlang Unicode编码","date":"2016-12-16T16:00:00.000Z","path":"2016/12/unicode-in-erlang/","text":"Unicode基础编码方式定义字符集中每个字符的codepoint(数字编码) ASCII: 不用多说，编码空间为7位(0-127) ISO 8859-1: 又称Latin-1，以ASCII为基础，在空置的0xA0-0xFF的范围内，加入96个字母及符号。编码空间为8位(0-255) UCS-2: 16位编码空间 又称基本多文种平面或零平面 UCS-4: 32位编码空间 在UCS-2基本上，加入辅助平面(目前有16个辅助平面，至少需要21位) 注1: UCS(Universal Character Set, 通用字符集) 注2: 以上四种编码都是向前兼容的，通常我们所说的Unicode编码指UCS-2和UCS-4，目前广泛运用的是UCS-2 实现方式实现方式将字符的数字编码存储在计算机字节中，由于节省空间和平台差异性等，衍生不同的实现方式 UTF-8: 一种变长编码，使用1-3个字节编码UCS-2字符集，1-6个字节可编码UCS-4字符集(目前只用最多四个字节即可表示UCS-4所定义的17个平面)。优点是兼容ASCII，节省空间，并且不存在字节序的问题 UTF-16: 和UTF-8类似，使用2个字节来编码UCS-2字符集(UCS-2中有预留的位用于实现UTF-16扩展多字节)，使用4个字节来编码UCS-4字符集。由于使用两个字节作为基本编码单位，UTF-16存在字节序的问题，通常使用BOM来解决 UTF-32: 32位定长编码，能够表示UCS-4字符集所有字符，但空间占用大，因此很少见 注1: UTF(Unicode Transformation Format, Unicode转换格式) 注2: BOM(byte-order mark, 字节顺序标记) Erlang中的UnicodeUnicode表示%% 环境 Mac OSX Yosemite &amp; Erlang OTP/19 Eshell V8.1 (abort with ^G) 1&gt; L = &quot;中文&quot;. [20013,25991] % Erlang lists存放的是字符的Unicode编码 2&gt; B = &lt;&lt;&quot;中文&quot;&gt;&gt;. &lt;&lt;45,135&gt;&gt; % Erlang只知&quot;中文&quot;的Unicode编码[20013,25991]，并不知应该用何种实现方式(UTF8或其他)，默认它会将Unicode编码 rem 256，产生0-255间的编码(并按照Lantin-1解码) % 下面我们将考虑将&quot;中文&quot;转换为binary % 方案一. erlang:list_to_binary -&gt; error 3&gt; list_to_binary(L). % 该函数支持的list只能是iolist(见后面术语参考)，否则Erlang并不知道你想将字符串转换为何种编码格式的binary ** exception error: bad argument in function list_to_binary/1 called as list_to_binary([20013,25991]) % 方案二. unicode:characters_to_binary -&gt; ok 4&gt; UTF8 = unicode:characters_to_binary(L).% 将L中的unicode编码转换为UTF8 binary &lt;228,184,173,230,150,135&gt;&gt; 5&gt; UTF16Big = unicode:characters_to_binary(UTF8,utf8,utf16). &lt;&lt;78,45,101,135&gt;&gt; % 默认为Big Endian 6&gt; UTF16Little = unicode:characters_to_binary(UTF8,utf8,{utf16,little}). &lt;&lt;45,78,135,101&gt;&gt; % 方案三. 利用binary构造语法构建 7&gt; UTF8 = &lt;&lt;&quot;中文&quot;/utf8&gt;&gt;. &lt;&lt;228,184,173,230,150,135&gt;&gt; 8&gt; UTF8 = &lt;&lt;L/utf8&gt;&gt;. % Why ? ** exception error: bad argument 在Erlang中，字符串就是整数列表，并且这个整数可以无限大，lists将保存其中每个字符的Unicode编码，只要lists中的整数是有效的Unicode codepoint，就可以找到对应的字符。因此也就不存在UTF8/UTF16格式的lists字符串一说。而binary的处理则要麻烦一些，Erlang用UTF8作为Unicode在binary上的实现方式，unicode模块提供了这方面丰富的unicode编码处理接口。 Unicode使用8&gt; io:format(&quot;~s&quot;, [L]). ** exception error: bad argument in function io:format/3 called as io:format(&lt;0.50.0&gt;,&quot;~s&quot;,[[20013,25991]]) 9&gt; io:format(&quot;~p&quot;, [L]). [20013,25991]ok 10&gt; io:format(&quot;~ts&quot;, [L]). 中文ok 11&gt; io:format(&quot;~s&quot;, [UTF8]). ä¸­æok 12&gt; io:format(&quot;~p&quot;, [UTF8]). &lt;&lt;228,184,173,230,150,135&gt;&gt;ok 13&gt; io:format(&quot;~ts&quot;, [UTF8]). 中文ok 先解释几个Erlang术语： iolist: 0-255编码(Latin-1)的lists，binary，或它们的嵌套，如[[&quot;123&quot;,&lt;&lt;&quot;456&quot;&gt;&gt;],&lt;&lt;&quot;789&quot;&gt;&gt;] unicode binary: UTF8编码的binary(Erlang默认使用UTF8 binary编码unicode) charlist: UTF8编码的binary，或包含有效unicode codepoint的lists，或它们的嵌套，如[&lt;&lt;&quot;hello&quot;&gt;&gt;, &quot;中国&quot;] ~s只能打印iolist，binary，或atom，因此不能直接打印中文lists(无法解码超过255的codepoint)或UTF8 binary(会按字节解释，出现乱码)。 ~ts则可打印charlist和unicode binary。 ~p如果不能打印出ASCII(0-127)字符，则直接打印出原生Term，不会对Unicode编码进行处理。 参考： http://erlang.org/doc/man/unicode.html http://erlang.org/doc/apps/stdlib/unicode_usage.html","tags":[{"name":"erlang","slug":"erlang","permalink":"http://wudaijun.com/tags/erlang/"}]},{"title":"探索Docker在Erlang集群中的应用","date":"2016-11-04T16:00:00.000Z","path":"2016/11/docker-erlang/","text":"接上篇，结合Erlang，对Docker的实际应用进一步理解。并探索将Docker应用到Erlang集群的方案。 简单Docker交互下面是个简单的echo server： -module(server_echo). -export([start/0]). start() -&gt; io:format(&quot;SERVER Trying to bind to port 2345\\n&quot;), {ok, Listen} = gen_tcp:listen(2345, [ binary , {packet, 0} , {reuseaddr, true} , {active, true} ]), io:format(&quot;SERVER Listening on port 2345\\n&quot;), accept(Listen). accept(Listen) -&gt; {ok, Socket} = gen_tcp:accept(Listen), WorkerPid = spawn(fun() -&gt; echo(Socket) end), gen_tcp:controlling_process(Socket, WorkerPid), accept(Listen). echo(Socket) -&gt; receive {tcp, Socket, Bin} -&gt; io:format(&quot;SERVER Received: ~p\\n&quot;, [Bin]), gen_tcp:send(Socket, Bin), echo(Socket); {tcp_closed, Socket} -&gt; io:format(&quot;SERVER: The client closed the connection\\n&quot;) end. 简单起见，我们直接用telnet命令对echo server进行测试。现在，考虑如何在Docker容器中运行echo server。 容器中运行sudo docker run -it --rm -v ~/docker:/code -w /code erlang erl Erlang/OTP 19 [erts-8.1] [source] [64-bit] [smp:4:4] [async-threads:10] [hipe] [kernel-poll:false] Eshell V8.1 (abort with ^G) 1&gt; c(server_echo). {ok,server_echo} 2&gt; server_echo:start(). SERVER Trying to bind to port 2345 SERVER Listening on port 2345 在docker run中，我们将本地代码路径挂载到容器的/code目录，并且将/code作为容器的工作目录，此后对本地代码的修改，将直接反映在容器中，而无需拷贝。运行容器后会进入erl shell，并且当前路径(/code)即为本地代码路径(~/docker)，之后编译运行server即可。 宿主机访问容器如下方案可以让宿主机能访问容器端口： 在docker run中指定-p 2345:2345导出2345端口，之后访问宿主机的2345端口等同于访问容器2345端口 在docker run中指定--network host使容器和宿主机共享网络栈，IP和端口 通过docker inspect查询容器IP地址(如:172.17.0.2)，可在宿主机上通过该IP访问容器 容器之间访问容器间交互方式主要有三种： 通过docker inspect得到容器IP地址，通过IP地址进行容器间的交互 通过docker run中指定--network container:&lt;name or id&gt;，将新创建的容器与一个已经存在的容器的共享网络栈，IP和端口 通过docker run的--link &lt;name or id&gt;选项链接两个容器，之后可以将容器名或容器ID作为Hostname来访问容器，注意--link选项仅在--network bridge下有效 定义Dockerfile前面我是通过挂载目录的方式将本地代码映射到容器中，这种方式在本地开发中比较方便，但是在项目部署或环境配置比较复杂时，我们需要通过Dockerfile来构建自己的镜像(而不是基于官方Erlang镜像)，初始化项目环境，就本例而言，Dockerfile非常简单： FROM erlang RUN mkdir code COPY server_echo.erl code/server_echo.erl RUN cd code &amp;&amp; erlc server_echo.erl WORKDIR /code ENTRYPOINT [&quot;erl&quot;, &quot;-noshell&quot;, &quot;-run&quot;, &quot;server_echo&quot;, &quot;start&quot;] Erlang多节点通信再谈Erlang分布式通信Erlang的分布式节点有自己的通信机制，这套通信机制对上层用户是透明的，我们只需一个节点名(node@host)，即可访问这个节点，而无需关心这个节点是在本机上还是在其它主机上。在这之上封装的Pid，进一步地屏蔽了节点内进程和跨节点进程的差异。 在Erlang分布式系统(2)中，我提到了Erlang的分布式设施，其中epmd扮演着重要的角色：它维护了本机上所有节点的节点名到节点监听地址的映射，并且由于epmd进程本身的监听端口在集群内是周知的(默认为4369)，因此可以根据节点名node@host得到节点所在主机上epmd的监听地址(host:4369)，进而从epmd进程上查询到节点名node所监听的地址，实现节点间通信。 在同主机不同容器中部署集群现在回到Docker，我们先尝试在同一个主机，不同容器上建立集群： # 容器A 启动后通过docker inspect查询得到IP地址: 172.17.0.2 sudo docker run -it erlang /bin/bash root@4453d880b5a5:/# erl -name n1@172.17.0.2 -setcookie 123 Eshell V8.1 (abort with ^G) (n1@172.17.0.2)1&gt; # 容器B 启动后通过docker inspect查询得到IP地址: 172.17.0.4 sudo docker run -it erlang /bin/bash root@dd0f30178036:/# erl -name n2@172.17.0.4 -setcookie 123 Eshell V8.1 (abort with ^G) (n2@172.17.0.4)1&gt; net_kernel:connect_node(&#39;n1@172.17.0.2&#39;). true (n2@172.17.0.4)2&gt; nodes(). [&#39;n1@172.17.0.2&#39;] 和在宿主机上一样，我们可以直接通过容器IP架设集群。这里使用的是-name node@host指定的longname，而如果使用shortname： # 容器A root@4453d880b5a5:/# erl -sname n1 -setcookie 123 Eshell V8.1 (abort with ^G) (n1@4453d880b5a5)1&gt; # 容器B root@dd0f30178036:/# erl -sname n2 -setcookie 123 Eshell V8.1 (abort with ^G) (n2@dd0f30178036)1&gt; net_kernel:connect_node(&#39;n1@4453d880b5a5&#39;). false 在shortname方案中，我们并不能通过nodename访问节点，本质上是因为n2节点不能通过4453d880b5a5:4369访问到n1节点所在主机上的epmd进程。我们测试一下网络环境： # 通过容器A名字ping ping 4453d880b5a5 ping: unknown host # 直接ping容器A IP ping 172.17.0.2 PING 172.17.0.2 (172.17.0.2): 56 data bytes 64 bytes from 172.17.0.2: icmp_seq=0 ttl=64 time=0.099 ms 64 bytes from 172.17.0.2: icmp_seq=1 ttl=64 time=0.089 ms 发现是hostname解析出了问题，容器链接来解决这个问题： # 重新启动容器B 并链接到容器A docker run -it --link 4453d880b5a5 erlang /bin/bash root@7692c8c71218:/# erl -sname n2 -setcookie 123 Eshell V8.1 (abort with ^G) (n2@dd0f30178036)1&gt; net_kernel:connect_node(&#39;n1@4453d880b5a5&#39;). true 有个有趣的问题是，当容器B link了容器A，那么容器B能通过容器A的Id或名字访问容器B，而反过来，容器A却不能以同样的方式访问容器B。也就是说link是单向的，这同样可以通过ping来验证。 在不同的主机上部署集群在不同的主机上部署集群，问题开始变得复杂： 不同的主机上的Docker容器处于不同的子网(一台主机对应一个子网)，因此不同主机上的容器不能直接访问，需要先发布(publish)Erlang节点监听端口 Erlang节点在Docker容器中的监听地址是由Erlang VM启动时分配的，因此我们无法在启动容器时就获知Erlang节点监听端口(从而发布该端口) 假定我们预配置了Erlang节点的监听端口xxx，如果我们使用-p xxx:xxx将可能导致端口争用(亦即一台物理机只能运行一个Docker容器)，如果我们使用-p xxx将该端口发布到主机任意一个端口，那么这个发布的主机端口，将只能通过Docker Daemon获取到(命令行下可通过docker port查看) 再来看epmd，每个Docker容器中都会跑一个epmd进程，它记录的是节点名到节点在容器中的监听地址，因此，epmd本身返回的地址是不能直接被其它主机上的节点使用的 Erlang In Docker基于上面的种种限制，有人给出了一套解决方案：Erlang In Docker。这套方案对Erlang集群做了如下制约： 每个Docker容器只能运行一个Erlang节点 预配置Erlang节点的监听端口 Erlang节点名格式为DockerContainerID@HostIP 使用Docker Daemon而不是epmd来获取节点监听端口 这套方案的核心思路是用Docker Daemon替换epmd做节点监听的服务发现，原因有二： Docker Daemon运行于主机同级网络中 维护了容器端口和主机端口的映射关系 如果节点A想要访问节点B，则节点A需要提供： 节点B所在主机地址: Host 节点B所在主机上Docker Daemon的监听端口: DaemonPort 节点B所在容器ID: ContainerID 节点B在所在容器中的监听端口: Port0 之后就可以通过Docker Daemon(Host:DaemonPort)查询到ContainerID容器的Port0端口在主机上对应的发布端口Port1，之后节点A即可通过Host:Port1与节点B通信。 然而节点A只有节点B的名字，要在节点B中编码这四条信息是非常困难的，因此Erlang In Docker的做法是，预配置Port0(12345)和DaemonPort(4243)，剩下的主机地址和容器ID则编码在节点名中：DockerContainerID@HostIP。 EID代码并不复杂，得益于Erlang可替换的分布式通信协议，EID只自定义了eid_tcp_dist(替换默认的inet_tcp_dist模块)和dpmd(通过与Docker Daemon交互模拟epmd的功能)两个模块。 总结将Erlang应用到Docker上比较困难的主要原因是Erlang已经提供了非常完备的分布式设施(参见Erlang分布式系统(2))，并且这一套对上层都是透明的。EID这套方案看起来限制很多，但细想也没多大问题，具体还要看在生产环境中的表现，目前我比较顾虑它的通信效率(NAT)和eid_tcp_dist是否足够健壮。","tags":[{"name":"docker","slug":"docker","permalink":"http://wudaijun.com/tags/docker/"}]},{"title":"Docker 学习","date":"2016-10-31T16:00:00.000Z","path":"2016/11/docker-basic/","text":"一. 理解 DockerDocker是一种轻量级的虚拟化方案，虚拟化本身可以从两个角度来理解： 隔离性：可传统的虚拟机类似，资源隔离(进程，网络，文件系统等)可用于更好地利用物理机。Docker本身虚拟化的开销非常小，这也是它相对于传统虚拟机最大的优势 一致性：同样一份虚拟机镜像，可以部署在不同的平台和物理机上，并且内部的环境，文件，配置是一致的，这在当前多样化的平台，日益复杂的配置/部署流程，以及团队和团队间的协作中，有着重要的意义。想象一下，当你用Docker提交代码时，你做的事情跟以前是完全不同的。在以前我们只是把代码提交上去，而在Docker中我们把整台计算机（虚拟机）提交上去。为什么Docker这么火，就是因为它帮助开发者很简单的就让自己的开发环境跟生产环境一致。环境的标准化，意味着目录、路径、配置文件、储存用户名密码的方式、访问权限、域名等种种细节的一致和差异处理的标准化。 Docker和其它虚拟机或容器技术相比，一是轻量，开销很小，二是发展迅速， 平台兼容性增长很快。虽然Docker的应用场景很多，但都是基于虚拟化和容器技术的这两种特性在特定问题下提出的解决方案。 下面来看看Docker的基本概念： Docker是C/S模式的，包括docker CLI和docker daemon两部分，它们之间通过RESTful API交互，Docker CLI就是我们用的docker命令 镜像(Image)：是一个只读的模板，包含了系统和运行程序，是用于创建容器的一系列指令(Dockfile)，相当于一份虚拟机的磁盘文件。 容器(Container)：当镜像启动后就转化为容器，容器是运行着的镜像，在容器内的修改不会影响镜像，程序的写入操作都保存在容器中。容器可被启动，停止和删除，由docker daemon管理。 仓库(Registry)：Docker镜像可通过公有和私有的仓库来进行共享和分发，仓库是存放和分享镜像文件的场所，功能类似于Github。Docker仓库有免费的Docker Hub和付费的Docker Store。 二. Docker 容器1. 容器操作通常我们都使用docker CLI和docker daemon交互完成docker操作，随着docker日渐完善，docker所提供的功能和参数也更复杂，以下只列举几个常用的。 docker run [OPTIONS] IMAGE [COMMAND] [ARG...] 从镜像中创建并启动容器，常用Options有： -d：后台运行 -t：为容器分配一个伪终端，通常于-i一起使用 -i：以交互模式运行容器，如果开了-i而没有指定-t，可以通过管道与容器交互 -v：为容器挂载目录，冒号前为宿主机目录，其后为容器目录 -p： [hip:]hport:cport 端口映射，将容器端口绑定到指定主机端口 --name：为容器命名 --link：链接到其它容器，之后可通过容器ID或容器名访问该容器(只针对bridge) --ip：指定容器的IP --network：配置容器的网络 --rm：当容器退出时，删除容器 完整的命令可通过docker run --help查看。 例如： docker run -it ubuntu:14.04 /bin/bash 我们就以ubuntu:14:04镜像启动了一个容器，并进入到bash交互模式。docker所做的事情为，先在本地查找ubuntu镜像，如果没有，将从Docker Hub中拉取到本地，解析镜像文件，创建容器，并运行/bin/bash命令。 每个容器在创建时，docker daemon都会为其生成一个Container ID，容器在运行结束后，为STOP状态，可以通过Container ID或容器名字再次启动/停止或删除。可通过docker ps来查看容器状态。以下是其它常用的容器管理命令： // 查看容器， 默认只显示运行中的容器，-a选项可显示所有容器 docker ps [OPTIONS] // 启动容器 docker start/stop [OPTIONS] CONTAINER [CONTAINER...] // 停止容器 docker rm CONTAINER // 把后台容器调到前端 docker attach [OPTIONS] CONTAINER // 查询容器的详细信息，也可用于镜像 docker inspect [OPTIONS] CONTAINER/IMAGE // 在容器内执行指定命令 如: docker exec -it CONTAINER bash docker exec [OPTIONS] CONTAINER COMMAND [ARG...] 也可使用第三方工具如nsenter来进入容器 2. 容器持久化镜像是分层存储的，容器也一样，每一个容器以镜像为基础层，在其上构建一个当前容器的可读可写层，容器对文件的所有更改都基于这一层。容器的可读可写层的生命周期与容器一样，当容器消亡时，容器在可读可写层作出的任何更改都将丢失(容器不能对基础镜像作出任何更改)。 有几种方式可以持久化容器作出的更改: 通过docker commit以镜像构建的方式将可读可写层提交为一个新的镜像(docker commit是docker run的逆操作)。这种方式并不推荐，因为手动commit构建的镜像没有Dockerfile说明，是”隐晦”的，使用者并不知道你对镜像作出了何种修改。 在运行容器时指定docker run -v hostdir:containerdir来将宿主机上的某个目录挂载到容器的指定目录下，这样容器对该目录作出的所有更改，都直接写入到宿主机上，效率也更高。这通常用于在容器中导出应用日志和数据，这样容器消亡后，日志和数据信息不会丢失。 通过网络IO，将数据持久化到其它地方，如mongo，redis等。 我们在运行容器时，要尽量保证容器的运行是”无状态”的，即容器可以随时被终止而重要数据不会丢失。 三. Docker 镜像1. DockerfileDocker的镜像通过一个Dockerfile构建，我们可以通过编Dockerfile来创建自定义镜像： # 这是注释 INSTRUCTION args Dockerfile不区分大小写，但惯例是将指令大写，下面介绍几个Dockerfile中常用的指令： FROMFROM命令必须是Dockerfile的第一条指令，用于指明基础镜像(镜像基础层)： # 格式：FROM &lt;image&gt;[:&lt;tag&gt;] FROM ubuntu:14:04 FROM erlang RUN在当前镜像的顶层执行命令(比如安装一个软件包)，将执行结果commit到当前镜像层。 RUN有两种格式： # shell 格式，相当于 /bin/sh -c &lt;command&gt; # 意味着可以访问shell环境变量 如$HOME RUN &lt;command&gt; # exec 格式，推荐格式，直接执行命令，不会打开shell # 这种格式更灵活，强大 RUN [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] # 以下两种写法完全等价 RUN echo &quot;hello&quot; RUN [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo hello&quot;] CMDCMD指令的主要目的是为容器提供默认值，这些默认值可以包含容器执行入口和参数，也可以只指定参数，这种情况下，容器入口由ENTRYPOINT指出。CMD有三种定义方式： # exec 格式 指定了执行入口和参数 # 可被docker run &lt;image&gt;后的参数覆盖 CMD [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;] # 当ENTRYPOINT存在时，exec格式退化为默认参数格式 # 此时CMD提供的参数将被附加到ENTRYPOINT指定的入口上 # 可被docker run &lt;image&gt;后的参数覆盖 CMD [&quot;param1&quot;, &quot;param2&quot;] # shell 格式 这种格式不能为ENTRYPOINT提供默认参数 只能提供默认执行入口 # 会被ENTRYPOINT或docker run &lt;image&gt;指定的入口覆盖 CMD command param1 param2 Dockerfile中只能有一个CMD命令(如果有多个，只有最后一个生效)，如果CMD要作为ENTRYPOINT的默认参数(即第二种定义方式)，那么CMD和ENTRYPOINT都必须以Json数组的方式指定。 CMD和RUN的区别：RUN在docker build构建镜像时执行，将执行结果写入新的镜像层(实际上也是通过容器写入的，详见后面docker build命令)，而CMD在docker run时执行，执行结果不会写入镜像。 ENTRYPOINTENTRYPOINT用于设置在容器启动时执行命令，ENTRYPOINT有两种定义方式： # exec格式 推荐格式 ENTRYPOINT [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] # shell格式 以这种方式定义，CMD和docker run提供的参数均不能附加给command命令参数 ENTRYPOINT command param1 param2 docker run &lt;image&gt;后面的参数将会附加在ENTRYPOINT指定的入口上，如： FROM ubuntu:14.04 ENTRYPOINT [&quot;echo&quot;, &quot;hello&quot;] CMD [&quot;world&quot;] 构建镜像docker build -t echo_img .，之后如果我们以docker run --rm echo_img启动容器，CMD指定的默认参数将附加在ENTRYPOINT的入口上，因此相当于执行echo hello world。而如果我们以docker run --rm echo_img wudaijun启动容器，此时docker run提供的参数将覆盖CMD指定的默认参数，相当于执行echo hello wudaijun。 再举个例子： FROM ubuntu:14.04 CMD [&quot;echo&quot;, &quot;hello&quot;] 由于没有指定ENTRYPOINT，因此CMD指定了默认的执行入口echo hello，如果docker run &lt;image&gt;未指定任何参数，则执行echo hello，否则docker run &lt;image&gt;的参数将覆盖CMD指定的执行入口。如果我们再加上Dockerfile中再加一行ENTRYPOINT [&quot;echo&quot;]，并且docker run &lt;image&gt;后未指定参数，那么将执行echo echo hello，输出echo hello。 和CMD一样，ENTRYPOINT在Dockerfile中最多只能生效一个，如果定义了多个，只有最后一个生效，在docker run中可通过docker run --entrypoint覆盖ENTRYPOINT。 CMD和ENTRYPOINT的区别：CMD和ENTRYPOINT都可用于设置容器执行入口，但CMD会被docker run &lt;image&gt;后的参数覆盖；而ENTRYPOINT会将其当成参数附加给其指定的命令（不会对命令覆盖）。另外CMD还可以单独作为ENTRYPOINT的所接命令的可选参数。如果容器是Execuatble的，通常用法是，用ENTRYPOINT定义不常变动的执行入口和参数(exec格式)，用CMD提供额外默认参数(exec格式)，再用docker run &lt;image&gt;提供的参数来覆盖CMD。另外，ENTRYPOINT指定的入口也可以是shell script，用于实现更灵活的容器交互。 ENTRYPOINT，CMD，RUN在定义时，均推荐使用Json数组方式。参见Dockerfile Best Practices Exec和Shell区别前面提到的RUN, CMD, ENTRYPOINT都有两种定义方式: # Exec定义 相当于直接执行: /bin/echo hello ENTRYPOINT echo hello # Shell定义 相当于执行: /bin/sh -c &quot;echo hello&quot; ENTRYPOINT [&quot;echo&quot;, &quot;hello&quot;] 这两者除了前面所描述的使用方法的不同之外，本质上的区别是前者(Exec)的容器主进程(Pid=1)为命令本身，而后者(Shell)的容器主进程为/bin/sh，这会导致容器接收信号的进程不同，如docker stop与docker kill会向容器发送SIGTERM和SIGKILL信号，如果使用Shell方式启动命令，命令作为主进程/bin/sh的子进程将不能正确接收到信号。 因此，统一使用Exec是最佳实践，将容器看做一个进程，这个进程即为应用本身。 其它命令ENV: 定义环境变量，该变量可被后续其它指令引用，并且在生成的容器中同样有效 ADD: src dst 将本地文件拷贝到镜像，src可以是文件路径或URL，ADD支持自动解压tar文件 COPY: 和ADD类似，但不支持URL并且不能自动解压 EXPOSE: port, 指定容器在运行时监听的端口 WORKDIR: path, 指定容器的工作目录(启动之后的当前目录) VOLUME: [path], 在容器中设置一个挂载点，用于挂载宿主机或其它容器的目录 关于Dockerfile的语法参考Dockerfile Reference。 2. docker build 原理docker build的核心机制包括docker commit和build cache两部分。 docker commit写好Dockerfile之后，通过docker build即可构建镜像： docker build -t 镜像名[:tag] Dockerfile所在目录或URL docker build将按照指令顺序来逐层构建镜像，每一条指令的执行结果将会commit为一个新的镜像层，并用于下一条指令。理解镜像层和commit的概念，是理解Docker镜像构建的关键。 镜像是被一层一层地”commit”上去的，而commit操作本身是由Docker容器执行的。docker build在执行一条指令时，会根据当前镜像层启动一个容器，Docker会在容器的层级文件系统最上层建立一层空的可读可写层(镜像层的内容对于容器来说是readonly的)，之后Docker容器执行指令，将执行结果写入可读可写层(并更新镜像Json文件)，最后再通过docker commit命令将可读可写层提交为一个新的镜像层。 Docker镜像层与镜像层之间是存在层级关系的，docker build会为Dockerfile每一条指令建立(commit)一个镜像层，并最终产生一个带标签(tag)的镜像，之前Dockerfile指令得到的镜像层(不会在构建完成后删除)是这个含标签镜像的祖先镜像。这样做的好处是最大化地复用镜像，不同的镜像之间可以共享镜像层，组成树形的镜像层级关系。 build cache在docker build过程中，如果发现本地有镜像与即将构建出来的镜像层一致时，则使用已有镜像作为Cache，充当本次构建的结果。从而加快build过程，并且避免构建重复的镜像。 那么docker是如何知道当前尚未构建的镜像的形态，并且与本地镜像进行比较呢？ Docker镜像由镜像文件系统内容和镜像Json文件两部分构成，前者即为docker commit提交的可读可写层，而镜像Json文件的作用为： 记录镜像的父子关系，以及父子间的差异信息 弥补镜像本身以及镜像到容器转换所需的额外信息 比如镜像Json文件中记录了当前镜像的父镜像，以及当前镜像与父镜像的差异(比如执行了哪条指令)，docker build则在这个基础上进行预测： 判断已有镜像和目标镜像(当前正在构建的镜像)是父镜像ID是否相同 评估已有镜像的Json文件(如执行了那条命令，有何变动)，与目标镜像是否匹配 如果条件满足，则可将已有镜像作为目标镜像的Cache，当然这种机制是并不完善的，比如当你执行的指令有外部动态依赖，此时可通过docker build --no-cache禁止使用Cache。 另外，基于build cache的机制，我们在写Dockerfile的时候，应该将静态安装，配置命令等尽可能放在Dockerfile前面，这样才能最大程度地利用cache，加快build过程。因为一旦Dockerfile前面有指令更新了并导致新的镜像层生成，那么该指令之后的镜像层cache也就完全失效了(树结构长辈节点更新了，子节点当然就不一样了)。 3. docker build 示例Dcokerfile: FROM ubuntu:14.04 # 创建一个100M的文件 /test RUN dd if=/dev/zero of=/test bs=1M count=100 RUN rm /test RUN dd if=/dev/zero of=/test bs=1M count=100 # 在根目录统计容器大小 ENTRYPOINT [&quot;du&quot;, &quot;-sh&quot;] build镜像： ▶ docker build . Sending build context to Docker daemon 599 kB Step 1 : FROM ubuntu:14.04 ---&gt; 1e0c3dd64ccd Step 2 : RUN dd if=/dev/zero of=/test bs=1M count=100 ---&gt; Running in d98f674c46f2 100+0 records in 100+0 records out 104857600 bytes (105 MB) copied, 0.0980112 s, 1.1 GB/s ---&gt; f3a606172d91 Removing intermediate container d98f674c46f2 Step 3 : RUN rm /test ---&gt; Running in 14544c0dc6a0 ---&gt; 7efc0655e95d Removing intermediate container 14544c0dc6a0 Step 4 : RUN dd if=/dev/zero of=/test bs=1M count=100 ---&gt; Running in 387be027ef2f 100+0 records in 100+0 records out 104857600 bytes (105 MB) copied, 0.0852024 s, 1.2 GB/s ---&gt; 38e3ea5c1412 Removing intermediate container 387be027ef2f Step 5 : ENTRYPOINT du -sh ---&gt; Running in e190adcbcce2 ---&gt; baec9103f182 Removing intermediate container e190adcbcce2 Successfully built baec9103f182 可以看到build过程为不断基于当前镜像启动中间容器(如d98f674c46f2容器基于1e0c3dd64ccd镜像层执行指令RUN dd if=/dev/zero of=/test bs=1M count=100并提交f3a606172d91镜像层)。通过docker history &lt;image&gt;可查看镜像层级关系： docker history baec9103f182 IMAGE CREATED CREATED BY SIZE COMMENT baec9103f182 4 minutes ago /bin/sh -c #(nop) ENTRYPOINT [&quot;du&quot; &quot;-sh&quot;] 0 B 38e3ea5c1412 4 minutes ago /bin/sh -c dd if=/dev/zero of=/test bs=1M cou 104.9 MB 7efc0655e95d 4 minutes ago /bin/sh -c rm /test 0 B f3a606172d91 4 minutes ago /bin/sh -c dd if=/dev/zero of=/test bs=1M cou 104.9 MB 1e0c3dd64ccd 3 weeks ago /bin/sh -c #(nop) CMD [&quot;/bin/bash&quot;] 0 B &lt;missing&gt; 3 weeks ago /bin/sh -c mkdir -p /run/systemd &amp;&amp; echo &#39;doc 7 B &lt;missing&gt; 3 weeks ago /bin/sh -c sed -i &#39;s/^#\\s*\\(deb.*universe\\)$/ 1.895 kB &lt;missing&gt; 3 weeks ago /bin/sh -c rm -rf /var/lib/apt/lists/* 0 B &lt;missing&gt; 3 weeks ago /bin/sh -c set -xe &amp;&amp; echo &#39;#!/bin/sh&#39; &gt; /u 194.6 kB &lt;missing&gt; 3 weeks ago /bin/sh -c #(nop) ADD file:bc2e0eb31424a88aad 187.7 MB 注意到其中一些镜像层的SIZE为0，这是因为该镜像层执行的命令不会影响到镜像的文件系统大小，这些命令会单独记录在镜像Json文件中。由于镜像的层级原理，Docker在执行RUN rm /test指令时，并没有真正将其当前镜像f3a606172d91中的/test文件真正删掉，而是将rm操作记录在镜像Json文件中(容器只能在其上层的可读写层进行更改操作)，最终我们得到的镜像大小约为400M。 然后我们基于得到镜像启动容器： docker run --rm baec9103f182 du: cannot access &#39;./proc/1/task/1/fd/4&#39;: No such file or directory du: cannot access &#39;./proc/1/task/1/fdinfo/4&#39;: No such file or directory du: cannot access &#39;./proc/1/fd/4&#39;: No such file or directory du: cannot access &#39;./proc/1/fdinfo/4&#39;: No such file or directory 296M . 我们的容器大小只是近300M，因此Docker镜像的大小和容器中文件系统内容的大小是两个概念。镜像的大小等于其包含的所有镜像层之和，并且由于镜像层共享技术的存在(比如我们再构建一个基于ubuntu14:04的镜像，将直接复用本地已有的ubuntu镜像层)，极大节省了磁盘空间。 Dockerfile Best Practices Dockerfile Reference Docker run Reference Docker 从入门到实践","tags":[{"name":"docker","slug":"docker","permalink":"http://wudaijun.com/tags/docker/"}]},{"title":"开发笔记(7) 记线上一次回档BUG","date":"2016-10-16T16:00:00.000Z","path":"2016/10/erlang-server-design7-cluster-bug-note/","text":"问题描述有十几个玩家报告被回档，几小时到一两天不等 问题背景在我们的集群架构中，集群有若干GS节点，每个GS节点可部署N个GS服务器，整个集群所有的玩家进程注册于cluster，我们通过为每个服开一个player_mgr来维护单服玩家状态，player_mgr维护{player_id, agent_pid, player_pid}三元组，用户处理多点登录，单服逻辑，离线玩家LRU等。cluster本身只提供服务注册/注销，如果做服务替换(如agent)，确保服务的唯一性(如player)应该由外部逻辑来确保，cluster并不知晓内部各种服务的特性。player进程启动/终止时，会向player_mgr和cluster分别注册/注销自己。 问题追踪 error日志中出现几十个rewrite player process(重写cluster中player服务)的错误日志，并且这些玩家基本都属于一个公会 所有玩家进程的启动(login, get_fork)均由player_mgr控制，player_mgr确保玩家进程唯一，依赖的是自身的State数据，而不是cluster，问题可能出在player_mgr 和 cluster 状态不一致上 写了个检查脚本，查出仍有有个别玩家存在于cluster而不在player_mgr中，这类玩家在get_fork或login时，player_mgr会重新开一个player进程，导致rewrite player process，此时同一时刻就存在两个player进程(老玩家进程Pid0，新玩家进程Pid1)，已有Agent消息会被重新路由(通过cluster服务查找)到Pid1进程上，而Pid0不在cluster和player_mgr中，不会被终止，但会不断写盘，称第三方进程，这是导致玩家回档的根本原因 现在问题焦点：为什么player_mgr维护的数据和cluster不一致(比cluster少) 在player_mgr LRU剔除玩家进程时，是先在自己State中删除玩家进程，再cast消失让玩家进程终止，最后在player_server:terminate中，再向player_mgr和cluster注销自己。那么存在这样一种情况：player_mgr LRU剔除玩家进程Pid0到 player_server:terminate从cluster中注销自己之间，新的login或get_fork请求到来，此时player_mgr再启动了Pid1，并且rewrite player process，那么当Pid0 terminate时，检查到cluster中当前服务不是自己，不会更新cluster，之后，Pid0还会向player_mgr注销自己，并且没有带上Pid进行Pid检查，因此将Pid1从player_mgr中删除了！至此，player_mgr和cluster出现了不一致，cluster中存在Pid1程，而player_mgr中没有。下一次玩家login或get_fork一个新的Pid2时，Pid1被rewrite，Pid1也就成了第三方进程 上面的概率看起来很小，但由于公会等组逻辑，可能导致N个玩家同时被get_fork起来，而LRU又是player_mgr统一定期(10分钟)清理的，因此如果alliance前后10分钟get_fork两次，问题出现的概率就被放大了，这也是本次出问题的玩家基本都在一个公会的原因 问题来源 player_mgr在没有确认玩家进程已经退出时(此时可能还有一堆消息没处理完)，就删除了它 玩家进程在向player_mgr注销自己时，没有做Pid检查，注销了其它进程(没有考虑容错) 问题修复线上热更的方案： player_mgr和cluster均在player terminate时才确认注销 服务注销时做Pid检查 在玩家进程定期存盘时检查其cluster和player_mgr状态，并stop掉第三方进程 问题反思 本质上来说，这次的问题源于： 数据冗余导致短暂的不一致状态(player_mgr和cluster不一致) 在这种不一致状态下的特定事件(player login/get_fork)，导致不一致的影响被放大(存在第三方玩家进程) 对这种不一致状态缺乏检查和处理，导致BUG(玩家回档) 在Code Review的过程中，还发现一些其它并发和异步问题。在多Actor异步交互模型中，调度时序，网络时延都可能导致状态不一致。在分布式系统中，想要从根本上杜绝不一致，是几乎不可能的(我们对同步和事务非常敏感)，因此我们不只是要从问题预防上考虑，还要从错误恢复上着手，让系统具备一定程度的”自愈能力”： 预防：减少不一致的可能性 减少数据冗余，将cluster作为数据的第一参照，player_mgr的优先级降低，并只用于全服逻辑 简化player_mgr的功能，如将离线玩家的LRU移到player自身去管理 恢复：检查并修复不一致 在服务启动/运行/终止时，加上检查和修复机制，并记录日志 跑定时脚本检查player_mgr和cluster的一致性，并予以临时修复和报警 最后，总结出的经验是，在分布式系统中，对问题的检查和修复，和问题的预防同样重要。","tags":[{"name":"erlang","slug":"erlang","permalink":"http://wudaijun.com/tags/erlang/"},{"name":"distribution","slug":"distribution","permalink":"http://wudaijun.com/tags/distribution/"}]},{"title":"goa - go web框架","date":"2016-09-19T16:00:00.000Z","path":"2016/09/goa-intro/","text":"一. 简介goa是基于微服务的go语言框架，能够有效帮助开发人员快速开发基于微服务的系统。它通过DSL和代码生成器来生成样板代码和辅助套件(如文档，客户端模块，客户端工具等)。这些生成数据均基于服务的设计描述，goa遵循单一数据源(Single Source of Truth, SSOT)原则，任何对设计的改变，都将自动反映到系统各处， goa可以分为三个部分： goa的设计语言是内置DSL，用于描述微服务的设计 goa代码生成器，用于根据DSL描述生成代码模块，辅助工具，和文档等 goa利用生成代码和用户代码来实现一个服务，并提供一个完全可插拨的框架 goa的特点： 重视框架设计(Design-Based)，将框架，文档，胶水代码和辅助工具作为一个整体来设计和描述 为用户生成了大量的代码(框架代码，胶水代码，测试代码，客户端工具等等)，上手快速 DSL，代码生成器，用户代码均使用Go语言编写，并且前两者使用plugin实现，可以替换 基于微服务，对RESTful API有非常好的支持，方便构建更高效，易于扩展的HTTP服务器 二. 使用1. 安装// 获取goa go get github.com/goadesign/goa go get github.com/goadesign/goa/goagen // 安装goagen go install github.com/goadesign/goa/goagen 2. DSL 服务设计123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package design // The convention consists of naming the design // package \"design\"import ( . \"github.com/goadesign/goa/design\" // Use . imports to enable the DSL . \"github.com/goadesign/goa/design/apidsl\")var _ = API(\"cellar\", func() &#123; // API defines the microservice endpoint and Title(\"The virtual wine cellar\") // other global properties. There should be one Description(\"A simple goa service\") // and exactly one API definition appearing in Scheme(\"http\") // the design. Host(\"localhost:8080\")&#125;)var _ = Resource(\"bottle\", func() &#123; // Resources group related API endpoints BasePath(\"/bottles\") // together. They map to REST resources for REST DefaultMedia(BottleMedia) // services. Action(\"show\", func() &#123; // Actions define a single API endpoint together Description(\"Get bottle by id\") // with its path, parameters (both path Routing(GET(\"/:bottleID\")) // parameters and querystring values) and payload Params(func() &#123; // (shape of the request body). Param(\"bottleID\", Integer, \"Bottle ID\", func()&#123; Minimum(0) // Do not allow for negative values. &#125;) &#125;) Response(OK) // Responses define the shape and status code Response(NotFound) // of HTTP responses. &#125;)&#125;)// BottleMedia defines the media type used to render bottles.var BottleMedia = MediaType(\"application/vnd.goa.example.bottle+json\", func() &#123; Description(\"A bottle of wine\") Attributes(func() &#123; // Attributes define the media type shape. Attribute(\"id\", Integer, \"Unique bottle ID\") Attribute(\"href\", String, \"API href for making requests on the bottle\") Attribute(\"name\", String, \"Name of wine\") Required(\"id\", \"href\", \"name\") &#125;) View(\"default\", func() &#123; // View defines a rendering of the media type. Attribute(\"id\") // Media types may have multiple views and must Attribute(\"href\") // have a \"default\" view. Attribute(\"name\") &#125;)&#125;) 上面的DSL主要用到的接口： API: 描述一个Service及其地址，协议规范等 Resource: 定义一个资源及其一系列相关的操作(Action)，以及这些操作所共用的一些属性 Action: 定义针对于某个资源的操作，包括方法(GET,POST等)，URL(可有多个)，参数(goa自动做类型检查，值检查等)等 Response: 定义一个响应，包括响应模板和承载内容(payload)，在代码中决定调用那个响应模板 MediaType: 定义Response返回的数据结构，一个Media可以有多个View，可在Response中指定返回的View goa本身DSL设计是RESTful的，通过Go的匿名函数，提供了非常强大的描述能力，如参数定义，参数检查，传输媒体，响应模板等。goa基于服务提供功能，每个API定义一个服务(Service)，每个服务有若干资源(Resource)，每个资源对应若干操作(Action)，每个操作(Action)有多种响应(Response)，每个响应可能返回不同媒介(Media)的不同视图(View)。当然goa提供了更好的层级控制和继承关系(如上例，Response返回的视图继承于Resource中定义的默认媒介(BottleMedia)的默认视图(default))。更详细的DSL设计文档参考goa dsl design和goa dsl api。 3. 生成代码通过goa根据单个DSL文件，即可生成一整套框架代码： cd src/cellar goagen bootstrap -d cellar/design goa会生成一堆代码，主要包括四个目录两个文件： app目录: 根据DSL，生成若干类，并将底层的HTTP服务器和DSL中的资源，路由结合起来 client目录: 配套的client包，包含对媒介类型的定义，和对请求响应的编解码 tool目录：根据client包生成的控制台工具，用于模拟客户端发送请求 swagger目录：包含对整个服务(API)的总体描述(Json和Yaml格式) main.go文件：主文件，挂载资源路由(BottleController)，启动服务 bottle.go文件：bottle资源的逻辑处理，即BottleController的Action实现 当改变DSL文件并再次用goagen生成代码时，goagen只会重新生成框架代码(app,client,tool,swagger)，而不会覆盖逻辑代码(main.go和bottle.go以及其它自定义文件)，做到框架与逻辑分离。 得到这些文件之后，我们直接编辑bottle.go，完善bottle资源的Action逻辑即可： 1234567891011121314151617181920// Show runs the show action.func (c *BottleController) Show(ctx *app.ShowBottleContext) error &#123; // BottleController_Show: start_implement // Put your logic here if ctx.BottleID == 0 &#123; return ctx.NotFound() &#125; bottle := app.GoaExampleBottle&#123; ID : ctx.BottleID, Name : fmt.Sprintf(\"Bottle #%d\", ctx.BottleID), Href : app.BottleHref(ctx.BottleID), &#125; // BottleController_Show: end_implement return ctx.OK(&amp;bottle)&#125; 至此，服务器就已经设计好了，剩下的HTTP Server，消息编解码，参数检查，路由，响应模板，甚至测试工具，gagen都已经为你做好了。 4. 运行和测试运行服务器： cd src/cellar go build -o cellar ./cellar 2016/09/20 00:26:41 [INFO] mount ctrl=Bottle action=Show route=GET /bottles/:bottleID 2016/09/20 00:26:41 [INFO] listen transport=http addr=:8080 通过curl测试： # 404 NOT FOUND curl -i localhost:8080/bottles/0 # 200 一个有效的BottleMedia View curl -i localhost:8080/bottles/1 # 400 无效参数 得到参数检查错误提示 curl -i localhost:8080/bottles/n 通过celler-cli工具测试： cd src/cellar/tool/cellar-cli go build -o cellar-cli # 使用帮助 ./cellar-cli # show bottle 命令的用法 ./cellar-cli show bottle # 发送HTTP请求 cellar-cli中集成了服务的地址信息 ./cellar-cli show bottle /bottles/1 最终我们只写了几十行的DSL和几行逻辑代码，就得到了一个基于微服务，RESTful风格的HTTP服务器，附以完整的客户端代码，测试工具，甚至服务API描述。更关键的是，这一套环境是SSOT(Single Source of Truth)的，更改一份DSL服务描述文件，整个服务器底层代码，胶水代码，测试环境，甚至API描述都会重新生成(不会影响到已有的逻辑代码)，这让整个服务保持高度一致性和可控性。 最后，以一段goa github上的描述收尾： There are a number of good Go packages for writing modular web services out there so why build another one? Glad you asked! The existing packages tend to focus on providing small and highly modular frameworks that are purposefully narrowly focused. The intent is to keep things simple and to avoid mixing concerns. This is great when writing simple APIs that tend to change rarely. However there are a number of problems that any non trivial API implementation must address. Things like request validation, response media type definitions or documentation are hard to do in a way that stays consistent and flexible as the API surface evolves. goa takes a different approach to building these applications: instead of focusing solely on helping with implementation, goa makes it possible to describe the design of an API in an holistic way. goa then uses that description to provide specialized helper code to the implementation and to generate documentation, API clients, tests, even custom artifacts. 完整示例参考goa learn guide和goa github。 三. 总结goa的优点： 先进的理念：Design-Based, DSL, Micro-Service, RESTful API，Plugins等 DSL，代码生成器，用户代码，辅助工具等一整套环境都用Go实现 一份服务设计(DSL文件)，生成了包括框架代码，辅助(胶水)代码，测试代码，客户端工具等一整套环境(SSOT) 上手简单，功能强大 文档齐全，社区活跃度高 后续会继续关注这个框架，尽快拿到实践中用用。","tags":[{"name":"go","slug":"go","permalink":"http://wudaijun.com/tags/go/"},{"name":"goa","slug":"goa","permalink":"http://wudaijun.com/tags/goa/"}]},{"title":"Rebar3 Erlang/OTP构建利器","date":"2016-09-09T16:00:00.000Z","path":"2016/09/erlang-rebar3/","text":"一. 依赖管理1. 包依赖和源码依赖Rebar3支持两种依赖： {deps,[ %% 包依赖 rebar, {rebar,&quot;1.0.0&quot;}, {rebar, {pkg, rebar_fork}}, % rebar app under a different pkg name {rebar, &quot;1.0.0&quot;, {pkg, rebar_fork}}, %% 源码依赖 {rebar, {git, &quot;git://github.com/erlang/rebar3.git&quot;}}, {rebar, {git, &quot;http://github.com/erlang/rebar3.git&quot;}}, {rebar, {git, &quot;https://github.com/erlang/rebar3.git&quot;}}, {rebar, {git, &quot;git@github.com:erlang/rebar3.git&quot;}}, {rebar, {hg, &quot;https://othersite.com/erlang/rebar3&quot;}}, {rebar, {git, &quot;git://github.com/erlang/rebar3.git&quot;, {ref, &quot;aef728&quot;}}}, {rebar, {git, &quot;git://github.com/erlang/rebar3.git&quot;, {branch, &quot;master&quot;}}}, {rebar, {git, &quot;git://github.com/erlang/rebar3.git&quot;, {tag, &quot;3.0.0&quot;}}} ]} Rebar3通过hex.pm来管理包依赖，在使用之前，需要通过rebar3 update从hex.pm更新包索引，并将包索引信息缓存到本地(~/.cache/rebar3/)。之后Rebar3便能正确解析包依赖，对应用程序使用上来说，两者没有明显区别。 2. 升级依赖在使用Rebar2的时候，如果项目依赖一个指向分支的dep，就会出现这种情况： 这个dep有远程分支更新时，rebar get-deps不会自动拉取更新，通常你只能进入dep目录执行git pull，或者删除该dep重新执行rebar get-deps。 项目成员各自的工作目录deps版本可能不一致，并且一些很久没更新的依赖可能在你部署新环境时(此时所有依赖都指向最新)出现问题。 所以在Rebar2的reabr.config中定义deps，都应该尽量使用tag, commitid来指定，而不是直接指向分支。那么Rebar3是如何解决这个问题的呢？ Rebar3解决此问题的核心在rebar.lock文件，该文件内容如下： {&quot;1.1.0&quot;, [{&lt;&lt;&quot;goldrush&quot;&gt;&gt;,{pkg,&lt;&lt;&quot;goldrush&quot;&gt;&gt;,&lt;&lt;&quot;0.1.8&quot;&gt;&gt;},1}, {&lt;&lt;&quot;lager&quot;&gt;&gt;,{pkg,&lt;&lt;&quot;lager&quot;&gt;&gt;,&lt;&lt;&quot;3.2.1&quot;&gt;&gt;},0}]}. [ {pkg_hash,[ {&lt;&lt;&quot;goldrush&quot;&gt;&gt;, &lt;&lt;&quot;2024BA375CEEA47E27EA70E14D2C483B2D8610101B4E852EF7F89163CDB6E649&quot;&gt;&gt;}, {&lt;&lt;&quot;lager&quot;&gt;&gt;, &lt;&lt;&quot;EEF4E18B39E4195D37606D9088EA05BF1B745986CF8EC84F01D332456FE88D17&quot;&gt;&gt;}]} ]. 该文件是项目当前使用依赖库的一个版本快照。当一个依赖被获取和锁定，Rebar3将从依赖中提取版本信息并写入rebar.lock文件中，该文件应该加入GIt仓库，并且由专人维护，这样只要rebar.lock一致，各本地仓库的依赖库版本就是一致的。 依赖升级分为两种，一种是直接通过rebar upgrade [dep]进行源码更新或包更新(只能更新Top Level依赖)。另一种是rebar.config发生变动，比如去除了某个依赖，此时需要rebar unlock [dep]命令来清理rebar.lock文件。 相关命令： rebar3 update // 更新包索引 rebar3 pkgs // 列出所有可用的包 rebar3 deps // 列出所有一级(Top Level)依赖 rebar3 tree // 以树形结构查看依赖 rebar3 compile // 获取并编译依赖 rebar3 upgrade [dep] // 升级依赖 rebar3 lock [dep] // 锁定依赖 rebar3 unlock [dep] // 解锁依赖 二. 构建rebar3 new app [appname] rebar3 new lib [libname] Rebar3建议应用程序按照OTP规范目录进行组织： ├── LICENSE ├── README.md ├── apps │ └── myapp │ └── src │ ├── myapp.app.src │ ├── myapp_app.erl │ └── myapp_sup.erl ├── config │ ├── sys.config │ └── vm.args ├── lib │ └── mylib │ ├── LICENSE │ ├── README.md │ ├── rebar.config │ └── src │ ├── mylib.app.src │ └── mylib.erl └── rebar.config 这样无需在rebar.config中指定sub_dirs，Rebar3会自动将lib和apps作为搜索路径。 Rebar3没有get-deps命令，通过rebar3 compile即可编译项目，并自动获取和编译不存在的依赖，Rebar3将所有编译文件和Release信息都置于_build目录下。默认apps，deps和lib下的应用都被编译到_build/default/lib中。要指定应用目录和输出目录等选项，请参考：Rebar3配置。 三. 发布1. 发布环境Rebar3放弃了reltool而使用relx作为发布工具。并且将relx.config内容集成到rebar.config当中，通过rebar new release [appname]可创建一个发布，rebar.config内容如下： {erl_opts, [debug_info]}. {deps, []}. %% 定义默认发布环境(default环境) {relx, [{release, { myapp, &quot;0.1.0&quot; }, [myapp, sasl]}, {sys_config, &quot;./config/sys.config&quot;}, {vm_args, &quot;./config/vm.args&quot;}, %% 当dev_mode==true时 _build/default/rel/myapp/lib/目录下的库其实是_build/default/lib目录下对应lib的软链接，这样重新编译后，无需重新发布，重启或热加载代码即可 {dev_mode, true}, %% 是否在发布目录中包含虚拟机 即为一个独立的运行环境 {include_erts, false}, {extended_start_script, true}] }. %% 定义其它发布环境 %% 参数使用覆盖(override)机制，即这里面没有定义的参数，将使用默认发布环境(default)配置 {profiles, [{prod, [{relx, [{dev_mode, false}, {include_erts, true}]}] }] } Rebar3中有发布环境(profiles)的概念，如开发环境(default)，生产环境(prod)，它们可以独立定义编译参数(erl_opts)，发布参数(dev_mode, include_erts)，甚至依赖应用(deps)。目前Rebar3支持四种环境定义： default：默认环境，也就是rebar.config中最外部定义的环境 prod：生产环境，通常在此环境下将使用库的完整发布包(而不是软链接)，有更严格的编译选项，并且可能还要包含Erlang运行时所需要的所有环境 native：原生环境，强制使用HiPE编译，从而得到更快的编译速度 test：测试环境，将加载一些额外的库(如meck)，打开调试信息，用于跑测试代码 不同发布环境将发布在不同的目录下，如prod环境默认将生成在_build/prod/下，无论顶层应用采用何种发布环境，依赖将始终只能使用prod环境发布。并且只有顶层依赖的default环境，可以被保存到rebar.lock中。 rebar3 release将按照default环境发布应用，通过rebar3 as prod release可以将应用在生产环境发布。具体环境配置及命令参考Rebar3环境。 2. 发布多个应用Rebar3支持在rebar.config中定义多个应用的发布，多个应用可以共享配置： {relx, [{release, {myapp1, &quot;0.0.1&quot;}, [myapp1]}, {release, {myapp2, &quot;0.1.0&quot;}, [myapp2]}, % 共用配置 {sys_config, &quot;config/sys.config&quot;}, {vm_args, &quot;config/vm.args&quot;}, {dev_mode, true}, {include_erts, false}, {extended_start_script, true}]}. 也可以独立配置： {relx, [ {release, {myapp1, &quot;0.0.1&quot;}, [myapp1], % 注意配置顺序和格式 各应用的独立配置是一个PropList [{sys_config, &quot;config/sys1.config&quot;}, {vm_args, &quot;config/vm1.args&quot;}] }, {release, {myapp2, &quot;0.1.0&quot;}, [myapp2], [{sys_config, &quot;config/sys1.config&quot;}, {vm_args, &quot;config/vm1.args&quot;}, {overlay}] }, {dev_mode, true}, {include_erts, false}, {extended_start_script, true}]}. 3. 应用依赖定义于rebar.config deps中的依赖被获取后放在_build/default/lib目录下，默认并不会打包到应用的发布目录_build/default/rel/myapp/lib中，你需要在relbar.config的relx中指定应用依赖： {relx, [{release, { myapp, &quot;0.1.0&quot; }, [ % 指定应用依赖 mylib会先于myapp被启动 mylib, myapp] }, {sys_config, &quot;./config/sys.config&quot;}, {vm_args, &quot;./config/vm.args&quot;}, {dev_mode, true}, {include_erts, false}, {extended_start_script, true}] }. 那么对于一些辅助lib呢，我们希望它被打包在应用发布目录中，但不希望它们被启动(它们可能根本不能启动)，一种方法是将mylib指定为{mylib, load}(参见Issue1, Issue2)，列表中的依赖项默认被relx解释为{mylib, permanent}，即以常驻的方式启动应用。 4. OverlaysOverlay允许用户定义一些文件模板和部署准备工作，如拷贝文件，创建文件夹等： {relx, [ {overlay_vars, &quot;vars.config&quot;}, {overlay, [{mkdir, &quot;log/sasl&quot;}, {template, &quot;priv/app.config&quot;, &quot;etc/app.config&quot;}， % root_dir是relx提供的变量 代表项目根目录 {copy, &quot;\\{\\{root_dir\\}\\}/configures&quot;, &quot;./&quot;}]} ]}. Overlay可以如sys_config和vm_config一样，放在各应用的独立发布配置中。 更多关于Rebar3发布流程，发布配置，以及库升级等，参考Rebar3发布。 四. 总结Rebar3无疑是个好东西，更先进的依赖管理，多节点发布，发布环境的概念，都是Rebar2 + Reltool所不能实现的，当前我们项目就使用的Rebar2.x，用于部署一个多节点的集群，遇到的问题： 依赖管理：各本地版本不一致问题，Rebar3的lock为依赖的一致性提供了保证。 多节点部署：Rebar2.x需要为每个节点创建release(create-node)，需要维护N份reltool.config和一份rebar.config。在Rebar3中只需一个rebar.config文件。并且可以灵活定义各节点配置文件(vm.args, sys.config)路径，更有利于项目结构管理和可读性。 开发模式：在本地开发时，Rebar2.x的generate和upgrade太慢了，前者可用二进制发布自己写脚本替代(用erl_call和节点通信)，后者可用reloader实现热更，这样提高了部署速度，却要自己维护节点交互脚本。Rebar3的dev_mode完美解决了这个问题。 环境管理：这一块的用处还有待挖掘和摸索。 Rebar3目前主要的缺点，在于relx文档匮乏，提供了很多好东西，但能传达到用户让用户理解和用上的很少。翻遍了relx wiki，也没有找到应用独立配置环境(sys_config, vm_args等)的方法，最后是看了其配置解析模块rlx_config.erl才猜出来的格式= =。 五. 参考： Rebar3文档 Rebar3文档中文翻译(部分) relx wiki OTP Release 结构","tags":[{"name":"erlang","slug":"erlang","permalink":"http://wudaijun.com/tags/erlang/"}]},{"title":"Go 笔记(3) 面向对象和接口","date":"2016-09-08T16:00:00.000Z","path":"2016/09/go-notes-3-object-oriented/","text":"探索Go类型扩展，类和继承，以及接口的用法和实现。 面向对象1. 类型扩展12345678910package main import \"fmt\" // 定义了一个新类型:Integer，与int不能直接比较/赋值type Integer int func (a *Integer) Add(b Integer) Integer&#123; return *a + b&#125; 2. 类和继承在Go中，传统意义上的类相当于是对struct的类型扩展： 1234567891011121314151617package main import \"fmt\" type Rect struct&#123; x, y float64 w, l float64&#125; func (r Rect) Area() float64&#123; return r.l * r.w&#125; func main()&#123; c := Rect&#123;1,1,4,4&#125; fmt.Println(c.Area())&#125; Go中的继承通过匿名组合实现： 1234567891011121314151617181920212223242526272829303132package main import \"fmt\" type Base struct &#123; Name string&#125;func (base *Base) Foo() &#123; fmt.Println(\"Base Foo()\")&#125;func (base *Base) Bar() &#123; fmt.Println(\"Base Bar()\")&#125;// 以组合的方式 定义继承// 当derive.xxx在Derive中未找到时，将从基类Base中查找// 也可通过derive.Base.xxx直接引用基类Base的方法或成员type Derive struct &#123; Base age int // 这里的同名成员将覆盖Base中的成员&#125;// 重写基类方法func (d *Derive) Bar() &#123; fmt.Println(\"Derive Bar()\")&#125; func main()&#123; b := Base&#123;\"name\"&#125; d := Derive&#123;b, 99&#125; d.Foo() // == d.Base.Foo() 语法糖，Foo()函数的接收者只能是Base* d.Bar() fmt.Println(d.Name,d.age)&#125; 还可以以指针的方式从一个类型派生： 1234type Derive struct &#123; *Base ...&#125; 这个时候Derive的初始化需要提供一个Base的指针，它存在的意义类似于C++中的虚基类，Go将C++面向对象中一些”黑盒子”放到了台面上来，如this指针(作为一个特殊的参数显现出来)，虚函数表(Go中不允许派生类指针到基类指针的隐式转换，也就无需虚函数表来实现多态)，虚基类(通过显式基类指针，简洁明了的实现了这一需求)。 Go中没有private public等关键字，要使符号对其它包可见，则需要将该符号定义为大写字母开头。如Base中的Name能被其它引用了Base所在包的代码访问到，而Derive中age则不能。Go中没有类级别的访问控制。 接口接口(interface)是一系列方法声明的组合，同时它本身也是一个类型。 1. 非侵入式接口侵入式接口是指实现类需要明确声明实现了某个接口，目前C++/Java等语言均为侵入式接口。这类接口的缺点是类的实现方需要知道需求方需要的接口，并提前实现这些接口。这给类设计带来很大困难，因为设计类的时候，你并不知道也不应该关心它会被怎么使用。 GO中的接口是非侵入式的，接口与类分离，类只需要关心它应该有那些功能(函数)，而无需操心其应该满足哪些接口(契约)，一个类只要实现了某个接口的所有函数，那么它就实现了这个接口： 12345678910111213141516171819202122232425262728293031323334353637383940type IReader interface&#123; Read(buf []byte) (n int, err error)&#125; type IWriter interface&#123; Write(buf []byte) (n int, err error)&#125; type IFile interface&#123; Read(buf []byte) (n int, err error) Write(buf []byte) (n int, err error)&#125; type IStream interface&#123; Read(buf []byte) (n int, err error) Write(buf []byte) (n int, err error)&#125; type IDevice interface&#123; Name() string&#125; // File定义无需指定实现接口，直接实现其方法即可// 根据File类的实现，可以得到：// File类实现了 IDevice接口// File*类实现了以上所有接口type File struct &#123; // ...&#125;func (f *File) Read(buf []byte) (n int, err error)&#123; // ... return&#125;func (f *File) Write(buf []byte) (n int, err error)&#123; // ... return&#125;func (f File) Name() (s string)&#123; return&#125; Go的非侵入式接口的意义： Go语言的标准库，没有复杂的继承树，接口与类之间是平坦的，无需绘制类库的继承树图。 实现类的时候，只需要关心自己应该提供哪些方法(自身功能)，不用再纠结实现哪些接口，接口由使用方按需定义，而不用事前规划。 不用为了实现一个接口而导入一个包，因为多引用一个外部的包，就意味着更多的耦合。接口由使用方按自身需求来定义，使用方无需关心是否有其他模块定义过类似的接口。 2. 接口赋值由于接口本身是一种类型，因此它可被赋值。接口赋值分为两种：将对象赋值给接口和将接口赋值给接口： 12345678910111213141516171819202122// 1. 将对象赋值给接口// 赋值条件：对象需实现该接口f := File&#123;&#125;// okvar I1 IDevice = f// ok. Go会根据 func (f File) Name() 自动生成 func (f *file) Name()方法var I2 IDevice = &amp;f// error. File类实现的IFile接口中，有函数的接收者为File*// func (f *File) Read(buf []byte) 不能转化为 func (f File) Read(buf []byte)// 因为前者可能在函数中改变f，后者不能，可能造成语义上的不一致var I3 IFile = f// okvar I4 IFile = &amp;f// 赋值完成之后 可通过接口直接调用对象方法I1.Name()// 2. 将接口赋值给接口// 赋值条件：左值接口需是右值接口的子集var I5 IReader = I1 // errorvar I6 IFile = I3 // okvar I7 IReader = I3 // ok 3. 接口查询既然我们可以将对象或者接口赋值给接口，那么也应该有方法能让我们从一个接口查询出其指向对象的类型信息和接口信息： f := File{} // 接口查询 var I1 IDevice = f // 判断接口I1指向的对象是否实现了IFile接口 I2, ok := I1.(IFile) // ok = false File类型没有实现IFile接口 File*类型实现了 // 类型查询 // 方法一 type assertions f2, ok := I1.(File) // ok = true // 方法二 type switch // X.(type)方法只能用在switch语句中 switch(I1.(type)){ case int: // 如果I1指向的对象为int case File: // 如果I1指向的对象为File ... } 4. 接口组合前面的IFile接口定义等价于： 1234type IFile interface&#123; IReader IWriter&#125; 接口组合可以以更简便的方式复用接口类似于类继承，只不过没有成员变量。 5. 空接口在Go中的任何对象都满足空接口interface{}，所以interface{}可以指向任何对象： 1234var v1 interface&#123;&#125; = 1var v2 interface&#123;&#125; = \"abc\"var v3 interface&#123;&#125; = struct&#123; x int &#125;&#123;1&#125;var v4 interface&#123;&#125; = v3 interface{}比C++中的void*更强大，比template&lt;&gt;更灵活，结合接口查询和反射，构建底层代码变得非常容易。 6. 反射简单概括，反射一种检查存储在接口变量(任意类型值)中的“类型-值对”的机制。任何接口变量(包括空接口变量)都包含了其对应的具体类型和值信息： 12345678var f = new(File)var r IReaderr = ffmt.Println(reflect.TypeOf(r), reflect.ValueOf(r))// 输出: *main.File &amp;&#123;&#125;var w IWriterw = r.(IWriter)... IReader接口变量只提供了访问Read方法的能力，但其接口变量仍然保存了有关该值的所有类型信息，因此我们可以通过接口查询得到IWriter接口变量。接口的静态类型决定了哪些方法可以通过接口变量调用，但接口变量本身可能包含更大的方法集。 有了这个机制，我们才能通过反射从任意接口变量，获取对象完整的属性。关于反射的API都在reflect包中提供，通过reflect.TypeOf和reflect.ValueOf获取接口变量的Type和Value，reflect为Type和Value提供了大量的方法，如Type.Kind(),Value.Interface()等。 现在我们尝试通过反射修改接口变量的值： var x float64 = 3.4 v := reflect.ValueOf(x) v.Set(4.1) // error: cannot use 4.1 (type float64) as type reflect.Value in argument to v.Set 由于在refect.ValueOf(x)中操作的是x的拷贝，因此实际上v.Set即使能操作成功，也不能如我们预期一般修改x的值。因此reflect提供Value.CanSet()来辨别这类不能成功修改的值： CanSet reports whether the value of v can be changed. A Value can be changed only if it is addressable and was not obtained by the use of unexported struct fields. If CanSet returns false, calling Set or any type-specific setter (e.g., SetBool, SetInt) will panic. 我们可以通过*float64类型的反射来修改x的值: 12345678910111213141516var x float64 = 3.4p := reflect.ValueOf(&amp;x)fmt.Println(\"type of p:\", p.Type())fmt.Println(\"CanSet of p:\" , p.CanSet())v := p.Elem()fmt.Println(\"CanSet of v:\" , v.CanSet())// v的地址是有效的(保存在p.Value()中) 因此可以修改v.SetFloat(7.1)fmt.Println(v.Interface())fmt.Println(x)// 输出:// type of p: *float64// CanSet of p: false// CanSet of v: true// 7.1// 7.1 推荐阅读: 接口和反射的好文：https://blog.go-zh.org/laws-of-reflection","tags":[{"name":"go","slug":"go","permalink":"http://wudaijun.com/tags/go/"}]},{"title":"Go 笔记(1) 常用数据结构及实现","date":"2016-09-08T16:00:00.000Z","path":"2016/09/go-notes-1-datastructures/","text":"学习一下go中常用的几种数据结构，结合源码了解其实现原理。 一. 类型系统1. array123456789func f(x [2]int)&#123; x[1] = 9&#125;func main()&#123; a := [3]int&#123;1,2,3&#125; b := [2]int&#123;4,5&#125; f(a) // error: cannot use a (type [3]int) as type [2]int in argument to f f(b) // 数组是值语义 因此f无法改变b中元素内容&#125; array的特性: 固定大小，且大小为类型的一部分 数组元素在内存中连续存放 值语义: 数组本身(传参会完整拷贝数组) 2. slice数组切片slice(切片)，提供描述array部分连续元素的能力。 A slice is a data structure describing a contiguous section of an array stored separately from the slice variable itself. A slice is not an array. A slice describes a piece of an array. slice只持有array的引用，而不会拷贝元素，因此它在实现上只需持有指向array元素的pointer和slice长度length即可。但由于slice的length可以收缩或扩张，因此slice还需要一个字段capacity来保存其最初引用的array的size，当length &gt; capacity时，说明对array的访问越界，触发panic错误。 因此slice一共有三个字段： 12345type sliceHeader&#123; Length int // slice长度 Capacity int // slice引用的array size Elem *ElemType // 指向slice第一个元素array中的地址&#125; 比如: 12345// 直接创建slice 等价于:// tmp := [5]int&#123;2,3,5,7,11&#125;// a := tmp[0:5]a := []int&#123;2,3,5,7,11&#125;b := a[1:3] 此时a,b的sliceHeader示意图为: 由于slice b在slice a中的起始偏移为1，因此 cap(b) = cap(a)-1 = 4。但b只能访问到a[1],a[2]两个元素: 1234567// 尝试访问&gt;=length(2)的元素，会触发panic errorfmt.Println(b[2])// 等价于 c := b[0:len(b)] c和b引用完全相同的数组切片c := b[:]// 虽然b只能访问数组[1],[2]两个元素，但d可以在[0,cap(b)]再次切片扩展引用的数组范围d := b[0:cap(b)]fmt.Println(d[3]) // 11 那么slice这种数组切片的概念，究竟带来了什么好处？比如我们有一个操作，要去掉数组的首尾元素，在C中，我们会创建(动态分配)一个新数组，然后将arr[1,n-1)拷贝出来。在C++中，有vector会方便一些，但移除元素会导致后续元素移动拷贝开销。而在Go中，slice = slice[1:len(slice)-1]即可完成操作，这中间不会涉及到内存分配，移动拷贝等，是个非常高效的操作。当然，由于slice是引用的数组元素，因此slice修改数组元素时，对其它引用到该元素的slice也是可见的。 下面来说说slice的值语义。前面提到的sliceHeader，实际就是slice的值语义，我们创建一个slice，在底层就创建了一个sliceHeader结构体。在参数传递时，将会拷贝sliceHeader，但由于sliceHeader中持有指针，因此在调用函数内可修改数组元素，但无法修改sliceHeader结构体的成员值： 12345func Extend(slice []int, element int )&#123; n := len(slice) slice = slice[0 : n+1] // 不会影响到传入的slice的length slice[n] = element // 修改了数组内容，对传入的slice可见&#125; 再次摘录一段golang blog关于slice值语义的描述: It’s important to understand that even though a slice contains a pointer, it is itself a value. Under the covers, it is a struct value holding a pointer and a length. It is not a pointer to a struct. BTW，在Go里面的参数传递都是值传递的，只是针对各种类型，其值语义不同，比如int,array它们的值语义就是数据本身，不包含对外的引用(指针)，因此在传参时会完整拷贝整个数据，当然，这里的拷贝是浅拷贝，比如对指针数组这类结构而言，仍然是有副作用的，但这是应用层的东西，就数组容器本身而言，是值拷贝的。而对slice来说，其值语义中包含对数组的引用，因此在传参时，其引用内容可能被修改，但其值语义(sliceHeader)本身仍然是完整拷贝的。 动态数组前面提到slice本质上是数组切片，但slice本身也可以作为动态数组: 1234567891011121314151617181920func main()&#123; a := [5]int&#123;1,2,3,4,5&#125; s := a[0:3] fmt.Println(\"cap: \",cap(s),\"len: \",len(s),\"slice: \",s,\"array: \",a) // len=3 cap=5 capacity足够 无需重新分配 因此修改会作用于a之上 s = append(s, 6, 7) fmt.Println(\"cap: \",cap(s),\"len: \",len(s),\"slice: \",s,\"array: \",a) // len=5 cap=5 append通过make()重新分配新的slice 并通过copy()拷贝已有元素 // 此后s不再指向a 而指向新分配的连续内存空间 s = append(s, 8) fmt.Println(\"cap: \",cap(s),\"len: \",len(s),\"slice: \",s,\"array: \",a) // 对s的修改将不在作用于a上 s[0] = 0 fmt.Println(\"cap: \",cap(s),\"len: \",len(s),\"slice: \",s,\"array: \",a)&#125;// 输出:cap: 5 len: 3 slice: [1 2 3] array: [1 2 3 4 5]cap: 5 len: 5 slice: [1 2 3 6 7] array: [1 2 3 6 7]cap: 10 len: 6 slice: [1 2 3 6 7 8] array: [1 2 3 6 7]cap: 10 len: 6 slice: [0 2 3 6 7 8] array: [1 2 3 6 7] append会在len(s)+添加的元素个数&gt;cap(s)时，重新分配(make)一个slice，拷贝(copy)已有元素，添加新元素，最后返回这个新的slice。在使用append时，需要保存其返回值，因为append传入的是slice的值，也就是sliceHeader结构体，当slice capacity扩展时，append函数内不能修改sliceHeader中的Length和Capacity字段，因此需要返回一个新的sliceHeader。 为了避免混淆，不要像上例一样将slice的切片特性和动态数组特性混用，使用动态数组时，使用空的slice(var s []int)或make(make([]int, len, cap))初始化一个slice会比较好。 3. stringGo中的string更像是C中的字符串字面量，而不是字符数组： 12345678910111213141516str := \"Hello, 世界\"//str[0] = 'X' // error 不可改变字符串(类似字面常量)// 字符串可通过 + 进行拼接str += \" !\"// 以ANSI字符遍历 ch是一个byte n=15(每个中文在UTF-8中占3个字节)n := len(str)for i := 0; i&lt; n; i++ &#123; ch := str[i] fmt.Println(i, ch)&#125;// 以Unicode字符遍历 ch是一个rune 而不是byte 此时遍历得到11个Unicode字符for i, ch := range str&#123; fmt.Println(i, ch) &#125; 在实现上，string是个read-only byte slice，另外，string的”sliceHeader”没有capacity字段： s := &quot;hello&quot; t := s[2:3] // &quot;l&quot; v := t[0:2] // 没有capacity字段，无法扩展，触发panic error: out of range 由于string的slice特性，len(s)操作非常高效，字符串切割也给代码处理带来很高的灵活度，如官方runtime/string.go的atoi函数是这样写的: func atoi(s string) int{ n := 0 for len(s) &gt; 0 &amp;&amp; &#39;0&#39; &lt;= s[0] &amp;&amp; s[0] &lt;= &#39;9&#39; { n = n*10 + int(s[0]) - &#39;0&#39; s = s[1:] } return n } PS，slice的这种切片特性，与Erlang的refc binary和sub binary实现有相似之处，这种高效的处理方案有个老大难问题，那就是slice string未释放，那么它引用的string本身也不会被GC，哪怕只引用了很小一部分。 4. mapmap通过hash表实现，实现位于runtime/hashmap.go，以下是主要字段: 123456789101112131415161718192021const( bucketCntBits = 3 bucketCnt = 1 &lt;&lt; bucketCntBits)type hmap struct &#123; count int // # live cells == size of map. Must be first (used by len() builtin) flags uint8 B uint8 // log_2 of # of buckets (can hold up to loadFactor * 2^B items) buckets unsafe.Pointer // array of 2^B Buckets. may be nil if count==0. oldbuckets unsafe.Pointer // previous bucket array of half the size, non-nil only when growing evacuate uintptr // progress counter for evacuation (buckets less than this have been evacuated)&#125; // A bucket for a Go map.type bmap struct &#123; tophash [bucketCnt]uint8 // Followed by bucketCnt keys and then bucketCnt values. // Followed by an overflow pointer.&#125; 摘自源码注释： A map is just a hash table. The data is arranged into an array of buckets. Each bucket contains up to 8 key/value pairs. The low-order bits of the hash are used to select a bucket. Each bucket contains a few high-order bits of each hash to distinguish the entries within a single bucket. If more than 8 keys hash to a bucket, we chain on extra buckets. When the hashtable grows, we allocate a new array of buckets twice as big. Buckets are incrementally copied from the old bucket array to the new bucket array. hmap的buckets数组大小为2^B，通过取余(hash(key)&amp;(1&lt;&lt;B-1))可得到key对应的bucket在buckets数组中的下标，每个bucket可以容纳2^bucketCntBits=8个key/value对，落到该桶的key个数超过8个时，会在堆上分配一个新的bucket，并挂在链表末，因此go hashmap通过链表(8个元素一组)来解决hash碰撞问题。 go的hash map使用的是可扩展hash算法，在负载因子loadFactor(hmap.count/(1&lt;&lt;B))大于某个值(这个值太大会导致overflow buckets过多，查找效率降低，过小会浪费存储空间，经源码作者测试确认为6.5)时，进行hash扩展。此时B=B&lt;&lt;1，原有buckets由oldbuckets指向，新的buckets重新分配，此时由于hash表大小变更，部分key得到的buckets下标也会改变，因此需要将oldbuckets中的数据按照新的hash表大小重新迁移(evacuate)，出于效率考虑，这个操作是增量进行的，在hash map每次写入时，都会尝试迁移两个bucket(以及后续overflow bucket)，一个是写入的目标bucket(局部迁移)，一个是hmap.evacuate指向的bucket(增量迁移)，这样兼顾局部性和全局性，同时也能保证在新的buckets loadFacotr到达6.5前，所有迁移工作一定能完成。迁移工作完成后，oldbucket置为nil。PS: hash map通过bucket的tophash[0]来标记bucket的迁移状态，保留的标记值为0-3，key的tophash在这个范围内时，会被+4修正 上述是基于go1.5 hashmap实现，在go1.8中，添加了sameSizeGrow，当overflow buckets的数量超过一定数量(2^B)而负载未大于阀值6.5时，此时可能存在部分空的bucket，即bucket未有效利用，这时会触发sameSizeGrow，即B不变，但走数据迁移流程，将oldbuckets的数据重新紧凑排列提高bucket的利用率。当然在sameSizeGrow过程中，不能触发loadFactorGrow。 下面来看个结构图: 再来看Key查找过程(简化版): 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455// From go 1.8.1 src/runtime/hashmap.gofunc mapaccess1(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer &#123; if h == nil || h.count == 0 &#123; return unsafe.Pointer(&amp;zeroVal[0]) &#125; // 并发检查 go hashmap不支持并发访问 if h.flags&amp;hashWriting != 0 &#123; throw(\"concurrent map read and map write\") &#125; alg := t.key.alg hash := alg.hash(key, uintptr(h.hash0)) m := uintptr(1)&lt;&lt;h.B - 1 b := (*bmap)(add(h.buckets, (hash&amp;m)*uintptr(t.bucketsize))) // step1: 找到bucket // 如果oldbuckets未迁移完成 则找打oldbuckets中对应的bucket(低B-1位) // 否则为buckets中的bucket(低B位) if c := h.oldbuckets; c != nil &#123; if !h.sameSizeGrow() &#123; m &gt;&gt;= 1 &#125; oldb := (*bmap)(add(c, (hash&amp;m)*uintptr(t.bucketsize))) if !evacuated(oldb) &#123; b = oldb &#125; &#125; top := uint8(hash &gt;&gt; (sys.PtrSize*8 - 8)) if top &lt; minTopHash &#123; top += minTopHash &#125; for &#123; // step2: 比较tophash for i := uintptr(0); i &lt; bucketCnt; i++ &#123; if b.tophash[i] != top &#123; continue &#125; // dataOffset为key数组在bucket(bmap结构)中的起始偏移 k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) if t.indirectkey &#123; k = *((*unsafe.Pointer)(k)) &#125; // step3: 比较key if alg.equal(key, k) &#123; v := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.valuesize)) if t.indirectvalue &#123; v = *((*unsafe.Pointer)(v)) &#125; return v &#125; &#125; b = b.overflow(t) if b == nil &#123; return unsafe.Pointer(&amp;zeroVal[0]) &#125; &#125;&#125; 限于理解深度，其它一些细节没有提到，比如对不含pointer的key/value优化，另外，go map还针对常用key类型(如int32,int64,string)进行了特例优化，代码位于src/runtime/hashmap_fast.go。以下是上面已经提到的一些小的优化细节： key value采用k1,k2,..v1,v2,…排列，而不是k1,v1,k2,v2，这是出于内存对齐考虑，节约空间 tophash可用于加快key的查找，同时用于标记key的迁移状态 map大小是2的幂，因此hash值可快速求余: hash(key)&amp;(1&lt;&lt;B-1) hash map的增量式扩展，sameSizeGrow 其它: go map不支持并发 go map目前只有扩展 没有收缩操作(shrink) go map迁移时，会创建新的bucket，而不会复用oldbucket中的overflow bucket(作者TODO里面) 值语义：如hmap结构体所示，buckets为bucket指针数组，那么对key,value的操作都是引用语义的。 5. channelchannel是goroutine用于数据交互的通道，和Erlang的Actor以通信实体为第一类对象不同(Actor模型)，Go以通信介质作为第一类对象(CSP模型)，channel支持多写入者和读取者，并且可通过缓冲来实现同步/异步(一定数量)通信。 在实现上，channel其实就是个消息队列： 1234567891011121314// 省略部分字段type hchan struct &#123; qcount uint // total data in the queue dataqsiz uint // size of the circular queue buf unsafe.Pointer // points to an array of dataqsiz elements elemsize uint16 closed uint32 elemtype *_type // element type sendx uint // send index recvx uint // receive index recvq waitq // list of recv waiters sendq waitq // list of send waiters lock mutex&#125; 下图描述了一个缓冲区大小为5，并阻塞了若干读goroutine的情况: 该图省略了hchan和sudog的部分字段，waitq在实现上是双向链表，虽然实际只会用到单链表语义(FIFO)。 根据上图情形，此时如果有其它goroutine写入channel: 从recvq中pop第一个读写者的sudog 将写入channel的数据拷贝到该sudog的elem字段 唤醒该读写者goroutine(sudog.g) 当recvq队列为空，此时写入: 将写入的数据缓存到buff[sendx] sendx环形自增，qcount++ 当buff缓冲区写满(qcount==dataqsiz)，此时写入: 为写入者创建一个sudog，并插入到sendq队列末 挂起该写入者goroutine 如果此时有goroutine再次读channel: 从buf[recvx]读取第一个数据 从sendq中pop第一个阻塞的写入者goroutine(sudog) 将该sudog中的elem字段数据拷贝到buf[recvx]，相当于将elem数据push到buf末尾 recvx++ 唤醒该发送者goroutine 没有缓冲的channel(dataqsize==0)操作要简单一些，写入时如果recvq-&gt;first!=nil，则直接拷贝数据到读取者的elem字段，否则将写入者挂起。反之，读写过程也类似。 另外，由于一个goroutine读写多个channel，因此go提供语言级别的select，用于处理异步IO问题。这其实本质上仍然是尝试对channel进行读写操作(chanrecv)，只不过由block参数为false表明该读写不阻塞，当读写操作需要挂起时，立即返回false。而select操作本身其实就是个多分支的if-elseif-else表达式: 123456789101112131415161718192021src/runtime/chan.go// compiler implements//// select &#123;// case c &lt;- v:// ... foo// default:// ... bar// &#125;//// as//// if selectnbsend(c, v) &#123;// ... foo// &#125; else &#123;// ... bar// &#125;//func selectnbsend(t *chantype, c *hchan, elem unsafe.Pointer) (selected bool) &#123; return chansend(t, c, elem, false, getcallerpc(unsafe.Pointer(&amp;t)))&#125; select的if-elseif-else语句分支顺序是随机的，在每次执行select时会将所有scase(包含hchan)顺序随机排列。参考src/runtime/select.go hselect和scase结构体。 通过cap(chan)和len(chan)可以获取channel的缓冲区大小(dataqsize)和当前消息数量(qcount)。 6. interfaceinterface接口的用法和实现放到go面向对象和go interface实现中。 7. make &amp; newgo中有make和new两个关键字用于分配一个对象，简要提一下两者的区别： 内建函数 new 用来分配内存，它的第一个参数是一个类型，不是一个值，它的返回值是一个指向新分配类型零值的指针 内建函数 make 用来为 slice，map 或 chan 类型分配内存和初始化一个对象(目前只能用于这三种类型)\u0010，跟 new 类似，第一个参数也是一个类型而不是一个值，跟 new 不同的是，make 返回类型的引用而不是指针，而返回值也依赖于具体传入的类型，具体使用如下： // 等价于 a := [capacity]int{} s := a[0:2] s := make([]int, length [,capacity]) m := make(map[int]string [,size]) c := make(chan int, [,length]) 8. 常量Go中的常量是无类型的，字面常量(如：3.14, “ok”)是无类型的，可以赋给任何满在其值域中的类型。Go预定义了三个常量：true, false, iota，其中iota是一个可以被编译器修改的常量，它代表一个整数，在每个const出现时被重置为0，然后iota每出现一次，其所代表的值即自增一次。iota通常用来定义枚举值，这类值应用程序不关心具体数值，只需确保其在同一个const枚举声明中不会冲突即可。 const ( c0 = iota // c0 == 0 c1 = iota // c1 == 1 c2 = iota // c2 == 2 ) // 根据枚举定义相同表达式的缩写，等价于 const ( c0 = iota // c0 == 0 c1 // c1 == 1 c2 // c2 == 2 )","tags":[{"name":"go","slug":"go","permalink":"http://wudaijun.com/tags/go/"}]},{"title":"Go 笔记(2) 顺序编程","date":"2016-09-08T16:00:00.000Z","path":"2016/09/go-notes-2-procedural-programming/","text":"不定参数&amp;多返回值不定参数只能是最后一个参数，它实际上是数组切片参数的语法糖： // 语法糖 相当于 func myfunc(args []interface{}) func myfunc(args ...interface{}){ for _, arg := range args { fmt.Println(arg) } // 参数会被打包为 []{arg1,arg2,arg3} myfunc(arg1,arg2,arg3) // 要完成可变参数的完美传递 需要用...将Slice打散 func myfunc2(args ...interface{}) // 此时args已经是Slice 如果不打散将作为一个参数 不能完美传递 myfunc(args) // 编译器在此处有优化 最终会直接将args传入 不会打散再打包 参考: http://www.jianshu.com/p/94710d8ab691 myfunc(args...) end 多返回值为函数提供了更大的便利性，无需传引用或者专门构造返回值结构体，并且在错误处理方面也更简便，在前面的示例代码中已经初尝甜头。 // 定义多返回值函数时，可以为返回值指定名字 func (file *File) Read(b []byte) (n int, err Error){ // n和err在函数开始时，被自动初始化为空 ... ... n = xxx ... ... err = xxx ... // 直接执行return时，将返回n和err变量的值 return } 多返回值的在Plan9 C编译器上的实现是由调用者在其栈上分配n和err的内存，由被调用方修改调用方栈上的n和err的值： 匿名函数&amp;闭包匿名函数允许函数像变量一样被定义，传递，和使用。Go语言支持随时在代码里定义匿名函数。 // 赋给变量 F = func (a, b int) int { return a + b } F(1,2) // 直接执行 func (a, b int) int { return a + b }(1,2) 1. 闭包的概念闭包是可以包含自由(未绑定到特定对象)变量的代码块，这些变量不在这个代码块内或者任何全局上下文中定义，而是在定义代码块的环境中定义。要执行的代码块(由于自由变量包含在代码块中，所以这些自由变量以及它们所引用的对象没有被释放)为自由变量提供绑定的计算环境(作用域)。 2. 闭包的价值闭包的价值在于可以作为函数对象或者匿名函数，对于类型系统而言，这意味着不仅要表示数据还要表示代码。支持闭包的多数语言都将函数作为第一类对象，就是说这些函数可以存储到变量中作为参数传递给其它函数，最重要的是能够被函数动态创建和返回。 3. Go语言中的闭包Go语言中的闭包同样也会引用到函数外的变量，闭包的实现确保只要闭包还被使用，那么闭包引用的变量会一直存在。 123456789101112131415161718192021222324252627282930package mainimport \"fmt\" func main() &#123; var j int = 5 return_closure := func()(func()) &#123; var i int = 10 return func() &#123; i = i + 1 j = j + 1 fmt.Printf(\"i, j: %d, %d\\n\", i, j) &#125; &#125; // 同一个闭包c1 共享所有外部环境 i, j c1 := return_closure() c1() c1() j = j + 1 // c1 c2 只共享return_closure作用域之外的变量 j // return_closure之内定义的变量i将在每次调用时重新生成，因此只对同一个closure有效 c2 := return_closure() c2()&#125; // 输出：i, j: 11, 6i, j: 12, 7i, j: 11, 9 为了实现闭包: Go必须有能力识别闭包函数的引用变量(这里的j)，并将它们分配在堆上而不是栈上(escape analyze技术) 用一个闭包结构体保存函数和其引用环境 下面分别阐述这两点： escape analyze123456package testfunc F() *int &#123; var i int i = 5 return &amp;i&#125; 在C语言中，在函数中返回该函数栈上的地址是不被允许的，因为当函数调用完成后函数栈会被回收。Go当然也有函数栈和栈回收的概念，因此它将i分配在堆上而不是栈上，通过go tool compile -S x.go查看汇编代码: ... 0x001d 00029 (tmp.go:3) LEAQ type.int(SB), AX 0x0024 00036 (tmp.go:3) MOVQ AX, (SP) 0x0028 00040 (tmp.go:3) PCDATA $0, $0 0x0028 00040 (tmp.go:3) CALL runtime.newobject(SB) // 相当于new(int) 0x002d 00045 (tmp.go:3) MOVQ 8(SP), AX // 将i的地址放入AX 0x0032 00050 (tmp.go:4) MOVQ $5, (AX) // 将AX存放的内存地址值设为5 ... 也可通过-gcflags=-m选项编译来查看: ▶ go build --gcflags=-m x.go ./tmp.go:2: can inline F ./tmp.go:5: &amp;i escapes to heap ./tmp.go:3: moved to heap: i Go编译器依靠escape analyze来识别局部变量的作用范围，来决定变量分配在堆上还是栈上，这与GC技术是相辅相成的。 闭包结构体闭包结构体在src/cmd/compile/internal/gc/closure.go的walkclosure函数生成，具体实现太过复杂，其注释简要地说明了实现方式： // Create closure in the form of a composite literal. // supposing the closure captures an int i and a string s // and has one float64 argument and no results, // the generated code looks like: // // clos = &amp;struct{.F uintptr; i *int; s *string}{func.1, &amp;i, &amp;s} // // The use of the struct provides type information to the garbage // collector so that it can walk the closure. We could use (in this case) // [3]unsafe.Pointer instead, but that would leave the gc in the dark. // The information appears in the binary in the form of type descriptors; // the struct is unnamed so that closures in multiple packages with the // same struct type can share the descriptor. 比如对我们闭包例子中return_closure生成的闭包，其闭包结构体表示为: type.struct{ .F uintptr//闭包调用的函数指针 j *int// 指向闭包的上下文数据，c1,c2指向不同的堆地址 } 3. 错误处理Go的错误处理主要依靠 panic，recover，defer，前两者相当于throw和catch，而defer则是Go又一个让人惊喜的特性，defer确保语句在函数结束(包括异常中断)前执行，更准备地说，defer语句的执行时机是在返回值赋值之后，函数返回之前: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152func f1() (r int) &#123; defer func() &#123; r++ &#125;() return 0&#125;/* 函数返回: 1f1等价于:func f1() (r int)&#123; r = 0 // 返回值赋值 func() &#123; // 执行defer函数 r++ &#125;() return // 函数返回&#125;*/func f2() (r int) &#123; t := 5 defer func() &#123; t = t + 5 &#125;() return t&#125;/*函数返回: 5f2等价于:func f2() (r int) &#123; t := 5 r = t func() &#123; t = t + 5 &#125;() return&#125;*/func f3() (r int) &#123; defer func(r int) &#123; r = r + 5 &#125;(r) return 1&#125;/*函数返回: 5f3等价于:func f3()(r int)&#123; r = 1 func(r int) &#123; r = r + 5 &#125;(r) // 值传参 不会影响返回的r的值 return&#125;*/ 因此，return x其实不是”原子操作”，在其中会插入defer函数执行，在Go官方文档中也提到了这点。 defer还有如下特性： 一个函数可定义多个defer语句 多个defer语句按照先入后出的顺序执行 defer表达式中的变量值在defer表达式定义时就已经明确 defer表达式可以修改函数中的命名返回值 defer的作用： 简化异常处理(在defetr中recover)，避免异常与控制流程混合(try … catch … finally) 在defer中做环境清理和资源释放 更多阅读: 多返回值和闭包: https://www.teakki.com/p/57df64ccda84a0c45338154e","tags":[{"name":"go","slug":"go","permalink":"http://wudaijun.com/tags/go/"}]},{"title":"Linux 作业管理","date":"2016-08-29T16:00:00.000Z","path":"2016/08/linux-job-control/","text":"进程组/会话简要概念： 进程组：N(N&gt;=1)个进程的集合，通常在同一作业中关联起来(通过管道)。进程组的ID(PGID)即为进程组组长的PID。进程必定且只能属于一个进程组，只有进程组中一个进程存在，进程组就存在，与组长进程终止与否无关。进程组的概念提出主要是为了进程管理与信号分发 会话：N(N&gt;=1)个进程组的集合，创建会话的进程叫会话首进程。会话ID即为会话首进程PID 控制终端：如果会话有控制终端，建立与控制终端连接的会话首进程叫控制进程(通常就是Shell进程)，当前与终端交互的进程组为前台进程组，其余进程组成为后台进程组 无论合适输入终端的退出键，都会将退出信号发送到前台进程组的所有进程 如果控制终端断开连接，则将挂掉信号(SIGHUP)发送至控制进程(会话首进程)，SIGHUP信号默认将导致控制进程终止 例如，打开Bash，输入： proc1 | proc2 &amp; proc3 | proc4 | proc5 进程关系如下图所示： 作业控制信号 SIGCHLD: 子进程终止 SIGTTIN: 后台进程组成员读控制终端 SIGTTOU: 后台进程组写控制终端 SIGCONT: 如果进程已停止，则使其继续运行(fg &amp; bg) SIGSTOP: 进程停止信号，不能被捕获或忽略 SIGTSTP: 交互式停止信号(Ctrl+Z) SIGINT: 中断信号(Ctrl+C) SIGQUIT: 退出信号(Ctrl+) SIGCHILD信号在子进程终止或停止时向父进程发送，系统默认将忽略该信号，如果父进程希望知晓子进程状态变更，应捕获该信号。 对于SIGTTIN和SIGTTOU信号，在后台作业尝试读取控制终端时，终端驱动程序知道它是个后台作业，于是将向改进程发送SIGTTIN信号，该信号默认将导致进程被挂起(停止)： ▶ /usr/local/opt/coreutils/libexec/gnubin/cat &gt; file &amp; [1] 44978 [1] + 44978 suspended (tty input) /usr/local/opt/coreutils/libexec/gnubin/cat &gt; file ▶ fg [1] + 44978 continued /usr/local/opt/coreutils/libexec/gnubin/cat &gt; file Hello World! // 重新获得终端 读取输入 [Ctrl+D] // 键入EOF 当后台作业尝试写终端时，默认情况下，后台作业的输出将成功输出到控制终端，但我们可以通过stty命令禁止后台作业向控制终端写，此时终端驱动程序向进程发送SIGTTOU信号： ▶ /usr/local/opt/coreutils/libexec/gnubin/cat file &amp; [1] 46166 Hello World! [1] + 46166 done /usr/local/opt/coreutils/libexec/gnubin/cat file ▶ stty tostop // 禁止后台作业向控制终端写 ▶ /usr/local/opt/coreutils/libexec/gnubin/cat file &amp; [1] 46290 [1] + 46290 suspended (tty output) /usr/local/opt/coreutils/libexec/gnubin/cat file ▶ fg [1] + 46290 continued /usr/local/opt/coreutils/libexec/gnubin/cat file Hello World! 注意，在MacOS X上，自带的cat程序有BUG，不是interrupt-safe的，在MacOS X上，尝试恢复cat程序的执行将得到cat: stdin: Interrupted system call错误，这篇文章和APUE 9.8节均提到了这个问题，因此我使用的是brew安装的GNU版本cat命令，安装方案参见这里。 关于SIGTSTP和SIGSTOP的区别，前者通常由键盘产生，可被捕获，当通过Ctrl+Z将前台作业放入后台时，前台作业收到该信号，意思是”从哪儿来到哪儿去”。而SIGSTOP通常由kill产生，不可被捕获或忽略，意思是”在那里待着别动”。两者均可由SIGCONT信号恢复运行。 对于键盘输入产生的信号，控制进程将信号发送至前台进程组的所有进程。 作业控制信号间有某些交互，当对一个进程产生四种停止信号(SIGTSTP,SIGSTOP,SIGTTIN,SIGTTOU)中的一种时，对该进程的任意未决SIGCONT信号将被丢弃，同样，当产生SIGCONT信号时，对同一进程的任意停止信号将被丢弃。 作业管理 &amp; 将作业放入后台执行，如果没有进行重定向，数据流仍然会输出到前台 Ctrl+C 强制中断前台当前作业执行 Ctrl+Z 将作业挂到后台 jobs -l 查看所有作业，作业ID和其PID fg %作业ID 将后台作业拿到前台来处理 bg %作业ID 将后台作业由挂起变为执行 kill -signal %作业ID 向指定作业的所有进程发送信号 作业管理的后台不是系统后台，因此，上述的任务管理依旧与终端有关，当远程连接的终端断开连接时，SIGHUP信号默认将导致改会话上所有的任务都会被中断。 脱机管理 nohup: nohup CMD &amp; 将任务放在后台执行，并忽略SIGHUP挂掉信号，但是在人机交互上比较麻烦 screen: 一个可以在多个进程之间多路复用一个物理终端的窗口管理器，在远端服务器上运行screen，开启一个新会话并执行任务，在终端断开后，任务继续执行，下次登录再attach上screen会话即可，Linux发行版自带 tmux: 功能类似于screen，但在分屏切换，配置方面更强大，完全可作为本地终端使用","tags":[{"name":"os","slug":"os","permalink":"http://wudaijun.com/tags/os/"},{"name":"linux","slug":"linux","permalink":"http://wudaijun.com/tags/linux/"}]},{"title":"一些GS设计理念","date":"2016-08-14T16:00:00.000Z","path":"2016/08/gs-design-concept/","text":"关于GS设计的一些体会，纯属个人理解。 一. 系统结构解耦是在做系统设计时，最应该铭记于心的原则，解耦的目的是让系统组件可以独立变化，构建易于理解，测试，维护的系统。 解耦的手段通常有如下几种： 1. 依赖倒置依赖倒置的原则：上层模块不应该依赖于下层模块，它们共同依赖于一个抽象。抽象不能依赖于具象，具象依赖于抽象。 依赖倒置原则的目的是把高层次组件从对低层次组件的依赖中解耦出来，这样使得重用不同层级的组件实现变得可能。如模块A依赖于模块B，那么在A和B之间加一层接口(interface)，A调用(依赖)该接口，B实现(依赖)该接口，这样，只要接口稳定，A，B即可独立变化。 这种依赖抽象的思想，在GOF的设计模式中，有大量宝贵实践，如策略模式，模板方法模式等。 2. 控制反转依赖倒置描述的是组件之间的依赖关系被倒置，而控制反转更强调的是控制流程，体现了控制流程的依赖倒置。典型的实现方式： 依赖注入反转依赖对象的获取，由框架注入组件所依赖的对象(被动接收对象)。 依赖查找反转依赖对象的获取，由组件通过框架提供的方法获取所需依赖对象(主动查找对象)。微服务系统中的服务发现(比如我们的cluster_server)，就是一种依赖查找机制。 事件发布/订阅反转对事件的处理，发布方不再关心有哪些接收方依赖了某个事件，由接收方主动订阅事件并注册处理函数。在GS设计中，经常会用到，如任务系统，通知中心等。 向依赖和耦合宣战，就是和混乱和失控划清界限，解耦也有助于更好地复用代码，在我看来，重复和耦合一样危险。在发现已有系统不能很好地兼容变化时，就应该理清组件依赖，将变化封装起来。这里有一篇关于依赖导致和控制反转不错的文章，在GOF设计模式中有更全面精辟的实践经验。 二. 系统拆分在系统结构中，更多地去梳理系统内部的结构和对象行为的关系，而系统拆分则尝试从架构设计的角度将系统拆分为多个小系统(服务)，这些服务独立运行(Routine/Actor/进程等)，服务之间遵循某种通信规范(Message/RPC/TCP/Channel等)。不同粒度的服务的优劣各不相同，一方面我们希望服务彼此独立并且无状态，另一方面我们也希望有服务间的通信足够高效(通过缓存，消息，或远程调用)。需要注意的是，这里所说的服务，并不只是微服务，像Erlang中的Actor，Go中的goroutine，都可以叫服务。以下只讨论最基本的服务设计。 1. 服务的数据管理以Erlang为例，GS中存在多种实体(Actor)，玩家，公会，地图等，实体之间的交互产生了一些关联数据，我们需要明确这类数据的归属权和数据的同步方式，制定清晰的数据边界。数据只能由其所属Actor进行更新和同步，并且是数据的唯一正确参照。关于数据同步，此前我们一直严格遵循”通过通讯来共享”，在带来很好的隔离性的同时，也带来更高的复杂度，大量的Actor数据同步通信，非必要的实时性同步，多份数据副本等等。之后开始使用Ets做Cache，数据冗余和逻辑复杂度都小了很多。使用Cache时，需要严格遵守单写入者，即数据的Cache只能由数据所属Actor进行更新。 2. 服务发现前面也提到，服务发现实则是对服务之间的依赖关系的倒置，服务发现是系统具备良好扩展性和容灾性的基础。目前已经有一些成熟的服务发现和配置共享工具，如etcd，zookeeper等。 3. 无状态服务服务应该尽量被设计为无状态的，这样对容错和透明扩展都有巨大好处，在这篇博客中我曾提到到无状态服务的实践。 三. 过度设计在设计系统时，有时候我们会为了设计而设计，过度抽象和封装，这种过度设计会导致： 浪费不必要的开发时间和精力在很简单的逻辑上 产生很多不必要的约定和限制，随着项目需求的变更和增长，会成为系统的负担，很可能也并不能满足新需求 如何辨别过度设计，我的理解是，首先这个系统是否需要重构，如果系统足够简单，或者足够稳定，那就let it alone。将精力花在核心系统上，并且在必要的时候(已有架构不能满足当前需求(不是YY的需求)或者已经带来大量的复杂度)再进行重构，特别是对于游戏服务器来说，需求迭代很快，提供可靠的服务才是宗旨，不要陷入设计的漩涡。 四. 防御式编程防御是为了隔离错误，而不是为了容忍错误。在实际运用中，API职责不单一，过度防御，都可能将错误隐藏或扩散出去，对系统调试带来麻烦。应该遵循职责单一，语义明确的API设计理念，对Erlang OTP这种高容错的系统，提倡让错误尽早暴露而不是容忍，对于一些严重错误，甚至应该Crash。错误的尽早暴露有利于Debug，找到问题的源头。 五. 注重测试测试分为黑盒和白盒，对后端来说，黑盒相当于模拟客户端，发出请求，并确保得到正确响应。白盒为服务器内部的函数测试，模块测试，数据检查等。 就实现上来说，游戏服务器主要的测试的方式有： 1. 测试用例以逻辑功能为测试单元，模拟客户端请求流程，尽可能多地覆盖正常分支和异常分支。优点是覆盖完善，使用简单，可以检查并暴露出绝大部分问题。缺点是维护麻烦，对上下文环境(配置，流程，协议等)进行了过多地依赖，适用于需求稳定，流程简单的功能模块。 2. 测试状态机状态机是一个独立的Actor，也叫做Robot，通常基于有限状态机，对所有事件(外部命令，服务器消息，内部事件)作出响应。在Erlang中可以用gen_fsm来实现，一般被设计为可扩展的事件处理中心，Robot的优点有很多，灵活，强大，可以对服务器进行压测，针对性测试，以及长期测试。将一些常用的测试模式做成一个Mode集成到状态机中，如大地图测试，登录流程测试等，再结合bash脚本和后台定时任务，一个服务器测试框架的雏形就有了。对于一些来不及写测试用例的功能模块，通过Robot也可以进行快速测试，这也是我们目前主要使用的测试手段。在这方面可以进一步探索的还有很多，比如将测试用例集成到测试状态机中，外部只定义期望的消息交互流程(如发送req1, 期望收到ack1, ack2,发送req2, 期望收到…)，再导入到状态机中进行执行，并判断整个流程是否符合预期。 3. 内部测试前两者更像是黑盒测试，而内部测试更像白盒，针对API，模块进行测试，除此之外，内部测试还包括一些服务器自身的数据逻辑检查，这类检查关注服务器本身的数据和服务的正确性，尽早地暴露问题，及时进行数据修复和调试。比如我们的大地图就有一些数据一致性检查，比如实体状态，实体交互，资源刷新等等，这类检查在开发期间可以直接作为routine让进程定时跑，配合机器人测试，能查出大量问题。 测试的重要性怎么强调也不为过，对服务器开发来说，测试的优点有： 节省大量和客户端以及QA的调试和交互时间 确保重构/改动的正确性 进一步理解交互流程 预先暴露问题，并获得更加详尽的错误信息 多种测试并行，加速测试流程","tags":[{"name":"gameserver","slug":"gameserver","permalink":"http://wudaijun.com/tags/gameserver/"}]},{"title":"web 初学笔记","date":"2016-08-01T16:00:00.000Z","path":"2016/08/web-get-started/","text":"一些简单的web学习笔记，用于在需要时快速搭建一个可用的网站。 基础知识 HTML: 通过一套标记标签，定义网页的内容 CSS: 通过选择器和层叠次序，定义网页的布局 JavaScript: 通过可编程的文档对象模型(DOM)和事件响应，定义网页的行为 后端框架由于Python的原因，选用了web.py这个非常轻量的框架，之前也看过Rails，用起来觉得很”神奇”，但约定和黑魔法太多，不合个人口味。 web.py是一个web framework，它也提供了http server的功能，但在线上环境，通常需要结合更高效专业的http server(如nginx)。这里有几种结合方案: 用nginx做反向代理，将请求路由到后端web.py 将web.py作为CGI/FastCGI程序，挂接到nginx/lighttpd/apache 第二种方式需要安装python flup库，它实现了CGI/FastCGI规范，并实现了这些规范的WSGI(定义flup这类服务与web.py这类framework的调用规范)接口。 CGI通用网关接口(Common Gateway Interface)，是外部应用程序（CGI程序）与Web服务器之间的接口标准。CGI规范允许Web服务器执行外部程序，并将它们的输出发送给Web浏览器，CGI将Web的一组简单的静态超媒体文档变成一个完整的新的交互式媒体。 我们知道http server提供的内容通常分为静态内容和动态内容，前者通常集成于web server中。而动态内容，需要web server(如nginx)将请求传递给处理程序(如web.py)并获取返回结果。那么web server传递哪些请求内容，如何传递，处理程序如何返回生成的响应内容等细节，就需要一个通信规范，并且这个规范最好抽象于双方的具体实现，这就是CGI存在的意义，CGI程序可以用任何脚本或编程语言实现，只要这种语言具有标准输入输出和环境变量。 CGI规定每次有请求，都会启动一个CGI程序进程(对Shell script来说，就是sh或bash命令，对python等脚本语言来说，通常是对应解释器)，并且通过标准输入输出以及环境变量与CGI程序交互。CGI的缺点是反复进程启动/初始化/关闭比较消耗资源并且效率低下，难以扩展。目前CGI已经逐渐退出历史舞台。 web内置模块针对CGI每次初始化进程(脚本解释器)的开销问题，一些web server(如apache)以插件的方式集成了CGI脚本的解释器(如mod_php,mod_perl等)，将这些解释器以模块的方式常驻，web服务器在启动时会启动这些模块，当新的动态请求到达，web服务器可利用解释器模块解析CGI脚本，避免进程fork。这种优化方式主要针对于脚本语言编写的CGI程序。 FastCGIFastCGI在CGI进程常驻的前提下，通过进程池，进程管理器进一步提高了CGI的可伸缩性和容错性。web server将动态请求发给FastCGI进程管理器，后者会将请求分发给进程池中的某一个worker进程。 web server和FastCGI管理进程的通信方式有socks(相同主机)和tcp(不同主机)两种，这提高了FastCGI本身的可扩展性。目前FastCGI进程管理器除了web server自带的fastcgi模块之外，还有spawn-fcgi(分离于lighttpd)，php-fpm(仅用于PHP)等可替换的独立模块。 参考： 什么是CGI、FASTCGI、PHP-CGI、PHP-FPM、SPAWN-FCGI? WSGI、flup、fastcgi、web.py的关系 nginx[+spawn-fcgi]+flup+webpy服务搭建 http://webpy.org/install.zh-cn http://webpy.org/cookbook/index.zh-cn 模板引擎模板引擎用于将用户界面和业务数据分离，使用模板语言，来根据业务数据动态生成HTML网页，简化HTML的书写。简单了解了一下Python的模板引擎，Jinja2似乎是个不错的选择，速度块，语法易懂，文档全面。控制结构，模板继承都很好用。 CSS框架前端框架定义一系列可复用，可扩展的CSS样式，常用组件，和JS插件等。让用户在排版样式上少操点心，直接拿来用就行了。目前觉得Bootstrap还不错，社区庞大，稳定，有多套可视化的布局系统。 JS框架JQuery应该是目前最火的前端JS框架了，基于CSS选择器扩展的JQuery选择器，简化了JavaScript的书写，实现脚本与内容分离。 其它类库除此之外，可能还需要用到一些第三方的类库，如Python的MongoDB库pymongo，Json的解析和渲染库pretty-json等。在开发过程中要善于搜索，提高开发效率。 综合使用写了一个简单Demo, 很Low, 没用JS, 只是用来熟悉基本流程: https://github.com/wudaijun/pyweb","tags":[{"name":"python","slug":"python","permalink":"http://wudaijun.com/tags/python/"},{"name":"web","slug":"web","permalink":"http://wudaijun.com/tags/web/"}]},{"title":"MongoDB 状态监控","date":"2016-07-04T16:00:00.000Z","path":"2016/07/monitor-mongodb/","text":"一. 关键指标 慢查询：当MongoDB处理能力不足时，找出系统中的慢查询，分析原因，看能否通过建立索引或重新设计schema改进 内存使用：MongoDB吃内存(特别是MMAPv1)，至少要给MongoDB足够的内存存放索引，最理想的情况是能够存放所有数据。当内存占用过高，或者page faults过高时，考虑能不能给MongoDB预留更多的内存。 磁盘占用：特别是对于MMAPv1，涉及到磁盘占用的因素有很多，不合理的schema(文档频繁移动)或集合/文档的删除都可能会导致磁盘空间利用不足。前期需要设计好schema，后期维护也需要定期整理磁盘数据。 连接数：MongoDB为每个连接分配一个线程，因此连接是占资源的，并且也不是越多连接越好。合理地控制连接数。 索引不命中：查看所有查询的索引不命中情况，尽量让所有查询都通过索引 锁等待：锁等待的原因有很多，连接数过多，操作频繁，慢操作，schema设计过于反范式化等，可从上面的原因针对性解决。 二. 监控工具1. mongostatmongodb自带的状态检测工具，按照固定时间间隔(默认1s)获取mongodb的当前运行状态，适用于对临时异常状态的监控： // from MongoDB 3.0 MMAPv1 ▶ mongostat insert query update delete getmore command flushes mapped vsize res faults qr|qw ar|aw netIn netOut conn time *0 40 1 *0 0 1|0 0 4.3G 11.1G 150.0M 0 0|0 0|0 2k 12k 201 19:07:04 *0 20 *0 *0 0 1|0 0 4.3G 11.1G 150.0M 0 0|0 0|0 1k 11k 201 19:07:05 *0 *0 1 *0 0 1|0 0 4.3G 11.1G 150.0M 0 0|0 0|0 244b 10k 201 19:07:06 *0 20 *0 *0 0 1|0 0 4.3G 11.1G 150.0M 0 0|0 0|0 1k 11k 201 19:07:07 *0 20 *0 *0 0 2|0 0 4.3G 11.1G 150.0M 0 0|0 0|0 1k 11k 201 19:07:08 具体各列的意义都很简单，见官方文档即可。比较重要的字段有： res: 常驻内存大小 mapped: 通过mmap映射数据所占用虚拟内存大小(只对MMAPv1有效) vsize: mongodb进程占用的虚拟内存大小 faults: page fault次数，如果持续过高，则可以考虑加内存 qr/qw: 读取/写入等待队列的大小，如果队列很大，表示MongoDB处理能力跟不上，可以看看是否存在慢操作，或者减缓请求 conn: 当前连接数，conn也会占用MongoDB资源，合理控制连接数 idx miss: 索引不命中所占百分比 如果太高则要考虑索引是否设计得不合理 flushes: 通常为0或1，对于MMAPv1，表示后台刷盘次数(默认60s)，对于WiredTiger，表示执行checkpoint次数(默认60s或2GB journal日志) lr/lw: 读取/写入操作等待锁的比例 (New In MongoDB 3.2, Only for MMapv1) lrt/lwt: 读取/写入锁的平均获取时间(微妙) 更多参考。 2. db.serverStatus()返回数据库服务器信息，该命令返回的数据量很大，但执行很快，不会对数据库性能造成影响，其中比较重要的字段有： db.serverStatus().mem: 当前数据库内存使用情况 db.serverStatus().connections: 当前数据库服务器的连接情况 db.serverStatus().extra_info: 在Linux下，包含page fault次数 db.serverStatus().locks: 数据库各种类型锁竞态情况 db.serverStatus().backgroundFlushing: 数据库后台刷盘情况(默认60s)一次，仅针对MMAPv1存储引擎 更多参考。 3. Profiler主要用于分析查询性能，默认是关闭的，Profiler获取关于查询/写入/命令等操作的详细执行数据，并将这些分析数据写入system.profile集合。Profiler有三个Level： Level 0: 意味着关闭Profiler，并不收集任何数据，也是mongod的默认配置。注意mongod总是会将”慢操作”(执行时间超过slowOpThresholdMs，默认100ms)的操作写入mongod日志(不是system.profile集合) Level 1: 只收集所有慢操作的信息，慢操作执行时间可通过修改slowOpThresholdMs参数指定 Level 2: 收集所有的数据库操作执行信息 需要注意，一个操作执行慢，可能是索引不合理，也可能是page fault从磁盘读数据等原因导致。需要进一步分析。 使用示例： &gt; db.setProfilingLevel(2) { &quot;was&quot; : 0, &quot;slowms&quot; : 100, &quot;ok&quot; : 1 } &gt; &gt; db.getProfilingStatus() { &quot;was&quot; : 2, &quot;slowms&quot; : 100 } &gt; db.user.insert({&quot;name&quot;:&quot;wdj&quot;}) WriteResult({ &quot;nInserted&quot; : 1 }) &gt; db.system.profile.find() { &quot;op&quot; : &quot;insert&quot;, &quot;ns&quot; : &quot;test.user&quot;, &quot;query&quot; : { &quot;insert&quot; : &quot;user&quot;, &quot;documents&quot; : [ { &quot;_id&quot; : ObjectId(&quot;577e62991fa7b960bb8bf0af&quot;), &quot;name&quot; : &quot;wdj&quot; } ], &quot;ordered&quot; : true }, &quot;ninserted&quot; : 1, &quot;keyUpdates&quot; : 0, &quot;writeConflicts&quot; : 0, &quot;numYield&quot; : 0, &quot;locks&quot; : { &quot;Global&quot; : { &quot;acquireCount&quot; : { &quot;r&quot; : NumberLong(2), &quot;w&quot; : NumberLong(2) } }, &quot;Database&quot; : { &quot;acquireCount&quot; : { &quot;w&quot; : NumberLong(1), &quot;W&quot; : NumberLong(1) } }, &quot;Collection&quot; : { &quot;acquireCount&quot; : { &quot;w&quot; : NumberLong(1), &quot;W&quot; : NumberLong(1) } } }, &quot;responseLength&quot; : 25, &quot;protocol&quot; : &quot;op_command&quot;, &quot;millis&quot; : 32, &quot;execStats&quot; : { }, &quot;ts&quot; : ISODate(&quot;2016-07-07T14:09:29.690Z&quot;), &quot;client&quot; : &quot;127.0.0.1&quot;, &quot;allUsers&quot; : [ ], &quot;user&quot; : &quot;&quot; } &gt; system.profile集合中，关键字段：op(操作类型), ns(操作集合), ts(操作时间)，millis(执行时间ms),query(操作详情)。 更多参考。 4. db.currentOp()当MongoDB比较繁忙或者在执行比较慢的命令时，可能会阻塞之后的操作(视数据库和操作的并发级别而定)。可通过db.currentOp()来获取当前正在进行的操作，并可通过db.killOp()来干掉它。 更多参考。 5. db.stats()返回对应数据库的信息，包括集合数量，文档总大小，文档平均大小，索引数量，索引大小等静态信息： &gt; db.stats() { &quot;db&quot; : &quot;test&quot;, &quot;collections&quot; : 2, &quot;objects&quot; : 3, &quot;avgObjSize&quot; : 430, &quot;dataSize&quot; : 1290, &quot;storageSize&quot; : 49152, &quot;numExtents&quot; : 0, &quot;indexes&quot; : 1, &quot;indexSize&quot; : 16384, &quot;ok&quot; : 1 } &gt; 更多参考。 6. db.collStats()返回集合详细信息. 更多参考。","tags":[{"name":"mongodb","slug":"mongodb","permalink":"http://wudaijun.com/tags/mongodb/"}]},{"title":"虚拟存储器","date":"2016-06-29T16:00:00.000Z","path":"2016/06/virtual-memory/","text":"前几天又翻了翻《深入理解计算机系统》，相比几年前刚看这本书有了更多的理解，这也许就是一本好书的价值吧。因此尝试提炼和总结出对自己有用的东西，把书读薄。其中也加了一些自己的理解和查阅的相关资料。 虚拟存储器的意义虚拟存储器提供了三个重要的能力： 将主存看作是存储在磁盘上的高速缓存，并根据需要在磁盘和主存中传送数据，高效地运用主存 为每个进程提供一致性的地址空间，简化了存储器管理 保护每个进程的地址空间不被其它进程破坏 下面分别从这三个方面来谈虚拟存储器 1. 内存是磁盘的Cache虚拟存储器和物理存储器一样，都分页管理，并且页大小是一致的。在任意时刻，虚拟页面(VP)的集合都分为三个不相交的子集： 未分配：VM系统还未分配或创建的页，没有任何数据和它们相关联，因此不占用任何磁盘空间 已缓存：当前缓存在物理存储器中的已分配页 未缓存：没有缓存在物理存储器中的已分配页 同任何缓存一样，虚拟存储系统必须有某种方法来判定一个虚拟页是否被缓存在主存中，如果是，系统还必须确认这个虚拟页放在哪个物理页中，如果不命中，系统需要判断这个虚拟页存放在磁盘的哪个位置，并且该磁盘对应数据加载到主存中，并且在必要时，选择一个牺牲页(页面调度)。VM通过页表(page table)来维护虚拟页的状态和当前实际地址(主存地址/磁盘地址)： 每个进程都有一份页表，页表本身是进程数据的一部分，操作系统负责维护页表的内容，并在磁盘和主存之间来回传送页。CPU通过虚拟地址访问主存时，MMU会先查看页表中该虚拟地址的页表条目： 已缓存：如果页表条目标志位为已缓存，MMU则取出对应的物理页面地址，返回给CPU 未缓存：如果页表条目标志位为未缓存，则DRAM缓存不命中，又称为缺页(page fault)，此时会触发一个缺页异常，缺页异常会调用缺页异常处理程序，该程序选择一个牺牲页VP2(如果VP2已经被修改了，则将其写回磁盘)，更新VP2的页表条目为未缓存，之后内核从磁盘拷贝目标页VP1到物理页，更新VP1的页表条目，最后返回。此时，页面置换已经完成，当异常处理程序返回时，会重新发起导致缺页的指令，而此时VP1对应的页表条目状态为已缓存，即按照页命中流程正常处理 未分配：导致非法地址访问，段错误(segment fault) 巨大的不命中开销(磁盘读写效率比主存低10000倍)驱动着整个DRAM缓存设计的方方面面，由于磁盘读第一个字节的效率是读连续字节的效率的100000倍，因此页不能太小，通常是在4KB~2MB之间，并且DRAM Cache是全相连的，也就是说任何虚拟页都可以放置在任何的物理页中，而对于不命中的替换算法也更为精密，最后，由于对磁盘的访问时间很长，DRAM缓存总是使用写回，而不是直写。DRAM通常都工作得很好，这是因为程序的局部性(时间局部性和空间局部性)原理，但是如果程序的常驻集(工作集)超过了DRAM的大小，则会导致页面不断被换入换出，也就是页面颠簸。 另外，前面我们讨论的页面调度，都是在不命中发生时，才换入页面，这种策略称为按需页面调度(demand paging)，也可以采用其它方法，比如尝试预测不命中，在页面实际被引用之前就换入页面。目前，所有现代系统使用的都是按需页面调度的方式。 2. 虚拟存储器提供的存储器管理由于每个进程都有独立的页表，因此也提供了一个独立的虚拟地址空间，多个虚拟页面可以映射到同一个共享物理物理上。独立虚拟地址空间和按需调度的结合，对系统中存储器的使用和管理有深远的影响。VM机制简化了链接，加载，代码和数据共享，以及应用程序的存储器分配。 简化链接：独立的地址空间，允许每个进程的存储器映像使用相同的基本格式，而不管代码和数据实际存放在物理存储器的何处。如Linux系统上每个进程都使用类似的存储器格式(.text,.data)， 这样的一致性极大地简化了链接器的设计与实现，允许链接器生成全链接的可执行文件，这些可执行文件独立于物理存储器中的代码和数据的最终位置。 简化加载：虚拟存储器还可以很容易地实现可执行文件和共享对象文件的加载，要把可执行文件加载到内存中，系统先分配虚拟页的一个连续的块(chunk)，将页表条目指向目标文件中的适当位置，并且标记为未缓存的，即可将可执行文件中的指定节加载到虚拟内存中。需要注意的是，此时文件并未被真正加载到物理内存中，而是要等每个页初次被引用时，才会真正执行页面调入(按需调度)。另外，虽然分配的虚拟内存是连续的，但是具体缓存的物理页面，可以是离散的。 简化存储器分配：虚拟存储器为用户提供一个简单的分配额外存储器的机制，当程序要求额外的堆空间时(如调用malloc)，操作系统分配k个连续的存储器页面，并且将它们映射到物理存储器中的任意位置的k个物理页面。由于页表的存在，存储系统没有必要分配k个连续的物理页，页面可以随机分散在物理存储器中 3. 虚拟存储器提供的存储器保护由于每个进程都有自己的独立虚拟地址空间，因此分离不同进程的私有存储器变得很容易。但进程私有存储器和共享存储器仍然需要访问控制(只读，可写等)，这只需要在页表标志位上加上可读，可写等许可位来控制即可。如果一些指令违反了许可条件，CPU会触发异常，Unix Shell将这种异常报告为”段错误”(segmentation fault)。 虚拟存储器的应用1. Linux虚拟存储器系统 如上，是Linux进程的虚拟存储器，Linux进程虚拟存储器有如下特性： 1.1 分段管理Linux将虚拟存储器组织成一些区域(段)的集合，一个区域就是已经分配虚拟存储器的连续片，这些页是以某种方式相关联的。如代码段，数据段，堆，栈，共享库等。该进程每个存在的虚拟页都保存在某个段中，不属于某个段的虚拟页是不存在的，并且不能被进程引用。段的概念很重要，因为它允许虚拟地址空间有间隙，内核不用记录哪些不存在的虚拟页，这样的页也不会占用存储器，磁盘，内核等任何额外资源。(页表中应该还是存在所有虚拟页的条目的)。 如图，每个task_struct对应一个进程，其中pgd字段为第一级页表(页全局目录)基址，mmap字段则指向进行的虚拟内存结构信息，Linux为每个段分配一个vm_area_struct结构，这个结构是个链表，在对虚拟地址执行地址翻译时，Linux会先遍历vm_area_struct链表，判断VA是否合法，即是否在某个段中。之后根据vm_area_struct中的信息判断进程是否对VA有访问权限。由于遍历vm_area_struct可能带来的开销，Linux还使用AVL树来加速查询。 1.2 数据共享与访问控制通过VM提供强大抽象能力，Linux进程可以实现多进程虚拟内存布局的灵活控制，从访问权限来看，进程虚拟存储器包括内核虚拟存储器和进程存储器，从共享/私有来看，所有进程共享内核代码与全局数据结构，并且有自己独立的页表，堆栈段，task_struct结构等。 这里有一些参考：Linux进程内存布局，Linux地址翻译 2. 存储器映射除了本身的段管理之外，Linux还允许用户手动建立段到磁盘对象之间的映射。虚拟存储器段可以映射到两种类型的文件对象： 普通文件：一个段可以映射到普通磁盘文件的连续部分，文件区被分为页大小的片，每一片即为对应虚拟页面的初始内容(剩余部分用0填充)。由于按需页面调度，所有在进行存储器映射时，虚拟页面并没有实际交换如物理存储器，直到CPU第一次引用页面。 匿名文件：匿名文件是由内核创建的，包含的全是二进制0，虚拟存储器段被映射到匿名文件时，当CPU第一次引用到虚拟页，该虚拟页面将被0覆盖。磁盘和存储器之间并没有实际的数据传送。因此，映射到匿名文件的段，也叫做请求二进制0的页。 无论在那种情况下，一旦虚拟页面被初始化了，就在一个由内核维护的交换文件(交换空间)中换来换去，在任何时刻，交换空间都限制这当前运行着的进程能够分配的虚拟页面总数。 一个对象可以以共享对象或私有对象的方式被映射到存储器段，前者对段的写操作其它进程可见，并且会写回到磁盘的原始对象中。后者的改动是私有的，只有自己可见，并且不会被写回。对于私有对象，Linux使用一种叫写时拷贝的机制来高效地执行映射，在执行私有映射时，物理内存在只有一份私有对象拷贝(但却被映射到不同虚拟地址空间中的段)，只有当有进程尝试写私有区域的某个页面时，才会创建拷贝。fork()函数内部就是写时拷贝原理。 内核通过唯一的文件名来判断文件是否已经被加载，从而复用已映射的物理页面，使映射到多个共享区域的对象，在内存中，只需要存放一份拷贝。存储器映射有很多实际应用，如进程快速加载文件，进程间共享文件，动态链接库，execve()等。在Linux下，用户可通过mmap()来手动建立一个映射，这里有一篇不错的[mmap()的用法详解][]。 3. 存储器分配应用程序可通过malloc申请一块连续的虚拟内存，在Linux下，虚拟内存的布局规定了malloc申请位置以及大小，当malloc申请小于MMAP_THRESHOLD(目前为128KB)的内存时，分配的是在堆区，用sbrk()进行对齐生长，而malloc一次性申请大内存(大于128K)时，分配在映射区(位于堆栈之间)，而不是在堆区，glibc会返回一块匿名的mmap内存块。虽然malloc得到的虚拟内存对应用程序来说是连续的，而实际上可以是离散的物理页面，这一点大家都应该很清楚了。","tags":[{"name":"os","slug":"os","permalink":"http://wudaijun.com/tags/os/"}]},{"title":"MongoDB 存储引擎","date":"2016-06-24T16:00:00.000Z","path":"2016/06/mongodb-storage-engine/","text":"MongoDB目前支持三种存储引擎：MMAPv1 Storage Engine，In-Memory Storage Engine, WiredTiger Storage Engine。 MMAPv1 Storage EngineMongoDB3.2之前版本的默认引擎。 1. 存储原理文档在磁盘中连续存放，文档所占用的磁盘空间包括文档数据所占空间和文档填充(padding)。 摘自：MongoDB MMAPv1内部实现：http://www.mongoing.com/archives/1484 1.1 文档移动由于文档在磁盘中连续存放，当文档大小增长时，可能需要重新分配文档空间，并更新索引。这会使写入效率降低，因此通常MongoDB为文档分配的record空间会包括document数据和padding空间。这样减少了文档移动的可能性，提高了写入效率。 1.2 padding算法 在MongoDB3.0之前，MMAPv1使用填充因子(padding factor)来决定空间分配，填充因子会根据文档移动的频繁度动态调整(初始时为1.0)，当padding factor = 1.5时，MMAPv1将为文档分配sizeof(record) = 1.5 * sizeof(document)的空间，其中0.5*sizeof(document)用作padding。 padding factor这种方式看起来很智能，但是由于文档的record大小不一，在文档删除或移动之后，文档原来分配的空间很难被再次利用，从而造成了磁盘碎片，这也是MongoDB3.0之前数据占用磁盘空间大的主要原因之一。 因此在MongoDB3.0之后，不再使用padding factor填充机制，而使用Power of 2 Sized Allocations，为每个文档分配2的N次方的空间(超过2MB则变为2MB的倍数增长)，这样做既可以减少文档的移动，文档被删除或移动后的空间也可以被有序地组织起来，达成复用(只能被其所在collection的文档复用)。除了Power of 2 Sized Allocations外，MongoDB3.0还提供了no padding分配策略，即只分配文档实际大小的磁盘空间，但应用程序需要确保文档大小不会增长。 虽然Power of 2 Sized Allocations解决了磁盘碎片的问题，但改进后的MMAPv1引擎仍然在数据库级别分配文件，数据库中的所有集合和索引都混合存储在数据库文件中，并且删除或移动文档后的空间会被保留用以复用，因此磁盘空间无法无法即时自动回收的问题仍然存在(即使drop collection)。 2.并发能力在MongoDB3.0之前，只有MMAPv1存储引擎支持，并且只支持Database级的锁，有时候不得不刻意将数据分到多个数据库中提升并发能力。在MongoDB3.0之后，MMAPv1终于支持collection级的并发，并发效率提升了一个档次。参考MongoDB concurrency FAQ。 3. 故障恢复MongoDB默认记录所有的变更操作日志(journal)并写入磁盘，MongoDB flush变更日志的频率(默认100ms)比flush数据的频率(默认60s)要高，因此journal是MongoDB故障恢复的重要保障。 4. 内存占用由于MMAPv1使用mmap来将数据库文件映射到内存中，MongoDB总是尽可能的多吃内存，以映射更多的数据文件。并且页面的换入换出基本交给OS控制(MongoDB不建议修改flush频率)，因此，将MongoDB部署在更高RAM环境下，是提升性能的最有效的方式之一。 5. 遗留问题 磁盘占用，运维人员可能需要定期的整理数据库(compat，repairDatabase) 内存占用，基本是有多少吃多少 collection级的并发控制仍然偏弱 WiredTiger Storage EngineMongoDB version3.0中引入，在MongoDB3.2中，已将WiredTiger作为默认存储引擎。 1. 并发能力文档级别的并发支持，WiredTiger通过MVCC实现文档级别的并发控制，即文档级别锁。这就允许多个客户端请求同时更新一个集合内存的多个文档。更多MongoDB并发模型，参见MongoDB concurrency FAQ。 2. 故障恢复支持checkpoint和journal两种方式进行持久化。 checkpoint是数据库某一时刻的快照，每60s或超过2GB的变更日志执行一次，在写最新快照时，上一个快照仍然有效，防止MongoDB在快照落地时挂掉，在快照落地完成后，上一个快照将被删除。 和MMAPv1一样，支持通过变更日志故障恢复，journal可与checkpoint集合使用，提供快速，可靠的数据恢复。可禁用wiredtiger journal，这在一定程度上可以降低系统开支，对于单点MongoDB来说，可能会导致异常关闭时丢失checkpoint之间的数据，对于复制集来说，可靠性稍高一点。在MongoDB3.2之前的版本中，WiredTiger journal默认在日志超过100MB时持久化journal一次，系统宕机最多会丢失100MB journal数据。在3.2版本中，加入了默认50ms时间间隔刷盘条件。参见官方文档journaling wiredtiger。 3. 磁盘占用不同于MMAPv1在数据库级别分配数据文件，WiredTiger将每个collection的数据和索引单独存放，并且会即时回收文档和集合占用空间。 WiredTiger的另一个两点是支持日志，文档数据块，索引压缩，可配置或关闭压缩算法，大幅度节省了磁盘空间。 4. 内存占用WiredTiger支持内存使用容量配置，用户可通过WiredTiger CacheSize配置MongoDB WiredTiger所能使用的最大内存，在3.2版本中，该参数默认值为max(60%Ram-1GB, 1GB)。这个内存限制的并不是MongoDB所占用的内存，MongoDB还使用OS文件系统缓存(文件可能是被压缩过的)。 5. 遗留问题相较于MMAPv1，压缩算法和新的存储机制，极大减少了磁盘空间占用，文档级别的并发控制，在多核上吞吐量有明显提升。MongoDB WiredTiger仍然是个吃内存的家伙，虽然可以配置内存最高占用，但更多的内存确实能带来更好的读写效率。 In-Memory Storage Engine纯内存版的MongoDB，限企业版，64bits。简单介绍一下： 不维护任何磁盘数据，包括配置数据，索引，用户证书，等等，Everything In Memory 文档级别的并发支持 在启动时配置最大使用内存，默认1GB，超出使用内存将会报错 不可落地，不可恢复 支持分片，复制集 总结：为啥不用Redis？ 参考： MongoDB Storage Engine: https://docs.mongodb.com/manual/core/storage-engines/ MongoDB Storage FAQ: https://docs.mongodb.com/manual/faq/storage/ MongoDB MMAPv1内部实现： http://www.mongoing.com/archives/1484 MongoDB WiredTiger内部实现：http://www.mongoing.com/archives/2540 MongoDB存储特性与内部原理： http://shift-alt-ctrl.iteye.com/blog/2255580 MongoDB3.0官方性能测试报告：http://www.mongoing.com/archives/862","tags":[{"name":"mongodb","slug":"mongodb","permalink":"http://wudaijun.com/tags/mongodb/"}]},{"title":"MongoDB 读写要点","date":"2016-06-22T16:00:00.000Z","path":"2016/06/mongodb-read-write/","text":"一. 查询1.1 “多段式”查询当执行find操作的时候，只是返回一个游标，并不立即查询数据库，这样在执行之前可以给查询附加额外选项。几乎所有游标对象的方法都返回游标本身，因此可以按任意顺序组成方法链。以下查询是等价的： &gt; var cursor = db.foo.find().sort({&quot;x&quot;:1}).limit(1).skip(10) &gt; var cursor = db.foo.find().limit(1).sort({&quot;x&quot;:1}).skip(10) &gt; var cursor = db.foo.find().skip(10).limit(1).sort({&quot;x&quot;:1}) 此时，我们只是构造了一个查询，并没有执行实际操作，当我们执行： &gt; cursor.hasNext() 这时，查询被发往服务器，sell立即获取第一个块(前101个文档或前1M数据，取其小者)，这样下次调用next或hasNext时，就不必再次向服务器发起一次查询。客户端用完了第一组结果，shell会再次向服务器获取下一组结果(大小不超过4MB)， 直至结果全部返回。可通过batchSize设置游标返回的块的文档数量。 注：如果在shell中，没有将返回的游标赋给一个var，shell将自动迭代游标20次，显示出前20调记录。 1.2 快照查询由于find()操作是多段式的，集合在游标查询的过程中，文档可能由于大小改变而发生了移动，比如某个文档由于增大，超过了原来分配的空间，导致文档被移动到集合的末尾处，此时使用游标查询可能会再次返回这些被移动的文档。解决方案是对查询进行快照: &gt; db.foo.find().snapshot() 1.3 游标释放前面看到的游标都是客户端游标，每个客户端游标对应一个数据库游标，数据库游标会占用服务器资源，因此合理地尽快地释放游标是有必要的。以下几种情况将会释放数据库游标： 客户端主动发起关闭游标请求 游标迭代完匹配结果 客户端游标不在作用域(客户端游标被析构/GC)，会向服务器发送消息销毁对应数据库游标 游标10分钟未被使用，数据库游标会自动销毁，可通过noCursorTimeout(注意和maxTimeMS的区别)取消游标超时 1.4 cursor.explain()游标的另一个很有用的函数是explain()，它能够提供db.collection.find()操作的详尽分析，包括 查询方案的决策：使用和何种方案(如使用哪个索引)，查询方向 执行结果分析：扫描了多少文档，多少个索引条目，花费时间等 服务器信息：地址，端口，版本等 分片信息：如果集合使用了分片，还会列出访问了哪个分片，即对应的分片信息 这些信息对于开发期间的查询性能分析和索引的对比性测试是非常有帮助的，关于它的详细解释，参见cursor.explain()官方文档。 1.5 读取策略在目前最新的MongoDB 3.2版本中，新加了读取策略(ReadConcern)，支持local和majority两种策略，前者直接读取当前的MongoDB实例，但是可能会读到副本集中不一致的数据，甚至可能回滚。majority策略读取那些已经被副本集大多数成员所认可的数据，因此数据不可能被回滚。目前majority只被WiredTiger存储引擎所支持。 1.6 其它查询技巧 不要使用skip()来实现分页，这样每次都会查询所有文档，可利用每页最后一个文档中的key作为查询条件来获取下一页。 获取随机文档，不要先将所有的文档都找出来，然后再随机。而是为所有的文档加一个随机Key，每次查询{“$gte”:randomkey}或{“$lt”:randomkey}即可 MongoDB对内嵌文档的支持非常完善，可通过{“key1.key2”: value2}直接查询内嵌文档，也可以在内嵌文档Key上建立索引 二. 写入2.1 写入策略MongoDB支持灵活的写入策略(WriteConcern): 用法：db.collection.insert({x:1}, {writeConcern:{w:1,j:false}}) w: 数据写入到number个节点才向客户端确认 {w: 0}: 对客户端的写入不需要发送任何确认，适用于性能要求较高，但不关注正确性的场景 {w: 1}: 默认的写入策略，数据写入到Primary就向客户端发送确认 {w: “majority”}: 数据写入到副本集大多数成员后向客户端发送确认，适用于对数据安全性要求高的场景，但会降低写入性能 j: 写入操作的journal持久化后才向客户端确认(需要w选项所指定的节点均已写入journal)，默认为false。 wtimeout: 写入超时时间，仅当w选项的值大于1才有效，当写入过程出现节点故障，无法满足w选项的条件时，超过wtimeout时间，则认定写入失败。 关于写入策略的具体实现，参见：http://www.mongoing.com/archives/2916 参考：MongoDB CURD概念： https://docs.mongodb.com/manual/core/crud/","tags":[{"name":"mongodb","slug":"mongodb","permalink":"http://wudaijun.com/tags/mongodb/"}]},{"title":"MongoDB 分片","date":"2016-06-14T16:00:00.000Z","path":"2016/06/mongodb-sharding/","text":"一. 什么是分片分片(shards)是使用多个机器存储数据的方法，MongoDB使用分片以支持巨大的数据存储量和对数据的操作。它将一个很大的集合分割成多个片(shard)，每个分片存储于不同的机器或副本集中，每个分片都是一个独立的数据库。 分片为提高系统吞吐量和大数据的存储提供了方法： 使用分片减少了每个分片需要处理的请求数 使用分片减少了每个分片存储的数据量 二. 分片的基础设施1. 片键（shard key）MongoDB通过片键将一个集合分为多个部分，它决定了一个集合的文档在不同的分片上的分布。它有如下特性： 片键字段可以是单个字段或者是复合字段 片键字段必须被索引(或是索引的前缀) 片键字段不能是多键索引 集合中的每个文档都必须包含片键字段 文档的片键值不可被修改，只能删除文档，并重新插入新片键文档 不同文档的片键可以相同 MongoDB的片键有两种： 1.1 基于范围的片键如有Person集合，其片键age，有两个分片，那么分片1可能负责存储-∞&lt;=age&lt;20的文档，分片2负责存取20&lt;=age&lt;∞的文档，这里的∞代表MongoDB所有值的最大值，而不仅限于数字。这是最简单的基于范围的片键模型，我们需要手动指定某个范围的片键位于某个分片上，MongoDB的分片机制要比这个要灵活强大得多。 1.2 基于哈希的片键：同样，我们可以基于片键哈希值来进行分片之间的分发，集合片键需要有足够大的基数。 在写入的时候，MongoDB(mongos)会将请求分发到负责该片键的分片上，在查询的时候，如果查询涉及了片键，则和写入一样，MongoDB会将请求分发到对应的片键上(针对性查询)，否则，MongoDB必须将请求发送到所有的分片上，以获取结果。 更多参考：http://docs.mongoing.com/manual-zh/core/sharding-shard-key.html 2. 块分裂（chunk split）随着文档的不断写入，各个分片的集合大小会拉开差距，单个分片的集合大小仍然可能是个问题。MongoDB采用单分片多区间的方式来将片键映射到分片，某个区间片键内的文档被称作”块”(chunk)，当某个块超过设置的块大小(sharding-chunk-size，默认为64M，会动态调整)时，MongoDB会负责将这个数据块分割为两个，分裂会改变元信息，但效率很高，对集群的性能也没有影响。 3. 块迁移 (chunk migration)随着块分裂持续下去，会导致不同分片之间的块数量和压力的不均衡，这个时候，MongoDB会开始一次分片间的数据块迁移，平衡各个分片的块数量，并调整各个分片的片键区间。MongoDB允许管理员控制块阀值大小，并且可以通过标记来直接决定集群分布。 块分裂，块迁移示例图： 分片 片键 块 分片1 age (-∞, 20) 分片2 age [20, ∞) 分片2的文档持续增长，导致块分裂： 分片 片键 块 分片1 age (-∞, 20) 分片2 age [20, 30), [30, 50), [50, ∞) 当各个分片之间的块数量差距过大时，导致块迁移： 分片 片键 块 分片1 age (-∞, 20), [20, 30) 分片2 age [30, 50), [50, ∞) 分块过程是由平衡器(balancer)来控制的，整个分裂和迁移过程都是自动的，块的分裂阀值可设置，如果你想要人为控制块分布，只能关闭平衡器，或者重新选择片键。 三. 如何选择片键1. 范围片键 vs 哈希片键基于范围的分片方式提供了更高效的范围查询，给定一个片键的范围，分发路由可以很简单地确定哪个数据块存储了请求需要的数据，并将请求转发到相应的分片中。 不过,基于范围的分片会导致数据在不同分片上的不均衡，有时候，带来的消极作用会大于查询性能的积极作用。比如，如果片键所在的字段是线性增长的，一定时间内的所有请求都会落到某个固定的数据块中，最终导致分布在同一个分片中。在这种情况下，一小部分分片承载了集群大部分的数据，系统并不能很好地进行扩展。 与此相比,基于哈希的分片方式以范围查询性能的损失为代价，保证了集群中数据的均衡。哈希值的随机性使数据随机分布在每个数据块中,因此也随机分布在不同分片中。但是也正由于随机性，一个范围查询很难确定应该请求哪些分片，通常为了返回需要的结果，需要请求所有分片。 2. 哈希片键的选择哈希片键的选择是比较简单的，片键只需要满足基数够大，通常ObjectId，自增ID，时间戳，都是不错的选择。 3. 范围片键选择范围片键的选择是比较复杂的，但目标是一致的：读写分离和数据局部性，举几个例子： a. 小基数片键：小基数片键随着数据增长和块分裂，单个块的片键范围越来越小，最终可能会形成单个块对应单个片键，此时无法再进行块分裂，从而导致单个块过大，吞吐量也会受到影响。 小基数片键满足数据局部性，并且基于片键的查询效率也很高，但MongoDB不能对数据块进行有效地分割，导致读写不能分离。 解决方案：如果是要基于该小基数片键进行大量的查询，可以选择组合片键，确保第二个字段有足够大的基数。 b. 升序片键：我们可能会选择ObjectId这类自增Key作为片键，这类Key的问题是可能会造成单一且不可分散的性能单点，因为新数据总是写入最新的数据块，没有做到写分离。 c. 随机片键随机片键的原理有点像哈希片键，比如选择MD5这类Key来做片键，这样能够很好地满足读写分离，文档被随机分布在分片中。但和哈希片键不同的是，哈希片键对片键查询仍然是比较高效的，根据片键算出哈希值，找到指定分片即可。对于随机片键来说，通常我们不会基于随机片键进行查询，而非片键查询需要向所有分片发出请求。因此实际上随机片键相对于哈希片键，在灵活性(哈希函数可以自己设置)，查询效率等方面都是有所不如的。 而无论哈希片键还是随机片键，都存在一个问题，数据过于分散，数据局部性不是很高，可能会导致块迁移时，磁盘IO开销很大(冷数据)。 d. 好的片键由于数据通常满足时间局部性，因此首先我们希望数据大致按照时间排序，但同时，我们希望数据能均匀分布，不要造成性能热点，因此添加一个搜索键作为第二片键。比如{&quot;month&quot;:1, &quot;name&quot;:1}，在3月时，当数据量够大时，MongoDB能够根据name键有效合理地分块，随着时间增长，在4月时，3月的数据开始不再被使用，置换出内存，渐渐成为冷数据，并且由于3月的数据不再写入，因此3月的数据也无需被分裂从而进一步造成块迁移，因此不会造成块迁移磁盘IO开销大的问题。 一般情况下，好的片键可以通过该公式推导：{控制局部化:1, 控制读写分离:1}，控制局部化的键可以是粗粒度时间(考虑一下为什么不是细粒度？)，控制读写分离的键通常是查询键。","tags":[{"name":"mongodb","slug":"mongodb","permalink":"http://wudaijun.com/tags/mongodb/"}]},{"title":"Erlang 状态监控","date":"2016-05-25T16:00:00.000Z","path":"2016/05/erlang-debug-online/","text":"一. 接入远程节点1. JCLerl -sname n2 -setcookie 123 (n2@T4F-MBP-11)1&gt; //^G User switch command --&gt; r &#39;n1@T4F-MBP-11&#39; --&gt; c Eshell V8.1 (abort with ^G) (n1@T4F-MBP-11)1&gt; 2. remsherl -setcookie abc -name node_2@127.0.0.1 -remsh node_1@127.0.0.1 和第一种JCL方式是同一个原理，这也是rebar2 remote_console的实现方式。 3. erl_call erl_call -s -a &#39;erlang memory &#39; -name node_1@127.0.0.1 -c abc 4. run_erl run_erl -daemon tmp/ log/ &quot;exec erl -eval &#39;t:start_link().&#39;&quot; run_erl是随OTP发布的命令，它通过管道来与Erlang节点交互，仅类Unix系统下可用。上面的命令启动Erlang节点，将tmp/目录设为节点管道目录，之后run_erl会在tmp下创建erlang.pipe.1.r erlang.pipe.1.w两个管道文件，外部系统可通过该管道文件向节点写入/读取数据。可用OTP提供的to_erl命令通过管道连接到节点: to_erl tmp/ Attaching to tmp/erlang.pipe.1 (^D to exit) 1&gt; 需要注意的当前你是直接通过Unix管道和节点交互的，并不存在中间代理节点(和remsh方式不同)，因此在这种情况下使用JCL ^G+q会终止目标节点。如果要退出attach模式而不影响目标节点，使用^D。 run_erl另一个作用是输出重定向，上例中将所有输出(包括虚拟机和nif输出)重定向到log/erlang.log.*，这对多日志渠道(lager,io:format,c,lua等)的混合调试是有所帮助的。 rebar2便通过run_erl实现节点启动，并使用to_erl实现attach命令。 5. ssh ---------- Server: ----------- $ mkdir /tmp/ssh $ ssh-keygen -t rsa -f /tmp/ssh/ssh_host_rsa_key $ ssh-keygen -t rsa1 -f /tmp/ssh/ssh_host_key $ ssh-keygen -t dsa -f /tmp/ssh/ssh_host_dsa_key $ erl 1&gt; application:ensure_all_started(ssh). {ok,[crypto,asn1,public_key,ssh]} 2&gt; ssh:daemon(8989, [{system_dir, &quot;/tmp/ssh&quot;}, 2&gt; {user_dir, &quot;/home/ferd/.ssh&quot;}]). {ok,&lt;0.52.0&gt;} ---------- Client ------------- $ ssh -p 8989 ferd@127.0.0.1 Eshell Vx.x.x (abort with ^G) 1&gt; 二. etopetop是Erlang提供的类似于top命令，它的输出格式和功能都与top类似，提供了必要的节点信息和进程信息。常用用法： % 查看占用CPU最高的进程 每10秒输出一次 &gt; spawn(fun() -&gt; etop:start([{interval,10}, {sort, runtime}]) end). % 查看占用内存最高的进程 每10秒输出一次 输出进程数量为20 &gt; spawn(fun() -&gt; etop:start([{interval,10}, {sort, memory}, {lines,20}]) end). % 连接远程节点方式一 &gt; erl -name abcd@127.0.0.1 -hidden -s etop -output text -sort memory -lines 20 -node &#39;server_node@127.0.0.1&#39; -setcookie galaxy_server % 连接远程节点方式二 &gt; erl -name abc@127.0.0.1 -hidden -setcookie galaxy_server &gt; etop:start([{node,&#39;server_node@127.0.0.1&#39;}, {output, text}, {lines, 20}, {sort, memory}]). % 连接远程节点方式三 &gt; erl -name abc@127.0.0.1 -setcookie galaxy_server &gt; rpc:call(&#39;server_node@127.0.0.1&#39;, etop, start, [[{output, text}, {lines, 20}, {sort, memory}]]). 输出样例(截断为前5条)： ======================================================================================== &#39;def@127.0.0.1&#39; 09:38:01 Load: cpu 0 Memory: total 14212 binary 40 procs 35 processes 4398 code 4666 runq 0 atom 198 ets 304 Pid Name or Initial Func Time Reds Memory MsgQ Current Function ---------------------------------------------------------------------------------------- &lt;6858.7.0&gt; application_controll &#39;-&#39; 7830 426552 0 gen_server:loop/6 &lt;6858.12.0&gt; code_server &#39;-&#39; 125106 284656 0 code_server:loop/1 &lt;6858.33.0&gt; erlang:apply/2 &#39;-&#39; 10300 230552 0 shell:get_command1/5 &lt;6858.3.0&gt; erl_prim_loader &#39;-&#39; 211750 122040 0 erl_prim_loader:loop &lt;6858.0.0&gt; init &#39;-&#39; 3775 18600 0 init:loop/1 ======================================================================================== 官方文档：http://erlang.org/doc/apps/observer/etop_ug.html 三. erlang API1. 内存通过erlang:memory()可以查看整个Erlang虚拟机的内存使用情况。 2. CPUErlang的CPU使用情况是比较难衡量的，由于Erlang虚拟机内部复杂的调度机制，通过top/htop得到的系统进程级的CPU占用率参考性是有限的，即使一个空闲的Erlang虚拟机，调度线程的忙等也会占用一定的CPU。 因此Erlang内部提供了一些更有用的测量参考，通过erlang:statistics(scheduler_wall_time)可以获得调度器钟表时间： 12345671&gt; erlang:system_flag(scheduler_wall_time, true).false2&gt; erlang:statistics(scheduler_wall_time).[&#123;&#123;1,166040393363,9269301338549&#125;, &#123;2,40587963468,9269301007667&#125;, &#123;3,725727980,9269301004304&#125;, 4,299688,9269301361357&#125;] 该函数返回[{调度器ID, BusyTime, TotalTime}]，BusyTime是调度器执行进程代码，BIF，NIF，GC等的时间，TotalTime是cheduler_wall_time打开统计以来的总调度器钟表时间，通常，直观地看BusyTime和TotalTIme的数值没有什么参考意义，有意义的是BusyTime/TotalTIme，该值越高，说明调度器利用率越高： 12345678910111&gt; Ts0 = lists:sort(erlang:statistics(scheduler_wall_time)), ok.ok 2&gt; Ts1 = lists:sort(erlang:statistics(scheduler_wall_time)), ok.ok 3&gt; lists:map(fun(&#123;&#123;I, A0, T0&#125;, &#123;I, A1, T1&#125;&#125;) -&gt; &#123;I, (A1 - A0)/(T1 - T0)&#125; end, lists:zip(Ts0,Ts1)).[&#123;1,0.01723977154806915&#125;, &#123;2,8.596423007719012e-5&#125;, &#123;3,2.8416950342830393e-6&#125;, &#123;4,1.3440177144802423e-6&#125;&#125;] 3. 进程通过length(processes())/length(ports())统计虚拟机当前进程和端口数量。 关于指定进程的详细信息，都可以通过erlang:process_info(Pid, Key)获得，其中比较有用的Key有： dictionary: 进程字典中所有的数据项 registerd_name: 注册的名字 status: 进程状态 links: 所有链接进程 monitored_by: 所有监控当前进程的进程 monitors: 所有被当前进程监控的进程 trap_exit: 是否捕获exit信号 current_function: 当前进程执行的函数，{M, F, A} current_location: 进程在模块中的位置，{M, F, A, [{file, FileName}, {line, Num}]} current_stacktrace: 以current_location的格式列出堆栈跟踪信息 initial_call: 进程初始入口函数，如spawn时的入口函数，{M, F, A} memory: 进程占用的内存大小(包含所有堆，栈等)，以bytes为单位 message_queue_len: 进程邮箱中的待处理消息个数 messages: 返回进程邮箱中的所有消息，该调用之前务必通过message_queue_len确认消息条数，否则消息过多时，调用非常危险 reductions: 进程规约数 获取端口信息，可调用erlang:port_info/2。 关于OTP进程，Erlang提供了更为丰富的调试模块，如sys，其中部分常用函数： sys:log_to_file(Pid, FileName)： 将指定进程收到的所有事件信息打印到指定文件 sys:get_state(Pid)： 获取OTP进程的State sys:statistics(Pid, Flag): Flag: true/false/get 打开/关闭/获取进程信息统计 sys:install/remove 可为指定进程动态挂载和卸载通用事件处理函数 sys:suspend/resume: 挂起/恢复指定进程 sys:terminate(Pid, Reason): 向指定进程发消息，终止该进程 四. reconrecon是learn you some erlang的作者写的一个非常强大好用的库，将erlang散布在各个模块的调试函数整合起来，以更易用和可读的方式提供给用户，包含了信息统计，健康状态分析，动态追踪调试等一整套解决方案。并且本身只是一系列的API，放入rebar deps即可attach上节点使用，强烈推荐。 下面是我常用的几个函数: % 找出当前节点Attr属性(如message_queue_len)最大的N个进程 recon:proc_count(Attr, N). % 对节点进行GC，并返回进程GC前后持有的binary差异最大的N个进程 recon:bin_leak(N). % process_info的安全增强版本 recon:info/1-2-3-4 % 返回M毫秒内的调度器占用 recon:scheduler_usage(M) % 强大的动态追踪函数，可用于动态挂载钩子。 % 1. 可挂载模块/函数调用(甚至可对参数匹配/过滤) % 2. 可对调用进程筛选(指定Pid，限制新建进程等) % 3. 可限制打印的追踪数量/速率 % 4. 其它功能，如输出重定向，追踪调用结果等 recon_trace:calls/2-3 详细用法参见：http://ferd.github.io/recon/ 五. 更多资料 Erlang实践红宝书：Erlang In Anger","tags":[{"name":"erlang","slug":"erlang","permalink":"http://wudaijun.com/tags/erlang/"}]},{"title":"Erlang 实践经验","date":"2016-05-24T16:00:00.000Z","path":"2016/05/erlang-practices/","text":"使用binary而不是lists所有能用binary的地方都用binary，配置，协议，DB，网络接口，等等，如果驱动或第三方库不能很好地支持binary，就换一个或者重写接口，如果项目一开始就这么做，会在后期省掉很多麻烦。 不要动态生成 atomatom不被Erlang GC，在代码中要时刻警惕诸如list(binary)_to_atom/2和binary_to_term/1等动态生成atom的API，特别是这些API的输入源来自于网络或客户端等不可信源，可用list(binary)_to_existing_atom/2和binary_to_term/2替换之，前者只能转换为已经存在的原子，否则会报错，后者功能类似，确保不会生成新的atom，函数引用，Pid等(这些资源都是不会被GC回收的)。 可通过erlang:memory/0来观察atom内存使用状况。 保证OTP进程的init/1是安全稳定的supervisor的start_link会同步启动整个supervisor树，如果某个孩子进程启动失败，supervisor会立即尝试重启，如果重启成功，继续启动下一个孩子进程，否则重试次数到达设定上限后，supervisor启动失败。 在实践中，不要将耗时，依赖外部不稳定服务的操作，放在init/1中，这会导致整个supervisor树启动的不稳定性。特别是对于一些网络连接，数据库读取等操作，应该尽量通过诸如gen_server:cast(self(), reconnect)的形式，将实际初始化延后，并维护好状态(是否已经初始化完成)。在handle_cast(reconnect, State)中处理具体的细节，不管是初始化，还是运行时的网络异常，都可以通过该函数处理。如果一个错误会经常性地出现在日常的操作中，那么是现在出现，还是以后出现是没有区别的，尽量以同样的方法处理它。 通常情况下，我们需要保证OTP进程的init/1是快速，稳定的，如果连启动过程都不是稳定的，那么supervisor的启动也就没有多大意义了(不能将整个系统恢复到已知稳定状态)，并且可能一个进程本身的非关键初始化错误，导致了整个supervisor启动失败。 initialization与supervision方法的不同之处：在initialization过程中，client的使用者来决定他们能容忍什么程序的错误，而不是client自己本身决定的。在设计容错系统中，这两者区别尤其重要。 — Erlang In Anger 消息队列膨胀Erlang系统最常见的问题是节点内存耗尽，而内存耗尽的主凶之一就是消息队列过长(Erlang消息队列的大小是无限制的)。要查看消息队列膨胀的进程不难，但是难的是找到消息队列膨胀的原因和解决方案。 常见的消息膨胀原因： 日志进程：特别是错误日志，重试，重启，链接都是产生大量错误日志消息的原因，解决方案：使用lager 锁和阻塞操作：如网络操作或阻塞等待消息等，解决方案：添加更多进程，化阻塞为异步等 非期望的消息：特别针对于非OTP进程，解决方案：非OTP进程定期flush消息队列，当然，尽量使用OTP进程 系统处理能力不足：解决方案：横向扩展，限制输入，丢弃请求等。","tags":[{"name":"erlang","slug":"erlang","permalink":"http://wudaijun.com/tags/erlang/"}]},{"title":"开发笔记(6) Erlang服务器集群常见问题","date":"2016-04-27T16:00:00.000Z","path":"2016/04/erlang-server-design6-distribution-question/","text":"集群的安全启动/终止典型地，我们需要在GS和各个服务都启动完成之后，再打开网关接入网络连接，这需要通过master节点进行监控，GS和服务也需要向cluster汇报自己的状态(启动中，运行中)，master在关键服务都启动完成并状态正常之后，再通知gateway开放连接。基于cluster中的控制信息，这一点不难做到。集群的终止流程也类似。 集群OPS管理master节点上还可以做一些简单运维工作： 监控服务器状态：所有GS和服务是否状态正常 自动开服：统计所有GS的人数，确认是否开新服，并同步到推荐服列表(DB操作) 故障恢复：服务器本身在sup之下，其组件Crash可通过sup重启恢复，而遇上物理机宕机或失联，master会通过心跳检测确认节点不可达，然后代为注销问题节点上所有的服务，之后master将问题节点上的GS重新部署到其它可用节点。比起物理机宕机，更棘手的是网络故障，当失联节点重新回到集群而master可能已经在其它节点重新部署了故障节点上的GS，解决方案之一是，当一个节点与master节点失联时，终止自身。 这些工作本质上仍然是，定时监控状态，统计状态，确认下一步操作(开新服，重新部署故障服务器等)。由于OPS监控时间间隔一般至少是分钟级别，master不需要将所监控的表设为ram_copies，减轻mnesia同步的网络负担。 master单点由于master负担了一些逻辑和状态(比如监控其它节点)，想要做一个透明过渡的主从是非常困难的，目前虽然有个主从，但应该问题还很多，待后续完善。或者是说，对一个游戏GS来说，master节点的单点是能够容忍的(因为Erlang OTP足够健壮)，遇上物理故障，停止整个集群或许是更好的办法。 多节点写入的安全性这在Actor模型中的经典问题，比如当服务进程S需要对玩家进程P上的资源进行操作，分两种情况，需要对资源进行增加时，向P发送资源增量即可，而需要对资源进行扣除时，则要分两步：1.检查资源是否足够，以决定下一步操作，2.向P发送消息扣除资源 ，有多种方案可以检查玩家进程资源是否足够，同步消息，异步消息，公共缓存，数据副本等，事实上，无论何种方式都不能保证当P进程在收到消息扣除资源时，资源还是S当时检查到的情形，因为这两步操作之间，P进程可能收到其它消息对资源进行了改动。这里我们选择容忍这种情况，因此一般我选择通过公共缓存或者数据副本来进行同步资源检查，然后通过向P发送消息来进行异步资源改动。注意，玩家资源的变动只能由玩家进程来进行，对缓存数据保证单写入者是减少不一致问题最有效的方式。 如果这类问题在开发中带来很多困扰，那么可以重新考虑一下资源所有权的问题。比如有一种资源A，它只在进程X中增加，在进程Y中扣除，如果这种设计是稳定的，那么我觉得将A挂在Y上更为适合，而如果X和Y都对A有频繁的改动操作的话，是否可以从设计上，将X中的A和Y中的A设计成两种资源，当玩家从X系统进入Y系统时，进行一次资源转换(策划案上可以叫”投放”，”运输”之类的东西)，这样两个系统都独立操作自己的资源，只在玩家选择进程资源转换时，再同步一次。当然，这里只是提供一种思路。 全联通问题由于cluster用mnesia来做后台同步，而mnesia后台同步时，会形成一个全联通网络(即使集群节点都是hidden节点)，虽然我们可以通过控制表的属性来避免不要的数据同步，但全联通网络的负载是比较大的，对于几十个节点的规模或许可以忍受，但更大的集群则需要正视这个问题了。由于负载量依赖于单个GS开销和机器性能，因此目前我们暂未考虑这个问题，等上线之后再观察。","tags":[{"name":"erlang","slug":"erlang","permalink":"http://wudaijun.com/tags/erlang/"},{"name":"distribution","slug":"distribution","permalink":"http://wudaijun.com/tags/distribution/"}]},{"title":"技术学习误区","date":"2016-04-17T16:00:00.000Z","path":"2016/04/learning-mistakes/","text":"前几天看一些Go框架的源码和相关工具，由于之前没有在正式项目中用过Go，看起来是有些吃力的，有一种顾头不顾尾的感觉，到之后仍然是困于其中，不得精髓。这和本人的一些学习习惯有关：迷信源码，觉得只有理解其源码，才能运用自如，不知道是不是部分受于C/C++出身的影响，底层思维根深蒂固。之前看的一些框架(skynet,firefly,kbengine等)也是一样，虽然直接看的源码，但是由于缺少实践，对其运用场景，优缺点没有足够的认识，能够吸收的干货也比较有限。加之工作上开始忙起来，难免急躁，事倍功半。 于是开始反思自己的学习方法，在游戏服务器这个领域也有两年多了，对这方面的技术细节，常用构件，设计模式等已经非常熟悉了，了解过多种语言设计服务器的特性和方式，因此技术细节和语言障碍对我来说已经问题不大了，如果按照木桶原理来说，我觉得，架构设计，线上经验，解决问题的能力才是木桶的短板，而这一块需要恰巧不是深度而是广度，以问题为出发点，将多种解决方案纵向对比，能够选出一套合适的方案并加以实施，才是正解。换句话说，如果设计一个可靠，易于维护的服务器，才是目前我应该关注的东西。其它的工具，都只是实现这个目标的一种方案。关于广度的学习，我觉得分为三个层级： 了解：了解方案的出发点，适用情形，以及别人的经验教训 实践：在项目中实践运用，评估其优缺点，瓶颈 剖析：在必要的时候，通过源码验证自己的想法，并针对于现有问题进行改进/优化 如此看来，自己确实是有些本末倒置了，错把技术而不是解决问题当做最终目标，这并不是说技术细节不重要，而是合适的学习方法，能够让你站在一个更高的角度去理解事物的出发点，目标，以及其存在的理由，更高效地学习。另一个重要的点是，技术始终是为产品服务的。 第一篇水文就这样了，引以为诫。","tags":[{"name":"other","slug":"other","permalink":"http://wudaijun.com/tags/other/"}]},{"title":"Erlang 分布式系统(2)","date":"2016-03-25T16:00:00.000Z","path":"2016/03/erlang-distribution-2/","text":"一. 分布式ErlangErlang为分布式提供的基础设施 良好的函数式编程语义，为并发而生 异步通信模型，屏蔽底层通讯细节(Erlang进程间/系统进程间/物理机间)，将本地代码扩展为分布式程序非常容易 透明的通信协议，完善的序列化/反序列化支持 完善的监控能力：监督(supervisor), 监视(monitor), 链接(link)等 其它分布式组件：如global,epmd,mnesia等 二. Erlang分布式基础1. Erlang node一个Erlang分布式系统由多个Erlang节点(node)组成，每一个节点即为一个Erlang虚拟机，这些节点可以彼此通信。不同节点节点上Pid之间通信(link,monitor等)，是完全透明的。 集群中每个Erlang节点都有自己的名字，通过-sname或-name设置节点名字，前者在局域网中使用，后者在广域网中使用，两种命名方式的节点不能相互通信。也可在节点启动后通过net_kernel:start/1来将一个独立节点转换为分布式节点。 Erlang节点之间通过TCP/IP建立连接并通信，集群中的节点是松散连接的(loosely connected)，只有当第一次用到其它节点名字时，才会和该节点建立连接(并且校验cookie)。但同时连接也是扩散(transitive)的，比如现有节点A,B相连，C,D相连，此时节点B连接节点C，那么A,B,C,D将两两相连形成一个全联通集群。要关闭Erlang节点的transitive行为，使用虚拟机启动选项-connect_all false。当节点挂掉后，其上所有的连接都会被关闭，也可通过erlang:disconnect_node/1关闭与指定节点的连接。 2. cookiecookie是Erlang节点连接时的简单验证机制，只有具有相同cookie的节点才能连接。通过-setcookie选项或erlang:set_cookie/2设置cookie，后者可以为一个节点设置多个cookie，在连接不同的节点时使用不同的cookie，连接到多个集群中。如果没有指定，将使用~/.erlang.cookie中的字符串作为cookie。由于cookie是明文的，并且共享于所有节点，更像是一种分隔集群的方式，而不是一种安全机制。 3. hidden node通过为节点启动参数-hidden，让一个节点成为hidden节点，hidden节点与其它节点的连接不会扩展，它们必须被显示建立。通过nodes(hidden)或nodes(connected)才能看到与本节点连接的hidden节点。 4. net_kernelnet_kernel管理节点之间的连接，通过-sname或-name启动参数或在代码中调用net_kernel:start/1可以启动net_kernel进程。net_kernel默认会在引用到其它节点时(如rpc:call/5, spawn/4, link/1等)，自动与该节点建立连接，通过-dist_auto_connect false选项可以关闭这种行为，如此只能通过net_kernel:connect_node/1手动显式地建立连接。 5. epmdepmd(Erlang Port Mapper Daemon)是Erlang节点所在主机上的守护进程，它维护本机上所有Erlang节点名到节点地址(Host:Port)的映射。 epmd会在主机上第一个Erlang分布式节点启动时自动后台启动，默认监听4369端口。当分布式节点启动时，VM会监听一个端口(可通过inet_dist_listen_min和inet_dist_listen_max限制端口范围)用于接收其它节点连接请求，之后节点会将节点名(@前半部分)和监听地址发给epmd进程，当节点和epmd进程断开TCP连接后，epmd会注销该节点地址信息。 当节点A(node_a@myhost1)尝试连接节点B(node_b@myhost2)时，节点A会先向myhost2上的empd进程(myhost2:4369)根据节点B名字(nodeb)查询节点监听地址，之后再连接这个监听地址和B节点通信。 默认配置下，epmd在物理机上的监听端口为4369，这意味着： 因为是周知端口，所以通过查询目标机器上的4369，就可以知道这个机器上节点的情况。 在同一机器可能会部署不同的Erlang集群，希望不要互相干扰。 防火墙不允许过4369端口，或者不在开放端口之列表。 我们可以指定epmd监听端口： // 单独启动epmd进程 empd -daemon -port 5000 // epmd随分布式节点启动而自动启动时，也可指定epmd的启动方案 erl -name hello -epmd &quot;epmd -port 5001 -daemon&quot; -epmd_port 5001 可以在虚拟机启动时，通过-epmd_port(或ERL_EPMD_PORT环境变量)指定要连接的epmd端口： // 通过启动选项 erl -name hello -epmd_port 5000 // 通过环境变量 ERL_EPMD_PORT=5000 erl -name hello 需要注意的是，一旦节点采用定制的epmd port，那么节点在连接其它节点的时候，也将使用定制的epmd端口访问epmd。因此，同一个集群的epmd端口必须是一致的。 参考： http://erlang.org/doc/man/epmd.html http://www.cnblogs.com/me-sa/p/erlang-epmd.html http://erlang.org/doc/apps/erts/erl\\_dist\\_protocol.html 6. globalglobal模块功能主要通过global_name_server进程完成，该进程在节点启动时启动。global模块主要包含如下功能： 全局锁global模块提供全局锁功能，可以在集群内对某个资源进行访问控制，当某个节点尝试lock某个资源时，global_name_server会muticall集群中所有节点上的global_name_server进程，只要其中一个节点上操作失败，本次lock也会失败，并引发下次重试或整个操作的失败。 global模块会在当前所有known(nodes())节点中推选出一个Boss节点(简单通过lists:max(Nodes)选出)，在设置全局锁时，会先尝试在Boss节点上上锁，再对其它节点上锁，这样保证全局资源的唯一性，又不需要单独设置中心节点。 全局名字管理global_name_server另一个职责是管理集群全局名字信息，global_name_server将全局名字信息缓存在ets，因此对全局名字的解析是非常快的，甚至不走消息流程。但是对名字的注册，需要先上全局锁，再muticall所有的global_name_server，进行本地ets名字更新，整个过程至少要muticall集群所有节点两次，对于这类耗时的操作，global_name_server有一个小技巧： 12345678910111213141516171819 % 外部进程（call调用）gen_server:call(global_server, &#123;something, Args&#125;)% global_name_server（任务异步分发）handle_call(&#123;something Args&#125;, State) -&gt; State#state.worker ! &#123;someting, Args, self()&#125;% Worker进程（实际任务，通过gen_server:reply手动模拟call返回）loop_the_worker() -&gt; receive &#123;do_something, Args, From&#125; -&gt; gen_server:reply(From, do_something(Args)); Other -&gt; unexpected_message(Other) end, loop_the_worker(). 维护全联通网络global的最后一个职责就是维护全联通网络，在global模块的源码注释中可以看到其网络信息同步协议： %% Suppose nodes A and B connect, and C is connected to A. %% Here&#39;s the algorithm&#39;s flow: %% %% Node A %% ------ %% &lt;&lt; {nodeup, B} %% TheLocker ! {nodeup, ..., Node, ...} (there is one locker per node) %% B ! {init_connect, ..., {..., TheLockerAtA, ...}} %% &lt;&lt; {init_connect, TheLockerAtB} %% [The lockers try to set the lock] %% &lt;&lt; {lock_is_set, B, ...} %% [Now, lock is set in both partitions] %% B ! {exchange, A, Names, ...} %% &lt;&lt; {exchange, B, Names, ...} %% [solve conflict] %% B ! {resolved, A, ResolvedA, KnownAtA, ...} %% &lt;&lt; {resolved, B, ResolvedB, KnownAtB, ...} %% C ! {new_nodes, ResolvedAandB, [B]} %% %% Node C %% ------ %% &lt;&lt; {new_nodes, ResolvedOps, NewNodes} %% [insert Ops] %% ping(NewNodes) %% &lt;&lt; {nodeup, B} %% &lt;ignore this one&gt; 在上面的源码注释中，可以看到global模块的全联通维护机制，集群中被连接的节点(Node A)，会将新加入的节点(Node B)介绍给集群中的其它节点(Node C)。同名字注册一样，global_name_server将全联通集群管理放在另一个Worker中执行。 global模块的名字注册只能在全联通网络下进行，这样才能在任意节点进行信息更新。在非全联通集群中(-connect_all false)，全局锁机制仍然是可用的。 注意到整个同步协议中，nodeup和nodedown消息是由net_kernel进程发布的。 global模块更加具体的实现细节没有细究，待后续详细理解。能够在不可靠的网络上实现一套全局锁和全联通管理方案，本身就是非常复杂的，因此还是值得一读。 7. mnesia参见：http://wudaijun.com/2015/04/erlang-mnesia/","tags":[{"name":"erlang","slug":"erlang","permalink":"http://wudaijun.com/tags/erlang/"},{"name":"distribution","slug":"distribution","permalink":"http://wudaijun.com/tags/distribution/"}]},{"title":"Erlang 分布式系统(1)","date":"2016-03-16T16:00:00.000Z","path":"2016/03/erlang-distribution-1/","text":"一. 分布式计算的八大谬论1. 网络总是可靠的在分布式系统中，你可能最容易犯下的错误就是认为远程节点是永远可用的，尽管这可以通过添加更多的硬件(比如主从)来实现，但这也带来了冗余性。网络可能随时因为断电，硬件故障，自然因素，人为因素等不可用，在这种情况下，保证你的程序能够在远程节点或者第三方服务不可用时，能够正常运行是非常重要的。Erlang除了能够监测到外部服务失去连接(或者不能响应)之外，没有任何的其它措施，毕竟，除了你自己以外，谁也不知道某个组件有多重要。 注意，Erlang跨节点的monitor和link操作是比较危险的，因为一旦远程节点不可用，将触发关于该节点所有的远程monitor和link，这可能会引起一场网络风暴，给系统带来意料之外的高负载。在不可靠的网络上构建可靠的分布式应用，需要你随时准备面临这种突发状况，并且保证系统能够继续正常稳定地工作。 2. 可忽略的延迟一个好的分布式系统的特性之一就是隐藏函数调用为远程调用的事实，然而这是一把双刃剑，一些本地上执行非常快的函数，通过网络调用时，会与预期大不相同。在这一点上，Erlang的通信模型处理得很好，Erlang的每个进程是孤立的，通过异步消息进行通信，这使得我们考虑超时，以及外部服务不可用的情况。将Erlang程序改造为分布式程序只需要做很少的工作，异步通信模型，超时，监视，链接等都可以保留。因此，Erlang在设计之初，并不忽略延迟的存在。但是作为设计者，在设计上，你需要对此留意。 3. 带宽是无限的尽管现在网络越来越快，每个字节在网络上传输的成本也越来越便宜，但是对网络负载过于乐观的假设仍然是具有风险的。关于这一点的一个小技巧是发送当前的事件，而不是当前的整个状态。如果在一些情况下，你不得不发送大消息，一定要小心，由于Erlang保证两个进程间的消息次序(即使是通过网络)，因此一个大消息可能会阻塞该通道上的其它消息。更糟糕的是，这可能会阻塞节点间的正常心跳，导致节点误以为对方节点无法响应并且断开连接。要避免这种情况发生的唯一方案就是控制消息的大小，这也是一条好的Erlang设计实践。 4. 网络是安全的当你在分布式环境下，信任你所接收到的任何消息是非常危险的，消息可能被刻意伪造或抓包改写，甚至外部节点可能已经完全被其他人控制。遗憾的是，Erlang做了这种假设，Erlang分布式系统的网络安全是非常薄弱的，这和它的历史有关。这意味着Erlang程序很少被部署在不同的数据中心，Erlang也不建议你这么做。你可以将你的系统分为很多小的，相互隔离的节点，并且部署在可靠的地方(一台物理机，或者安全的局域网)。但任何除此之外的东西，都需要开发者自己去实现，例如切换到SSL，或者实现你自己的更高层次的通信协议，或重写节点之间的通信协议等。关于这些主题可以参见How to implement an alternative carrier for the Erlang distribution和Distribution Protocol。即使做了更多的安全工作，也要非常小心，因为一旦有人获取了你的远程节点的访问权限，他就可以获取到节点上的一切，并且在该节点上执行任何命令。 5. 网络拓扑是不变的在分布式系统构建之初，你的系统原型可能有指定数量的节点并且确定了它们的网络位置(IP+Port)，但硬件错误，人为部署，负载均衡等，都会导致节点的动态添加和删除，这时集群的网络拓扑结构就会变化，如果你在程序中对集群节点位置有任何依赖，都将很难适应这种变化。在Erlang中，每个节点都有其名字(相当于IP+Port)，如果在程序中对这些节点地址硬编码，将会使程序很难动态扩展。 6. 只有一个管理员这是从系统运维上来说的，你可能只管理着整个系统的一部分，一方面你需要诊断系统问题的工具，这一点上来说，Erlang提供了比较完善的调试诊断系统，并且支持热更。另一方面，你需要做好系统各个部分间的通讯协议，以管理各个子系统的不同版本的兼容问题。 7. 数据传输代价为零这里的代价包括时间代价和金钱代价，前者指数据的序列化和反序列化时间，后者指数据对带宽的占用，从这个角度来看，将消息优化得小而短又一次被证明是很重要的。基于Erlang的历史，Erlang并没有对网络传输数据做任何压缩处理(尽管提供了这样的函数)，Erlang的设计者选择让开发者自己按需实现其通讯协议，减小数据传输的代价。 8. 集群是同质的这里的同质(Homogeneous)指的是同一种语言(或协议)，在分布式集群中，你不能对所有节点都是同质的作出假设，集群中的节点可能由不同的语言实现，或者遵从其它通讯协议。你应该对集群持开放原则而不是封闭原则。对Erlang来说，分布式协议是公开的，但所有的Erlang节点都假设其它节点都是同质的，外部节点想要将自己整合在Erlang集群中，有两种方式： 第一种方式是使自己看起来像Erlang节点，实现Erlang节点的\b通讯协议，比如Erlang C-nodes，这样其它语言实现的节点也能像Erlang节点一样运行于Erlang集群中。 另一种方式是使用其它的数据交换协议，如BERT-RPC，一种类似于XML或JSON的数据交换格式，但与Erlang External Term Format更为契合。 关于以上8点更多请参考：Fallacies of Distributed Computing Explained。 二. 节点检活对于分布式系统来说，其中最让人头疼的事情之一就是当节点不可响应(或网络错误)时的处理流程。一个节点不可响应的因素有很多：网络故障，网络拥塞，硬件错误，应用崩溃，节点忙碌等，几乎没有方式能够对问题节点当时的状态进行确认和诊断，这个时候，其它节点有几种处理方式：继续等待响应，再次发起请求，或者假设问题节点已经挂掉并且继续后续事宜。如果真是问题节点挂掉了，那你可以忽略这个节点，整个集群继续运转。而更差的情况下，问题节点此时仍然运行于孤立的环境中，从该节点的视角来看，其它节点都挂掉了，整个集群只剩自己一个节点。 Erlang集群默认即视不可达的节点为死节点，这是一种悲观的方案，这可以让我们对灾难性故障迅速作出反应。Erlang假设网络故障的概率比硬件或应用故障的概率低，Erlang最初就是这么设计的(为电信通讯平台服务)。另一种乐观的方案(假设失连节点仍然存活)可能或延迟故障恢复相关的处理，因为它假设网络故障的可能性要比硬件或应用故障的可能性更大，因此它会重试等待更长的时间，以便失连节点重返集群。 那么问题来了，在悲观的解决方案中，如果失连节点突然回归集群(比如网络恢复)又会怎么样呢？此时失连节点可能已经运行在孤立的环境数日，有自己的数据，连接，状态等。这个时候想要协调数据和状态的一致性是很困难的，并且你的集群可能已经启动了失连节点的备用节点，而直接忽略节点也不可取，也许该节点已经处理了很多外部请求，甚至向DB写入了数据。总之，会有一些非常奇怪的事情发生。 那么，是否有一个方案可以在节点失连的时候保持应用正常运行，并且不会出现数据丢失或不一致呢？ 三. CAP理论在CAP理论中，关于上一个问题的答案是”没有”，你没有办法让一个分布式系统在网络断开的时候保持正常运行并且正确地运行。CAP理论在Wiki中解释： 一致性（Consistence) (等同于所有节点访问同一份最新的数据副本） 可用性（Availability）（对数据更新具备高可用性） 容忍网络分区（Partition tolerance）（以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择) 根据定理，分布式系统只能满足三项中的两项而不可能满足全部三项。理解CAP理论的最简单方式是想象两个节点分处分区两侧。允许至少一个节点更新状态会导致数据不一致，即丧失了C性质。如果为了保证数据一致性，将分区一侧的节点设置为不可用，那么又丧失了A性质。除非两个节点可以互相通信，才能既保证C又保证A，这又会导致丧失P性质。 关于CAP理论的另一篇很有意思的文章：CAP理论十二年回顾：”规则”变了。 从CAP理论来看，我们只有三种选择: CA, CP, AP，通常情况下，CA是我们无需考虑的，除非你可以保证你的网络不会出现故障，抑或集群是部署在同一台主机上。剩下的CP, AP需要根据你自己的系统进行取舍。 四. 待续尽管在Erlang中构建分布式系统是非常Easy的一件事情，但是分布式系统本身的复杂度，需要你根据系统需求，从多个维度去设计，考量和评估整个分布式系统，同时理解分布式系统的常见误区，注重底层细节。本文内容参考：http://learnyousomeerlang.com/distribunomicon 之后可能详细整理一下Erlang本身在分布式方面提供的具体支持，并且评估一下当前项目用到的服务器集群系统。","tags":[{"name":"erlang","slug":"erlang","permalink":"http://wudaijun.com/tags/erlang/"}]},{"title":"3D贴图基础","date":"2016-02-29T16:00:00.000Z","path":"2016/03/3d-mapping/","text":"漫反射贴图(Diffuse Mapping)漫反射贴图反映出物体表面在漫反射下的颜色和强度，表现出物体的固有色以及纹理。是物体最基础的贴图。通常也可以直接理解为纹理。 高光贴图(Specular Mapping)高光贴图表现物体表面的光照属性，包括镜面反射强度，光泽度，以及菲涅耳衍射强度，决定物体在强光下，表面不同材质(布料，金属，皮肤等)的光照表现。一些高光贴图只包含镜面反射强度信息，每个像素只需要8位，即使用一个通道。 法线贴图(Normal Mapping)法线贴图保存了物体表面每个像素点的法线向量： 这样，将大小为1的光源向量与法线向量相乘，得到的值越接近于0，表面越暗，越接近于1，表面越亮。要保存所有点的法线向量，需要保存[x,y,z]三元组，这正好可以作为RGB值放到一张图片中，因此就有了法线”贴图”，虽然它表现为一张图片，但实际上只是存放向量信息的载体。 有了法线贴图，我们既避免了复杂模型带来的运算和内存占用，又实现了比较细节的光照效果，突出了模型的细节。 视差贴图(Parallax Mapping)视差贴图技术和法线贴图差不多，但它有着不同的原则。和法线贴图一样视差贴图能够极大提升表面细节，使之具有深度感。它根据储存在纹理中的几何信息对顶点进行位移或偏移。但是这种偏移只用于光影表现，不作用于实际模型轮廓。一种实现的方式通过高度贴图(High Mapping)保存纹理中顶点的高度信息。 置换贴图(Displacement Mapping)又叫位移贴图，替换贴图可以通过一种向量贴图的方式来实现，这个向量贴图并不像普通贴图那样改变物体表面的颜色，而是改变物体表面点的位置。它不像法线贴图和视差贴图，因为这些技术都是在制造凹凸效果的假象，而位移映射是真正通过贴图的方式制造出凹凸的表面。它必须要配合切面细分算法，增加渲染的多边形数目来制造出细节的效果。因此它是同类贴图中消耗最大的。 环境贴图(Environment Mapping)又叫反射贴图，把反射对象当作一个虚拟眼睛，生成一张虚拟的纹理图，然后把该纹理图映射到反射对象上，得到的图像就是该场景的一个影像。反射贴图主要实现的功能是：使物体表面能显示出真实场景的影像，而又无需逐个渲染场景中的物体。环境贴图根据反射对象的不同，主要分为三种：球面环境贴图，立方体环境贴图和双曲面环境贴图。 光照贴图(Light Mapping)光照贴图针对于模型周围光照作用于模型效果的一个快照，以避免实时计算光照效果和阴影效果。这通常用在对静态物体(如墙面，箱子)的渲染优化，如Unity中可通过烘焙得到光照贴图。 实例效果漫反射+高光贴图 漫反射+环境贴图 漫反射+法线+环境贴图 更多参考 环境贴图：http://www.twinklingstar.cn/2014/1322/environment-mapping/ Wiki中的各种贴图：https://zh.wikipedia.org/wiki/%E4%BD%8D%E7%A7%BB%E8%B4%B4%E5%9B%BE Learn OpenGL: https://learnopengl-cn.readthedocs.org/zh/latest/ GTA5中的贴图运用：http://www.adriancourreges.com/blog/2015/11/02/gta-v-graphics-study/","tags":[{"name":"unity","slug":"unity","permalink":"http://wudaijun.com/tags/unity/"}]},{"title":"3D动画基础","date":"2016-02-19T16:00:00.000Z","path":"2016/02/3d-animation/","text":"顶点动画3D动画本质上是模型的顶点轨迹，因此要记录一段动画，最原始的办法就是记录动画过程中所有的顶点信息。但由于肉眼的识别速度和GPU的处理能力都有限，因此有了桢(frame)的概念，帧是模型特定姿态的一个快照。为了进一步减少动画桢的内存占用，我们可以从动画的轨迹中提出中关键帧，保存每一个关键帧的模型网格信息，由引擎来得到平滑的动画效果(关键帧过渡)，在关键帧之间平滑过渡的帧叫过渡帧或插值桢。 骨骼动画针对于人/动物等具备骨骼特征的模型动画的一种优化，由于人的皮肤相对于其所属骨骼的相对偏移固定不变，因此皮肤顶点的空间位置可由其骨骼位置加上其相对偏移得到，所以我们无需记录皮肤上每个顶点信息，而记录对应的骨骼信息，这样我们实际得到的是一段简化版的”线段动画”，谓之骨骼动画。 由于骨骼与骨骼通常只有相对角度，没有相对位移，比如手掌不会脱离肘，因此我们无需单独记录每个骨骼的位置信息，只需记录骨骼与其父关节的相对角度，比如手掌的父骨骼为肘，肘的父关节为臂等，这样，我们需要为模型指定一个根骨骼(比如人体的脊椎)，根据根骨骼的位置即可以推算出所有骨骼的位置，以完成正确渲染，为模型顶点绑定其内在骨骼的过程叫做蒙皮(skining)，有些顶点可能需要绑定在多个骨骼上，比如关节处的皮肤。 骨骼动画为动画插值运算，逆向运动学得提供了可行性基础。 关于骨骼动画更多参考：http://www.cnblogs.com/kex1n/archive/2011/10/11/2207546.html 动画过渡在给定的时间内，从一个动画平滑过渡到另一个动画的过程，叫做动画过渡，如在0.2s内，将动画从A切换到B，若动画A当前帧为FA1，动画B起始帧为FB1，我们用F1(W1)+F2(W2)表示一个插值桢，设过渡桢为10桢，那么这里有两种过渡方案： 首尾插值：动画引擎将随时间调整FA1，FB1的权重，对其插值融合，得到插值帧：FA1(1)+FB1(0), FA1(0.9)+FB1(0.1) … FA1(0)+FB1(1)。 交叉插值：动画一边过渡一边插值：FA1(1)+FB1(0), FA2(0.9)+FB2(0.1), … FA11(0)+FB11(1) 动画融合在美术导入的资源中，只有每个模型的基本的动作，比如走，跑，跳等，而真正运动中需要的远不止这些，比如我们还需要快走，慢跑等，这些动画完全通过美术导出是不切实际的。现在的动画系统提供了一种动画融合机制，即由程序去控制多个动画的权重，插值合成新的动画。比如我们可以通过速度去控制走，跑动画的插值权重，得到我们想要的慢跑动画。另一个例子是CS中的持枪动作，美术可能导出了上下左右前后的持枪动作，对于任意角度，我们可能需要用到1-3个动画，对其进行融合得到对应角度的持枪动作，融合权重由当前角度而定，而如果这时候我们还希望模型能够保持慢跑动作，我们还需要合成下半身的动画，在融合时，模型慢跑动画的上半身动画权重为0，下半身动画权重为1，持枪动画权重相反，这样就得到一个灵活的模型，而美术只导入有限的基础动画即可。 动画过渡和动画融合本质上都用于生成平滑的动画，但前者用于两个动画的平滑切换，通过时间决定动画插值权重，而后者可用于多个动画(融合)，并且动画插值权重由程序控制。 动画层前面提到了CS人物模型的动画融合，它需要用到多个动画，对于慢跑动作，其上半身的融合权重通常为0，而对于持枪动作，其下半身的融合权重为0，但是动画系统仍然会去计算这一部分，造成了不必要的GPU/CPU浪费。如今一些动画系统提供了动画层的概念，可以在一个动画控制器里面存在多个动画状态机，并对它们自动融合。每个动画层可以有身体遮罩(Mask)，亦即决定该动画运用于身体的那一部分，禁止哪一部分，被禁止的部分融合权重为0，并且不会参与运算。比如持枪动画层的身体遮罩应禁止下半身。 动画与模型变换动画本身可由美术导入位移(Postion)和转向(Rotation)信息，这些变换是模型相对于自身坐标系的，要将这些变换运用于模型在世界坐标系的变换，有两种方式：由程序控制和由动画控制。由程序控制是指，程序控制模型的Transform，然后将Transform信息设置到动画系统，由动画系统去展现当前动作，比如走，跑等，这也是最早的动画交互方式。如此程序需要事先精确匹配动画的变换信息，否则可能出现滑步等现象。另外一种方式为了解决这类问题的：由动画控制模型变换，程序去动画系统拉取模型的变换信息，这虽然在表现层上体验更好，但也有一些问题：让渲染层决定了逻辑层，如果渲染帧卡了，会影响到逻辑帧处理。 Unity关于这方面有更灵活的控制，参见：http://blog.csdn.net/cubesky/article/details/39478207 逆向运动学IK(Inverse Kinematic，逆向运动学，也称反向运动学)主要用于骨骼动画，前面提到，骨骼动画通过根骨骼(也叫模型的根节点)出发，根据其子节点的相对位置(角度)推算出子节点的绝对位置，从手臂到肘到手掌到手指，整个推算结构呈树形。而如果我们已知末节点的位置，逐步推算出所有其它节点的合理位置的过程即为IK。比如人物模型用手触碰一个固定物品，我们需要根据物品的位置，决定模型的动作。","tags":[{"name":"unity","slug":"unity","permalink":"http://wudaijun.com/tags/unity/"}]},{"title":"瞎扯：Erlang thinking in rails","date":"2016-02-15T16:00:00.000Z","path":"2016/02/erlang-thinking-in-rails/","text":"出于项目历史原因，我们项目按照Rails的方式来构建Erlang服务器，一些重构和思考历程记录于此。 Rails的核心在于MVC： Router: 消息路由，决定消息由哪个Controller处理 Controller: 协议层，协议解析，消息响应 Model: 数据层，操作数据 Model层不能访问Controller层 我们项目使用Fat Model，即将逻辑放在Model层。由此引发一些需要解决的问题： Controller/Model如何交互在Model层操作数据完成后，Controller层需要知晓数据变动，以同步给客户端。主要有以下几种方式： a. Model层通过返回值告诉Controller数据变动，这种方式对简单调用能够胜任，但是对于多层嵌套调用的返回值处理会很麻烦，特别是对一些底层基础模块。调用者需要去组合，合并这些返回值。b. 我们希望Model层封装自己的状态，累计并缓存每次调用所造成的数据变动(delta值)，再由Controller统一地将本次请求涉及到的数据变动同步给客户端。这对于背包这类基础模块来说特别适合，不论我本次请求加减了多少物品，我只需要发一次notify，并且外部的调用接口很干净。缺点是Controller层要知道Model可能影响到哪些模块，然后再刷新同步这些模块的变化。c. 在b的基础上，我们考虑让Model层通知Controller层本次操作所影响到的模块(通过进程字典/返回值)，或者在每次请求处理完成之后，去检查所有模块的状态标记，来实现一个更为合理的通信模型。 Model的自主逻辑处理好问题1之后，我们能满足简单的请求/响应模式。现在来考虑一些更为复杂的逻辑，比如大地图，玩家交互，怪物AI，区块同步，这些都需要在逻辑处理时，实时地向玩家发送Notify，这与请求/响应模式有如下不同： 逻辑自主触发：通过怪物AI，定时器等，需要不定期地向玩家推送消息消息实时同步：比如我们调用行军march函数，我们希望底层自动处理好区块同步，消息广播等，而不是我在外部再触发同步操作 而这，刚好是游戏服务器和Web服务器最大的不同，也是游戏本身的特性。这也导致我们在项目的后续开发中，Model要调用Controller进行消息推送。实践证明，用Fat Model的思维来做游戏无疑是错误的。 Fat Controller？我们或许能够通过Fat Controller在解决上述问题，这时的Model只是一系列的get/set接口(不是DB层)，然而逻辑处理和数据存取本身就是紧耦合的，将它们隔开你会发现仍然写得很不顺畅，一方面由于函数式编程的特性，没有内部状态，想要达成Model层的复用比较困难，另一方面，数据的层级较深时，维护Model层接口本身就是一件难事。那么Model层还有存在必要吗？我能想到的好处，也就只有在数据升级时，有所便利而已了。 Thinking in rails again再看Rails本身的MVC哲学，是基于Web服务器的，而Web服务器是传统的请求-响应模式，没有复杂的自主逻辑和实体交互，只是对数据的增删查改，是非常简单的逻辑。这是不适应于游戏服务器的，用Rails的思路做游戏服务器本身也许就是个误区。","tags":[{"name":"erlang","slug":"erlang","permalink":"http://wudaijun.com/tags/erlang/"}]},{"title":"开发笔记(5) cluster_server集群优化","date":"2016-01-07T16:00:00.000Z","path":"2016/01/erlang-server-design5-server-node/","text":"在使用cluster_server做gameserver后台集群支撑的过程中，逐步暴露出一些问题，在此记之。 1. 节点交互效率低下我们按照业务职责将集群节点分为，agent，player，map，alliance，battle等，这些节点可以在一台物理机上(操作系统进程间交互)，也有可能部署到不同的物理机上(网络IO交互)，而玩家一个业务逻辑可能涉及到多个节点，形成一个节点交互链：如玩家在大地图上进行一场战斗，按照流程需要走：agent -&gt; player -&gt; map -&gt; battle -&gt; map -&gt; player -&gt; agent。整个过程都是异步的。这种交互是频繁的且低效的。 反思cluster_server最初的设计：根据业务职责来划分节点，这样做主要优势在于容错(灾)性和负载均衡，在节点调试和调优方面也很方便。但是对于业务逻辑来说，玩家的数据被分散到各个节点，这些节点之间异步交互(我们对call是敏感的)，造成了数据同步和逻辑交互都变得复杂，这种复杂度随节点数量和逻辑复杂程度(涉及到的节点数)还会不断上升。 针对于这些问题，我们首先对集群进行了重整，将同一个server_id下的player，map，pvp，alliance等轻量的进程放在一起，由server进程统一管理，并挂载server_node之上。这样整个集群的节点只剩三个：agent，server，battle，满足了数据局部性原则，业务逻辑交互也更高效(Erlang进程之间)。mnesia集群表仍然保留，用于执行快速的消息路由和进程查找。 2. 玩家数据同步这一直是分布式系统中的一个大问题，玩家的数据被分散在各节点，不同的节点还需要玩家一些基础信息的副本，这部分需要同步的数据很难管理。主要有以下几种方案： 玩家跨节点时，将本次处理所需要的信息带上。这种方案只能运用于玩家本身的数据同步。而玩家可能或查看或拉取其它玩家的数据 各节点存有玩家部分数据的副本，玩家对应数据更新时，player通过cast消息到相关节点进行数据更新。这适用于简单的数据同步，如玩家name，level等，复杂的数据，比如玩家的英雄信息，会导致通信量过大 各节点在需要实时玩家信息时，去player身上拉取。这是不可取的：第一，不能call player 第二，目标player进程不一定还在，特别是流失玩家 做一个Cache中心，并且满足单写多读的原则。比如map节点改变了玩家数据，那么它应该将数据变动发给player节点，由player节点来完成数据更新并更新Cache 在重构集群之前，我们用的是方案2，主要原因是数据简单，更新实时。要在跨节点同步中，使用Cache中心，可以直接通过mnesia实现，但是这是软实时，对于一些实时性很高的数据，需要使用事务。但在建立了server_node之后，就简单很多了，通过ets即可实现，但仍然要满足只有一个写入者的原则。 3. 节点交互链过长可以在agent消息路由处，将map相关消息直接路由到map，而map业务处理完成之后，如果不需要在玩家身上增减资源，则可以直接将Ack发到玩家对应的agent上，流程得以简化。 总结目前服务器的分布式特性： 按照ServerId组成一颗大的监督树，一个Server及其所有包含模块，均作为整体部署在一个server_node上。这样Server内部的逻辑与数据得以聚合。而负载均衡将以Server为基本单位 在节点内部，为了方便交互，仍然使用Mnesia作服务注册与发现，但由于Server之间没有隔离，要求服务ID全局唯一 对于单服管理，可能需要一个进程维护一个Server的同类服务(player, alliance)，以进行LRU，全服广播等服务器逻辑 可选的优化方案： 服务隔离：对各Server之间的服务进行隔离。隔离的好处：一是减轻Mnesia单表压力，二是对Server逻辑有更好的支撑：如全服通知，停服操作。但就Mnesia来说，我目前没有找到好的隔离方案 本地服务：一些服务是否可以优化至单节点读写，这样Mnesia可退化为ETS，换来更好读取速度，并且极大减轻Mnesia压力 控制节点数量：删除不必要的Mnesia节点，通过消息而不是Mnesia来同步服务","tags":[{"name":"erlang","slug":"erlang","permalink":"http://wudaijun.com/tags/erlang/"},{"name":"distribution","slug":"distribution","permalink":"http://wudaijun.com/tags/distribution/"}]},{"title":"Erlang 常用数据结构实现","date":"2015-12-05T16:00:00.000Z","path":"2015/12/erlang-datastructures/","text":"简单介绍一下Erlang常用数据结构的内部实现和特性，主要参考Erlang OTP 18.0源码，和网上很多优秀博客(参见附录)，整理了一些自己项目中常用到的。 Erlang虚拟机使用一个字(64/32位)来表示所有类型的数据，即Eterm。具体的实施方案通过占用Eterm的后几位作为类型标签，然后根据标签类型来解释剩余位的用途。这个标签是多层级的，最外层占用两位，有三种类型： 01: list，剩下62位是指向列表Cons的指针 10: boxed对象，即复杂对象，剩余62位指向boxed对象的对象头。包括元组，大整数，外部Pid/Port等 11: immediate立即数，即可以在一个字中表示的小型对象，包括小整数，本地Pid/Port，Atom，NIL等 这三种类型是Erlang类型的大框架，前两者是可以看做是引用类型，立即数相当于是值类型，但无论对于哪种类型，Erlang Eterm本身只占用一个字，理解这一点是很重要的。 对于二三级标签的细分和编码，一般我们无需知道这些具体的底层细节，以下是几种常用的数据结构实现方式。 一. 常用类型1. atomatom用立即数表示，在Eterm中保存的是atom在全局atom表中的索引，依赖于高效的哈希和索引表，Erlang的atom比较和匹配像整数一样高效。atom表是不回收的，并且默认最大值为1024*1024，超过这个限制Erlang虚拟机将会崩溃，可通过+t参数调整该上限。 2.Pid/Port/* erts/emulator/beam/erl_term.h * * Old pid layout(R9B及之前): * * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ * |s s s|n n n n n n n n n n n n n n n|N N N N N N N N|c c|0 0|1 1| * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ * * s : serial 每次n到达2^15之后 自增一次 然后n重新从低位开始 * n : number 15位, 进程在本地进程表中的索引 * c : creation 每次节点重启，该位自增一次 * N : node number 节点名字在atom表中索引 * * * PID layout (internal pids): * * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ * |n n n n n n n n n n n n n n n n n n n n n n n n n n n n|0 0|1 1| * +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ * * n : number 28位进程Pid */ 在Old Pid表示中(R9B及之前版本)，在32位中表示了整个Pid，包括其节点名字等信息，也就是本地进程和外部进程都可以用Eterm立即数表示，显示格式为&lt;N, n, s&gt;。 在R9B之后，随着进程数量增加和其它因素，Pid只在32位中表示本地Pid(A=0)，将32位中除了4位Tag之外的28位，都可用于进程Pid表示，出于Pid表示的历史原因，仍然保留三段式的显示，本地Pid表示变成了&lt;0, Pid低15位, Pid高13位&gt;。对于外部Pid，采用boxed复合对象表示，在将本地Pid发往其它node时，Erlang会自动将为Pid加上本地节点信息，并打包为一个boxed对象，占用6个字。另外，Erlang需要维护Pid表，每个条目占8个字节，当进程数量过大时，Pid表将占用大量内存，Erlang默认可以使用18位有效位来表示Pid(262144)，可通过+P参数调节，最大值为27位(2^27-1)，此时Pid表占用内存为2G。 12345678910111213Eshell V8.1 (abort with ^G)(n1@T4F-MBP-11)1&gt; node().'n1@T4F-MBP-11'% 节点名的二进制表示(n1@T4F-MBP-11)2&gt; term_to_binary(node()).&lt;&lt;131,100,0,13,110,49,64,84,52,70,45,77,66,80,45,49,49&gt;&gt;(n1@T4F-MBP-11)3&gt; self().&lt;0.63.0&gt;% term_to_binary会将A对应的节点名编码进去(n1@T4F-MBP-11)4&gt; term_to_binary(self()).&lt;&lt;131,103,100,0,13,110,49,64,84,52,70,45,77,66,80,45,49, 49,0,0,0,63,0,0,0,0,2&gt;&gt;(n1@T4F-MBP-11)5&gt; 3. lists列表以标签01标识，剩余62位指向列表的Cons单元，Cons是[Head|Tail]的组合，在内存中体现为两个相邻的Eterm，Head可以是任何类型的Eterm，Tail是列表类型的Eterm。因此形如L2 = [Elem|L1]的操作，实际上构造了一个新的Cons，其中Head是Elem Eterm，Tail是L1 Eterm，然后将L2的Eterm指向了这个新的Cons，因此L2即代表了这个新的列表。对于[Elem|L2] = L1，实际上是提出了L1 Eterm指向的Cons，将Head部分赋给Elem，Tail部分赋给L2，注意Tail本身就是个List的Eterm，因此list是单向列表，并且构造和提取操作是很高效的。需要再次注意的是，Erlang所有类型的Eterm本身只占用一个字大小。这也是诸如list,tuple能够容纳任意类型的基础。 Erlang中进程内对对象的重复引用只需占用一份对象内存(只是Eterm本身一个字的拷贝)，但是在对象跨进程时，对象会被展开，执行速深度拷贝： 12345678910111213141516Eshell V7.0.2 (abort with ^G)1&gt; L1 = [1,2,3].[1,2,3]2&gt; erts_debug:size(L1). 63&gt; L2 = [L1,L1,L1].[[1,2,3],[1,2,3],[1,2,3]]4&gt; erts_debug:size(L2). % 获得L2对象树的大小 3*2+6125&gt; erts_debug:flat_size(L2). % 获得对象平坦展开后的大小 3*(2+6)246&gt; P1 = spawn(fun() -&gt; receive L -&gt; io:format(\"~p~n\",[erts_debug:size(L)]) end end).&lt;0.45.0&gt;7&gt; P1 ! L2. % 在跨进程时，对象被展开 执行深度拷贝24[[1,2,3],[1,2,3],[1,2,3]] 此时L1, L2的内存布局如下： 4. tupletuple属于boxed对象的一种，每个boxed对象都有一个对象头(header)，boxed Eterm即指向这个header，这个header里面包含具体的boxed对象类型，如tuple的header末6位为000000，前面的位数为tuple的size： tuple实际上就是一个有头部的数组，其包含的Eterm在内存中紧凑排列，tuple的操作效率和数组是一致的。 list和tuple是erlang中用得最多的数据结构，也是其它一些数据结构的基础，如record，map，摘下几个关于list，tuple操作的常用函数，便于加深对结构的理解：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// 位于 $OTP_SRC/erts/emulator/beam/bif.cBIF_RETTYPE tuple_to_list_1(BIF_ALIST_1)&#123; Uint n; Eterm *tupleptr; Eterm list = NIL; Eterm* hp; if (is_not_tuple(BIF_ARG_1)) &#123; BIF_ERROR(BIF_P, BADARG); &#125; // 得到tuple Eterm所指向的tuple对象头 tupleptr = tuple_val(BIF_ARG_1); // 得到对象头中的tuple size n = arityval(*tupleptr); hp = HAlloc(BIF_P, 2 * n); tupleptr++; // 倒序遍历 因为list CONS的构造是倒序的 while(n--) &#123; // 相当于hp[0]=tupleptr[n]; hp[1] = list; list = make_list(hp); // 最后返回的是指向hp的list Eterm list = CONS(hp, tupleptr[n], list); hp += 2; &#125; BIF_RET(list);&#125; BIF_RETTYPE list_to_tuple_1(BIF_ALIST_1)&#123; Eterm list = BIF_ARG_1; Eterm* cons; Eterm res; Eterm* hp; int len; if ((len = erts_list_length(list)) &lt; 0 || len &gt; ERTS_MAX_TUPLE_SIZE) &#123; BIF_ERROR(BIF_P, BADARG); &#125; // 元素个数 + 对象头 hp = HAlloc(BIF_P, len+1); res = make_tuple(hp); *hp++ = make_arityval(len); while(is_list(list)) &#123; cons = list_val(list); *hp++ = CAR(cons); list = CDR(cons); &#125; BIF_RET(res);&#125; 可以看到，list，tuple中添加元素，实际上都是在拷贝Eterm本身，Erlang虚拟机会追踪这些引用，并负责垃圾回收。 5. binaryErlang binary用于处理字节块，Erlang其它的数据结构(list,tuple,record)都是以Eterm为单位的，用于处理字节块会浪费大量内存，如”abc”占用了7个字(加上ETerm本身)，binary为字节流提供一种操作高效，占用空间少的解决方案。 之前我们介绍的数据结构都存放在Erlang进程堆上，进程内部可以使用对象引用，在对象跨进程传输时，会执行对象拷贝。为了避免大binary跨进程传输时的拷贝开销，Erlang针对binary作出了优化，将binary分为小binary和大binary。 heap binary小于64字节(定义于erl_binary.h ERL_ONHEAP_BIN_LIMIT宏)的小binary直接创建在进程堆上，称为heap binary，heap binary是一个boxed对象： 12345typedef struct erl_heap_bin &#123; Eterm thing_word; /* Subtag HEAP_BINARY_SUBTAG. */ Uint size; /* Binary size in bytes. */ Eterm data[1]; /* The data in the binary. */&#125; ErlHeapBin; refc binary大于64字节的binary将创建在Erlang虚拟机全局堆上，称为refc binary(reference-counted binary)，可被所有Erlang进程共享，这样跨进程传输只需传输引用即可，虚拟机会对binary本身进行引用计数追踪，以便GC。refc binary需要两个部分来描述，位于全局堆的refc binary数据本身和位于进程堆的binary引用(称作proc binary)，这两种数据结构定义于global.h中。下图描述refc binary和proc binary的关系： 所有的OffHeap(进程堆之外的数据)被组织为一个单向链表，进程控制块(erl_process.h struct process)中的off_heap字段维护链表头和所有OffHeap对象的总大小，当这个大小超过虚拟机阀值时，将导致一次强制GC。注意，refc binary只是OffHeap对象的一种，以后可扩展其它种类。 sub binarysub binary是Erlang为了优化binary分割的(如split_binary/2)，由于Erlang变量不可变语义，拷贝分割的binary是效率比较底下的做法，Erlang通过sub binary来复用原有binary。ErlSubBin定义于erl_binary.h，下图描述split_binary(ProBin, size1)返回一个ErlSubBin二元组的过程： ProBin的size可能小于refc binary的size，如上图中的size3，这是因为refc binary通常会通过预分配空间的方式进行优化。 要注意的是，sub binary只引用proc binary(通过orig)，而不直接引用refc binary，因此图中refc binary的refc字段仍然为1。只要sub binary还有效，对应的proc binary便不会被GC，refc binary的计数也就不为0。 bit string当我们通过如&lt;&lt;2:3,3:6&gt;&gt;的位语法构建binary时，将得到&lt;&lt;65,1:1&gt;&gt;这种非字节对齐的数据，即二进制流，在Erlang中被称为bitstring，Erlang的bitstring基于ErlSubBin结构实现，此时bitsize为最后一个字节的有效位数，size为有效字节数(不包括未填满的最后一个字节)，对虚拟机底层来说，sub bianry和bit string是同一种数据结构。 binary追加构造优化在通过C = &lt;&lt;A/binary,B/binary&gt;&gt;追加构造binary时，最自然的做法应当是创建足够空间的C(heap or refc)，再将A和B的数据拷贝进去，但Erlang对binary的优化不止于此，它使用refc binary的预留空间，通过追加的方式提高大binary和频繁追加的效率。 12345678910111213141516171819202122232425262728Bin0 = &lt;&lt;0&gt;&gt;, %% 创建一个heap binary Bin0Bin1 = &lt;&lt;Bin0/binary,1,2,3&gt;&gt;, %% 追加目标不是refc binary，创建一个refc binary，预留256字节空间，用Bin0初始化，并追加1,2,3Bin2 = &lt;&lt;Bin1/binary,4,5,6&gt;&gt;, %% 追加目标为refc binary且有预留空间 直接追加4,5,6Bin3 = &lt;&lt;Bin2/binary,7,8,9&gt;&gt;, %% 同样，将7,8,9追加refc binary预留空间Bin4 = &lt;&lt;Bin1/binary,17&gt;&gt;, %% 此时不能直接追加，否则会覆盖Bin2内容，虚拟机会通过某种机制发现这一点，然后将Bin1拷贝到新的refc binary，再执行追加&#123;Bin4,Bin3&#125; % 通过erts_get_internal_state/1可以获取binary状态% 对应函数源码位于$BEAM_SRC/erl_bif_info.c erts_debug_get_internal_state_1f() -&gt; B0 = &lt;&lt;0&gt;&gt;, erts_debug:set_internal_state(available_internal_state,true), % 打开内部状态获取接口 同一个进程只需执行一次 f2(B0). % 通过参数传递B0 是为了避免虚拟机优化 直接构造B1为heap binaryf2(B0) -&gt; io:format(\"B0: ~p~n\", [erts_debug:get_internal_state(&#123;binary_info,B0&#125;)]), B1 = &lt;&lt;B0/binary, 1,2,3&gt;&gt;, io:format(\"B1: ~p~n\", [erts_debug:get_internal_state(&#123;binary_info,B1&#125;)]), B2 = &lt;&lt;B1/binary, 4,5,6&gt;&gt;, io:format(\"B2: ~p~n\", [erts_debug:get_internal_state(&#123;binary_info,B2&#125;)]), ok. % get_internal_state(&#123;binary_info, B&#125;)返回格式:% proc binary：&#123;refc_binary, pb_size, &#123;binary, orig_size&#125;, pb_flags&#125;% heap binary：heap_binaryB0: heap_binaryB1: &#123;refc_binary,4,&#123;binary,256&#125;,3&#125;B2: &#123;refc_binary,7,&#123;binary,256&#125;,3&#125; binary追加实现源码位于$BEAM_SRC/erl_bits.c erts_bs_append，B1和B2本身是sub binary，基于同一个ProcBin，可追加的refc binary只能被一个ProcBin引用，这是因为可追加refc binary可能会在追加过程中重新分配空间，此时要更新ProcBin引用，而refc binary无法快速追踪到其所有ProcBin引用(只能遍历)，另外，多个ProcBin上的sub binary可能对refc binary覆写。 只有最后追加得到的sub binary才可执行快速追加(通过sub binary和对应ProBin flags来判定)，否则会拷贝并分配新的可追加refc binary。所有的sub binary都是指向ProcBin或heap binary的，不会指向sub binary本身。 binary降级Erlang通过追加优化构造出的可追加refc binary通过空间换取了效率，并且这类refc binary只能被一个proc binary引用(多个proc binary上的sub binary会造成覆写，注意，前面的B1，B2是sub binary而不是ProBin)。比如在跨进程传输时，原本只需拷贝ProBin，但对可追加的refc binary来说，不能直接拷贝ProBin，这时需对binary降级，即将可追加refc binary降级为普通refc binary： bs_emasculate(Bin0) -&gt; Bin1 = &lt;&lt;Bin0/binary, 1, 2, 3&gt;&gt;, NewP = spawn(fun() -&gt; receive _ -&gt; ok end end), io:format(&quot;Bin1 info: ~p~n&quot;, [erts_debug:get_internal_state({binary_info, Bin1})]), NewP ! Bin1, io:format(&quot;Bin1 info: ~p~n&quot;, [erts_debug:get_internal_state({binary_info, Bin1})]), Bin2 = &lt;&lt;Bin1/binary, 4, 5, 6&gt;&gt;, % Bin1被收缩 这一步会执行refc binary拷贝 io:format(&quot;Bin2 info: ~p~n&quot;, [erts_debug:get_internal_state({binary_info, Bin2})]), Bin2. % 运行结果 117&gt; bs_emasculate(&lt;&lt;0&gt;&gt;). Bin1 info: {refc_binary,4,{binary,256},3} Bin1 info: {refc_binary,4,{binary,4},0} Bin2 info: {refc_binary,7,{binary,256},3} &lt;&lt;0,1,2,3,4,5,6&gt;&gt; 降级操作会重新创建一个普通的refc binary(原有可追加refc binary会被GC?)，同时，降级操作会将B1的flags置0，这保证基于B1的sub binary在执行追加时，会重新拷贝分配refc binary。 // 降级函数($BEAM_SRC/erl_bits.c) void erts_emasculate_writable_binary(ProcBin* pb) { Binary* binp; Uint unused; pb-&gt;flags = 0; binp = pb-&gt;val; ASSERT(binp-&gt;orig_size &gt;= pb-&gt;size); unused = binp-&gt;orig_size - pb-&gt;size; /* Our allocators are 8 byte aligned, i.e., shrinking with less than 8 bytes will have no real effect */ if (unused &gt;= 8) { // 根据ProBin中的有效字节数，重新创建一个不可追加的refc binary binp = erts_bin_realloc(binp, pb-&gt;size); pb-&gt;val = binp; pb-&gt;bytes = (byte *) binp-&gt;orig_bytes; } } Q: ProcBin B1的字段被更新了，那么Erlang上层如何维护变量不可变语义? A: 变量不可变指的是Erlang虚拟机上层通过底层屏蔽后所能看到的不变语义，而不是变量底层实现，诸如Pid打包，maps hash扩展等，通过底层差异化处理后，对上层体现的语义和接口都没变，因此我们将其理解为”变量不可变”)。 另外，全局堆GC也可能会对可追加refc binary的预留空间进行收缩(shrink)，可参考$BEAM_SRC/erl_gc.c sweep_off_heap函数。 以上都是理论的实现，实际上Erlang虚拟机对二进制还做了一些基于上下文的优化，通过bin_opt_info编译选项可以打印出这些优化。关于binary优化的更多细节，参考Constructing and Matching Binaries。 二. 复合类型基于list和tuple之上，Erlang还提供了一些其它的数据结构，这里列举几个key/value相关的数据结构，在服务器中会经常用到。 1. record这个类型无需过多介绍，它就是一个tuple，所谓record filed在预编译后实际上都是通过数值下标来索引，因此它访问field是O(1)复杂度的。 2. map虽然record的语法糖让我们在使用tuple时便利了不少，但是比起真正的key/value结构仍然有许多限制，如key只能是原子，key不能动态添加或删除，record变动对热更的支持很差等。proplists能够一定程度地解决这种问题，但是它适合键值少的情况，通常用来做选项配置，并且不能保证key的唯一。 map是OTP 17引进的数据结构，是一个boxed对象，它支持任意类型的Key，模式匹配，动态增删Key等，并且最新的mongodb-erlang直接支持map。 在OTP17中，map的内存结构为： 123456//位于 $OTP_SRC/erts/emulator/beam/erl_map.htypedef struct map_s &#123; Eterm thing_word; // boxed对象header Uint size; // map 键值对个数 Eterm keys; // keys的tuple&#125; map_t; 该结构体之后就是依次存放的Value，因此maps的get操作，需要先遍历keys tuple，找到key所在下标，然后在value中取出该下标偏移对应的值。因此是O(n)复杂度的。详见maps:get源码($BEAM_SRC/erl_map.c erts_maps_get)。 如此的maps，只能作为record的替用，并不是真正的Key-&gt;Value映射，因此不能存放大量数据。而在OTP18中，maps加入了针对于big map的hash机制，当maps:size &lt; MAP_SMALL_MAP_LIMIT时，使用flatmap结构，也就是上述OTP17中的结构，当maps:size &gt;= MAP_SMALL_MAP_LIMIT时，将自动使用hashmap结构来高效存取数据。MAP_SMALL_MAP_LIMIT在erl_map.h中默认定义为32。 仍然要注意Erlang本身的变量不可变原则，每次执行更新maps，都会导致新开辟一个maps，并且拷贝原maps的keys和values，在这一点上，maps:update比maps:put更高效，因为前者keys数量不会变，因此无需开辟新的keys tuple，拷贝keys tuples ETerm即可。实际使用maps时： 更新已有key值时，使用update(:=)而不是put(=&gt;)，不仅可以检错，并且效率更高 当key/value对太多时，对其进行层级划分，保证其拷贝效率 实际测试中，OTP18中的maps在存取大量数据时，效率还是比较高的，这里有一份maps和dict的简单测试函数，可通过OTP17和OTP18分别运行来查看效率区别。通常情况下，我们应当优先使用maps，比起dict，它在模式匹配，mongodb支持，可读性上都有很大优势。 3. arrayErlang有个叫array的结构，其名字容易给人误解，它有如下特性： array下标从0开始 array有两种模式，一种固定大小，另一种按需自动增长大小，但不会自动收缩 支持稀疏存储，执行array:set(100,value,array:new())，那么[0,99]都会被设置为默认值(undefined)，该默认值可修改。 在实现上，array最外层被包装为一个record: 123456-record(array, &#123; size :: non_neg_integer(), %% number of defined entries max :: non_neg_integer(), %% maximum number of entries default, %% the default value (usually 'undefined') elements :: elements(_) %% the tuple tree&#125;). elements是一个tuple tree，即用tuple包含tuple的方式组成的树，叶子节点就是元素值，元素默认以10个为一组，亦即完全展开的情况下，是一颗十叉树。但是对于没有赋值的节点，array用其叶子节点数量代替，并不展开： 123456789101112131415161718192021222324Eshell V7.0.2 (abort with ^G)1&gt; array:set(9,value,array:new()).&#123;array,10,10,undefined, % 全部展开 &#123;undefined,undefined,undefined,undefined,undefined,undefined,undefined,undefined,undefined,value&#125;&#125; % 只展开了19所在的子树 其它9个节点未展开 % 注意tuple一共有11个元素，最后一个元素代表本层节点的基数，这主要是出于效率考虑，能够快速检索到元素所在子节点2&gt; array:set(19,value,array:new()).&#123;array,20,100,undefined, &#123;10, &#123;undefined,undefined,undefined,undefined,undefined， undefined,undefined,undefined,undefined,value&#125;, 10,10,10,10,10,10,10,10,10&#125;&#125; % 逐级展开了199所在的子树3&gt; array:set(199,value,array:new()).&#123;array,200,1000,undefined, &#123;100, &#123;10,10,10,10,10,10,10,10,10, &#123;undefined,undefined,undefined,undefined,undefined, undefined,undefined,undefined,undefined,value&#125;, 10&#125;, 100,100,100,100,100,100,100,100,100&#125;&#125;4&gt; 由于完全展开的tuple tree是一颗完全十叉树，因此实际上array的自动扩容也是以10为基数的。在根据Index查找元素时，通过div/rem逐级算出Index所属节点: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051%% 位于$OTP_SRC/lib/stdlib/src/array.erlget(I, #array&#123;size = N, max = M, elements = E, default = D&#125;) when is_integer(I), I &gt;= 0 -&gt; if I &lt; N -&gt; % 有效下标 get_1(I, E, D); M &gt; 0 -&gt; % I&gt;=N 并且 array处于自动扩容模式 直接返回DefaultValue D; true -&gt; % I&gt;=N 并且 array为固定大小 返回badarg erlang:error(badarg) end;get(_I, _A) -&gt; erlang:error(badarg). %% The use of NODEPATTERN(S) to select the right clause is just a hack,%% but it is the only way to get the maximum speed out of this loop%% (using the Beam compiler in OTP 11). % -define(NODEPATTERN(S), &#123;_,_,_,_,_,_,_,_,_,_,S&#125;). % NODESIZE+1 elements!get_1(I, E=?NODEPATTERN(S), D) -&gt; % 到达已展开的中间节点 向下递归 get_1(I rem S, element(I div S + 1, E), D);get_1(_I, E, D) when is_integer(E) -&gt; % 到达未展开的中间节点 返回默认值 D;get_1(I, E, _D) -&gt; % 到达叶子节点层 element(I+1, E).set(I, Value, #array&#123;size = N, max = M, default = D, elements = E&#125;=A) when is_integer(I), I &gt;= 0 -&gt; if I &lt; N -&gt; A#array&#123;elements = set_1(I, E, Value, D)&#125;; I &lt; M -&gt; % 更新size, size的主要作用是让读取更加高效 %% (note that this cannot happen if M == 0, since N &gt;= 0) A#array&#123;size = I+1, elements = set_1(I, E, Value, D)&#125;; M &gt; 0 -&gt; % 自动扩容 &#123;E1, M1&#125; = grow(I, E, M), A#array&#123;size = I+1, max = M1, elements = set_1(I, E1, Value, D)&#125;; true -&gt; erlang:error(badarg) end;set(_I, _V, _A) -&gt; erlang:error(badarg). %% See get_1/3 for details about switching and the NODEPATTERN macro. set_1(I, E=?NODEPATTERN(S), X, D) -&gt; % 所在节点已展开，向下递归 I1 = I div S + 1, setelement(I1, E, set_1(I rem S, element(I1, E), X, D));set_1(I, E, X, D) when is_integer(E) -&gt; % 所在节点未被展开，递归展开节点 并赋值 expand(I, E, X, D);set_1(I, E, X, _D) -&gt; % 到达叶子节点 setelement(I+1, E, X). 更多细节可以参见源码，了解了这些之后，再来看看Erlang array和其它语言数组不一样的地方： 索引不是O(1)复杂度，而是O(log10n) array并不自动收缩 array中的max和size字段，和array具体占用内存没多大关系(节点默认未展开) array中并没有subarray之类的操作，因为它根本不是线性存储的，而是树形的，因此如果用它来做递归倒序遍历之类的操作，复杂度不是O(n)，而是O(n*log10n) array中对于没有赋值的元素，给予默认值undefined，这个默认值可以在array:new()中更改，对使用者来说，明确赋值undefined和默认值undefined并无多大区别，但对array内部来说，可能会导致节点展开。 三. 参考 Erlang数据结构实现文章汇总: http://www.iroowe.com/erlang_eterm_implementation/ [zhengsyao] Erlang系列精品博客(文中大部分图片出处): http://www.cnblogs.com/zhengsyao/category/387871.html [坚强2002] Erlang array: http://www.cnblogs.com/me-sa/archive/2012/06/14/erlang-array.html Erlang Effciency Guide: http://erlang.org/doc/efficiency_guide/introduction.html","tags":[{"name":"erlang","slug":"erlang","permalink":"http://wudaijun.com/tags/erlang/"}]},{"title":"Erlang Map映射到Record","date":"2015-11-19T16:00:00.000Z","path":"2015/11/erlang-map2record/","text":"在游戏服务器中，通常要面临对象到模型的映射，以及对象到协议的映射，前者用于GS和DB交互，后者用于GS和Client交互。我们的项目中做到了对象到模型的自动映射，这样在开发过程中无需关心GS和DB的交互，很方便。 而现在我们还没有实现对象(map)到协议(record)的自动映射，我觉得这个特性是比较有用的，特别是在同步一些实体数据的时候。无需写一堆Packer函数来将对象数据打包为协议。因此就研究了一下如何将map的数据自动映射到protobuffer，也就是转换为record。 我希望实现一个接口： 123456789% RecordName: type:atom, 协议名字 如hero% MapData: type:map, 对象数据% RecordData: type:tuple, 被转换后的协议包map2record(RecodName, MapData) -&gt; RecordData.如：&gt; rd(hero, &#123;id, level, star&#125;).&gt; HeroMap = #&#123;id =&gt; 1, level =&gt; 2, star =&gt; 3&#125;.&gt; &#123;hero, 1, 2, 3&#125; = map2record(hero, HeroMap). 在实际使用中，还应该考虑到protobuffer中的嵌套结构，map2record应该能够实现嵌套结构，repeated字段的自动解析。 实现1. 识别record由于record类型在erlang运行时并不存在，因此我们无法判断一个原子是否是record，也无法获取它的字段。因此需要实现is_record/1和record_fields/1接口。 在网上找到这篇博客为此提供了一个很好的解决方案。它通过erlang epp模块对record定义进行语义级的解析，并且手动生成我们所需要的函数。我只需要其中的record_fields接口，并对is_record接口进行了一些修改，让其判断一个原子是否是一个record名字，而不是判断一个数据是不是record类型。 2. 填充record填充比较简单，参见代码： 123456789101112131415161718192021&#123;% codeblock lang:erlang %&#125;-module(map2record).-export([auto_transfer/2]).auto_transfer(RecordName, MapData) -&gt; case &#123;myhead_util:is_record(RecordName), is_list(MapData)&#125; of &#123;true, true&#125; -&gt; lists:map(fun(SubData) -&gt; auto_transfer(RecordName, SubData) end, MapData); &#123;true, false&#125; -&gt; Fields = myhead_util:fields_info(RecordName), Values = lists:map(fun(Field) -&gt; auto_transfer(Field, maps:get(Field, MapData, undefined)) end, Fields), list_to_tuple([RecordName|Values]); &#123;false, _&#125; -&gt; MapData end.&#123;% end codeblock %&#125; 整个填充需要满足一些条件， record中的field名字要和map中的key一致 repeated字段，在map中的值，也应该是个list 对于嵌套record，字段名应该为被嵌套的record名字 举个例子： 12345678910111213// 协议文件message hero&#123; required int32 id = 1; required hero_base hero_base = 2; repeated hero_skill hero_skill = 3;&#125;// 那么内测中的HeroMap应该是这样：#&#123; id =&gt; 1, hero_base =&gt; #&#123; ... &#125; hero_skill =&gt; [#&#123;...&#125;, #&#123;...&#125;]&#125; 结语完整代码参见Github: https://github.com/wudaijun/erl_utils/tree/master/map2record 由于我们项目中大部分列表型实体都被组织成了譬如skill_id =&gt; SkillData的map，因此在项目中并没有采用这份方案，也不知具体实践会遇到什么问题。暂时只当个map2record的工具吧。","tags":[{"name":"erlang","slug":"erlang","permalink":"http://wudaijun.com/tags/erlang/"}]},{"title":"开发笔记(4) SLG 地图","date":"2015-11-12T16:00:00.000Z","path":"2015/11/erlang-server-design4-slg-map/","text":"数据组织我们将地图上的数据分为动态数据(行军)和静态数据(点数据)，因为行军过程不会对地图上的点造成影响(飞行)，并且不考虑实时碰撞，因此行军的逻辑是非常轻的，将其剥离出地图点数据，处理起来会更简单。也就是说，行军是地图上特殊的一类事件，而不是动态的点。 数据同步主动拉取主动拉取是指玩家在拖动屏幕时，能够看到地图上实时的点信息和行军信息。 静态数据：最简单的处理，将玩家屏幕的中心点周围一定范围(&gt;=玩家屏幕大小)的点信息，发送给客户端，但这样有一个问题，玩家稍微移动屏幕(中心点变动)，都会导致频繁的请求和数据包，因此我们需要一定程度的”缓冲和限制”来避免频繁的请求。在这个基础上，我们在地图上建立了块(Block)这个概念，一个Block大概可以看做玩家屏幕大小的一个区块，玩家在拖动屏幕中，只有屏幕中心点跨Block了，才发送拉取数据请求，而每次拉取，服务器会将以该Block为中心的九宫格Blocks都发送给客户端，用于客户端平滑过渡。 动态数据：目前只考虑行军，在行军建立时，算出行军所经历的所有Block，将行军这个事件挂在这些Block上，当玩家拉取该Block的数据时，取出这些行军信息，发送客户端。客户端根据行军上面的时间戳等信息，推算出行军当前位置。 实时推送实时推送，是指玩家在屏幕上不进行任何操作，也能看到屏幕所在范围地图上发生的实时信息。这是通过视口(ViewPort)来实现的，视口是一个抽象的概念，可以看做玩家屏幕，玩家视口挂在哪个Block上，玩家就能实时收到该Block上的变化通知，默认情况下，玩家的视口挂载在主城所在的位置，随着玩家屏幕拖送，玩家的视口也会挂载在不同Block上。在服务器上，视口可以简单表示为PlayerId。 当一个点数据变动时，底层会自动通知它所在Block上所有的玩家(视口) 新建行军时，底层会实时通知相关Block，并且将该事件ID挂在Block事件队列上。当有新视口挂载在这些Block上时，将得到通知 行军结束后，通知相关Block，并且将事件ID从相关Block事件队列上移除 当玩家离开地图后，清除玩家视口信息 行军设计一个典型的场景是，玩家选定一支队伍，然后派出行军，采矿/战斗/XXX，回城。在这上面，最开始我们的设计是，行军是队伍的一种状态，是队伍信息的一部分，行军抵达，只是队伍状态由行军转换为了采矿/战斗，这样实际上行军和队伍是一个东西。这样后来在分离出其它玩家/NPC的行军信息时就会很困难。 最后我们将行军单独抽了出来，行军可以承载任何类型的可移动单位，行军和可移动单位相互索引，这样行军本身只携带很少量的信息，并且到达目的地之后就会被删除。 模块设计两个模块分别用于管理点和行军，外加共用底层模块来管理Block并支撑数据同步。Block模块需要维护三个映射: Player2Blocks，Block2Players，Block2Events，显然，在这套机制上，一个玩家是可以有多个视口的。 由于将数据同步机制做在了底层，因此优化的空间是比较大的。对于操作地图点的API，可以给出一个sync选项，用于标明是否需要底层自动通知，这样上层在操作多个点时，可以在修改完之后一并通知。对于事件信息，可以优先将消息发送给事件相关的玩家(如行军所属玩家和行军的目标玩家)等。","tags":[{"name":"erlang","slug":"erlang","permalink":"http://wudaijun.com/tags/erlang/"}]},{"title":"开发笔记(3) mongodb driver","date":"2015-11-01T16:00:00.000Z","path":"2015/11/erlang-server-design3-mongodb-driver/","text":"erlang mongodb驱动地址: https://github.com/comtihon/mongodb-erlang 先说说mongodb-erlang驱动的一些特性： 支持atom和binary作为Key，atom最终是转换为binary进行存储的，而在读取时，驱动并不会将对应binary转换为atom(它也不知道怎么转) 不支持integer，string(对erlang来说是字符串，对mongodb来说是数组)作为Key 支持atom，binary，integer作为值，这三者的存取是透明的，不需要特殊转换，在mongodb中，atom被存为Symbol(xxx) 支持string作为值，但实际上存的是字符数组，如果想存字符串，应使用binary 目前最新的mongodb-erlang驱动使用erlang map来存储doc(之前版本用的是bson list) 基于游戏服务器的需求，我们希望： mongodb driver能够支持integer作为key 从模型到对象的转换是透明的，无需我们关心 之前我们服务器逻辑中的数据模型是Dict，而mongodb-erlang使用的是bson-list来表示文档，在此之上做了一些比较繁杂转换。自mongodb-erlang支持map之后，我们也将数据结构由dict改为了map(PS: 非直观的是，map的效率不比dict差，参见测试代码)，如此我们需要对驱动读取的map的key value做一些类型转换。为了达到以上两点，我们对mongodb-erlang驱动做了些更改： 修改mongodb-erlang的依赖bson-erlang，在src/bson_binary.erl中添加对integer key的存储支持： put_field_accum(Label, Value, Bin) when is_atom(Label) -&gt; &lt;&lt;Bin/binary, (put_field(atom_to_binary(Label, utf8), Value))/binary&gt;&gt;; % add this line to suport integer key put_field_accum(Label, Value, Bin) when is_integer(Label) -&gt; &lt;&lt;Bin/binary, (put_field(integer_to_binary(Label), Value))/binary&gt;&gt;; put_field_accum(Label, Value, Bin) when is_binary(Label) -&gt; &lt;&lt;Bin/binary, (put_field(Label, Value))/binary&gt;&gt;. 在读取时，为了支持atom key和integer key的透明转换，我们约定了服务器只使用integer和atom(不能是integer atom，如’123’)作为key，这样我们可以在驱动读取完成后，进行key的自动转换： % 将Key由二进制串 转为整数或者原子 convert_map_key(Map) when is_map(Map) -&gt; maps:fold(fun(Key, Value, NewMap) -&gt; NewKey = case catch binary_to_integer(Key) of {&#39;EXIT&#39;, {badarg, _R}} -&gt; binary_to_atom(Key, utf8); IntegerKey -&gt; IntegerKey end, maps:put(NewKey, convert_map_key(Value), NewMap) end, maps:new(), Map); convert_map_key(List) when is_list(List) -&gt; lists:map(fun(Elem) -&gt; convert_map_key(Elem) end, List); convert_map_key(Data) -&gt; Data. 最后还有一个小问题，就是mongodb-erlang的mongo.erl中，在插入文档时，会自动检查文档是否包含&lt;&lt;”_id”&gt;&gt;键，如果没有，则会为其生成一个ObjectId()作为&lt;&lt;”_id”&gt;&gt;键的值，这里我们需要将其改为检查’_id’原子键，否则我们在逻辑中创建的包含’_id’键的文档，最终存入时，mongodb中的”_id”键的值是驱动自动生成的ObjectId()，而不是我们定义的’_id’键的值： assign_id(Map) when is_map(Map) -&gt; case maps:is_key(&#39;_id&#39;, Map) of true -&gt; Map; false -&gt; Map#{&#39;_id&#39; =&gt; mongo_id_server:object_id()} end; assign_id(Doc) -&gt; case bson:lookup(&#39;_id&#39;, Doc) of {} -&gt; bson:update(&#39;_id&#39;, mongo_id_server:object_id(), Doc); _Value -&gt; Doc end. 现在我们已经支持integer，atom作为key，binary，integer，atom，list作为value，基于这些类型的key/value是无需我们关心模型到对象的映射转换的。对于一个游戏服务器来说，基本上已经能够满足大部分需求了。对于一些极为特殊的模块，再通过设定回调(on_create/on_init/on_save)等方式特殊处理。","tags":[{"name":"erlang","slug":"erlang","permalink":"http://wudaijun.com/tags/erlang/"}]},{"title":"TCP复习笔记(三) TCP套接字","date":"2015-10-10T16:00:00.000Z","path":"2015/10/tcp-notes-3/","text":"主要流程服务器端套接字的主要流程： socket()：创建一个主动套接字 bind()：为套接字绑定一个本地协议地址和端口(这一步不是必须) listen()：将套接字改为被动套接字，如果套接字没有绑定端口，为套接字选择一个临时端口，此时TCP状态机等待SYN包的到达 accept()：从listen backlog队列中取出一个已经建立(已完成三次握手)的连接 而对于客户端来说，只需要知道服务器IP,Port，直接connect()即可，客户端一般无需主动调用bind()绑定端口，因为客户端不关心它的本地端口，connect()会为其选择一个临时端口。 套接字APIsocket()创建一个指定协议簇和套接字类型套接字，对于IPv4 TCP套接字，通常创建方式为socket(AF_INET, SOCKET_STREAM, 0)，该函数创建的TCP套接字默认是主动套接字。 bind()为指定套接字分配一个本地协议地址，该地址根据套接字类型而定。对于TCP套接字来说，调用bind()函数可以为套接字指定一个IP地址和本地端口，IP地址必须是本机的一个接口。 对于服务器套接字来说，绑定一个固定端口一般是必要的，因为客户端需要指定这个端口才能找到对应服务器进程。而对于IP地址，通常服务器可能不止一个IP，而绑定了一个固定IP意味着套接字只能接收那些目的地为此IP的连接。因此一般我们指定绑定地址为INADDR_ANY，意味着让内核来选择IP地址，内核的做法是：将客户端SYN包的目的IP地址，作为服务器的源IP地址。 如前面所说，客户端一般是不需要调用bind函数的，在调用connect()时，由内核选定一个本地IP地址和临时端口。 listen()通过socket()函数创建的套接字为主动套接字，调用listen()将使该套接字变为被动套接字，这意味着内核应接收该套接字上的连接请求(SYN包)，listen()使套接字从CLOSED状态变为LISTEN状态。 由于TCP三次握手，客户端与服务器建立连接需要两步： 在客户端SYN包到达时，服务端回复SYN-ACK包，此时套接字处于SYN_RECV状态 客户端ACK包到达，此时服务器套接字从SYN_RECV变为ESTABLISH状态，之后等待被accept()取出 因此，针对这两种状态的套接字的管理，有两种方案： 维护一个队列，里面包括SYN_RECV和ESTABLISH两种状态的套接字，当客户端三次握手最后一个ACK到达时，将对应套接字状态由SYN_RECV改为ESTABLISH。而accept()只会取出ESTABLISH状态的套接字。在这种实现中，listen()的backlog参数就是这个队列的最大长度。 维护两个队列，一个未完成连接队列(SYN队列)，存放SYN_RECV状态的套接字。一个已完成连接队列(Accept队列)，存放ESTABLISH状态的套接字。当连接完成(收到客户端的三次握手ACK)后，套接字将从SYN队列移到Accept队列尾部。accept()函数每次从已完成连接队列头部取出一个套接字。这种实现中，backlog参数指的是Accept队列最大长度。 历史上两种方案均被不同的套接字实现版本采取过，而目前Linux2.2以上的版本使用的是第二种方案（参见Linux listen() man page），意味着backlog限制Accept队列的大小，而SYN队列的大小通过tcp_max_syn_backlog内核参数来控制。那么这里我们有几个问题需要讨论： 当SYN队列满时，新客户端再次尝试连接(发送SYN包)，会发生什么？ 当Accept队列满时，收到了客户端的握手ACK，需要将套接字从SYN队列移至Accept队列，会发生什么？ 客户端发完握手ACK后，对客户端来说，连接已经建立(处于ESTABLISH状态)了，而服务器套接字由于各种原因(如Accept队列满)并未到达ESTABLISH状态，此时客户端向服务器发送数据，会发送什么？ 当SYN队列满时，通常的TCP实现会忽略SYN包(而不是发送RST包重置连接)，这使得客户端connect()会进行超时重传，等待SYN队列有空闲位置。tcp_syn_retries参数可以控制客户端SYN报文的重试次数。 当Accept队列满时，这通常是由于accept()调用处理不过来，如果这时收到了客户端的握手ACK包，如果内核参数tcp_abort_on_overflow=0，也就是默认情况，Linux实现会忽略该ACK，这将导致服务器会超时重传SYN-ACK包(参数tcp_synack_retries可控制重传次数)，然后客户端收到SYN-ACK包，也会假设之前的ACK包丢失了，仍然会回复ACK，此时服务器再次收到ACK，可能Accept队列就有空闲位置了。而如果tcp_abort_on_overflow=1，服务器在Accept队列满了，处理不过来时，将直接回复一个RST包，这将导致一个客户端connect()错误: ECONNREFUSED。客户端将不会再次重试。在Linux下，当Accept队列满时，内核还会限制SYN包的进入速度，如果太快，有些SYN包将会被丢弃。 站在客户端的角度来说，当它收到SYN-ACK并回复ACK后，连接就已经建立了。此时如果它立即向服务器发送数据，而服务可能由于Accept队列满，忽略了ACK，也就仍然处于SYN_RECV状态，此时客户端发送的数据仍然将被忽略，并且由客户端重传。TCP的慢启动机制确保了连接刚建立时不会发送太多的数据。 最后，在Linux下，backlog指定的大小受限于/proc/sys/net/core/somaxconn。另外，不要将backlog设为0，不能的实现可能对此有不同的解释。 关于listen() backlog更详细的讨论参见：http://veithen.github.io/2014/01/01/how-tcp-backlog-works-in-linux.html accept()从Accept队列中取出一个已完成连接，若Accept队列为空，则进程睡眠(假设为阻塞方式)。 accept()返回一个新的连接套接字(内核已经为它已经完成三次握手)，之后与客户端套接字的通信均通过该连接套接字来完成。 connect()向指定地址发送SYN报文，尝试建立连接。如果套接字之前没有调用bind()绑定地址端口，内核会选择源IP地址和一个临时端口。 connect()仅在连接成功或出错时才返回，出错的可能有： 没有收到SYN包的响应，尝试超时重发，最后仍无响应。返回ETIMEOUT 如果收到的SYN响应为RST，表明服务器对应端口还没有进程监听(未调用listen并处于LISTEN状态，状态机不能接收SYN报文)，客户端收到RST包立即返回ECONNREFUSED错误 如果客户端发出的SYN引发了目的地不可达的ICMP错误，那么将按第一种情况重试，重试未果最终返回EHOSTUNREACH或ENETUNREACH","tags":[{"name":"tcp/ip","slug":"tcp-ip","permalink":"http://wudaijun.com/tags/tcp-ip/"}]},{"title":"TCP复习笔记(二) TCP服务器参见问题和参数设置","date":"2015-10-08T16:00:00.000Z","path":"2015/10/tcp-notes-2/","text":"TCP核心参数关于TCP核心参数，参见：https://www.frozentux.net/ipsysctl-tutorial/chunkyhtml/tcpvariables.html 其中比较重要的有： tcp_syn_retries三次握手中，发出的SYN未得到响应时，超时重传SYN包的次数 tcp_synack_retries三次握手中最后一个ACK未收到时，超时重传SYN-ACK包的次数 tcp_max_syn_backlog服务器端SYN队列大小 tcp_abort_on_overflow当服务器忙不过来时(listen backlog满了)，发送RST包重置连接 tcp_tw_reuse复用正在TIME_WAIT状态的端口 tcp_defer_acceptserver端会在接收到最后一个ack之后，并不进入ESTABLISHED状态，而只是将这个socket标记为acked，然后丢掉这个ack。此时server端这个socket还是处于syn_recved，然后接下来就是等待client发送数据， 而由于这个socket还是处于syn_recved,因此此时就会被syn_ack定时器所控制。直到收到客户端第一个包(此时连接才ESTABLISHED)或重传超时(丢掉连接)。 针对客户端发送第一个包(典型地，如HTTP浏览器)的情况下，这个参数可以延迟连接的建立(ESTABLISHED)，在应用层体现为延迟连接服务(进程/线程/Actor)的创建，对某些对最大连接(服务)数有限制的服务器，可以更充分地利用资源。并且由于少了服务的休眠/唤醒，可能在这方面有细微地性能提升。 下面是一些比较危险，通常不建议使用的选项： tcp_syncookies 当SYN队列满时，可通过cookies的方式与客户端建立连接(即使该连接不在SYN队列中)。它违反了TCP三次握手协议，是非正规不严谨的。尽管对防止Syn Flood很有帮助 tcp_tw_recycle打开TIME_WAIT状态套接字的快速回收，比tcp_tw_reuse更为激进，慎用 tcp_max_tw_buckets处于TIME_WAIT状态的套接字最大数量，超过这个限制的套接字将被销毁 服务器常见问题1. SYN Flood 攻击SYN泛洪攻击是指伪造TCP请求，发送SYN包，被攻击服务器将该连接加入SYN队列中，发送SYN-ACK包，但永远等不到客户端的ACK包，直到超时重传SYN-ACK多次后，这种”半连接”才能正常释放。大量的这种请求会耗尽SYN队列，导致正常连接请求得不到响应。 通过Shell命令： netstat -n | awk &#39;/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}&#39; 查看当前所有连接的状态统计。SYN_RECV状态即为服务器端等待客户端ACK的状态，当该状态的连接数量过多时，通常是遭受了SYN Flood攻击。 解决方法： 调整tcp_synack_retries，减少超时重发SYN-ACK的次数(默认为5次) 调整tcp_max_syn_backlog，增大”半连接”队列，即SYN队列 不到万不得已不要使用tcp_syncookies选项 参考：http://tech.uc.cn/?p=1790 2. TIME_WAIT 状态前面已经说过这个问题，解决这个问题的方法： 尽量让客户端主动断开连接 服务器监听套接字使用SO_REUSEADDR选项 3. 半打开连接半打开连接(Half-Open)是指，一方已经关闭或异常终止连接而另一方还不知道。 比如客户端突然异常关机，没有发送FIN，而服务器并不知道客户端已经不存在，仍然维护着这个连接，占用着服务器资源。当客户端重启后，将使用一个新的临时端口，即通过一个新连接与服务器通信，而旧的半打开连接仍然存在(我们假设服务器不会主动向客户端发消息，如果有，参见服务器异常关闭的情况)。 而如果服务器异常关闭了，客户端仍然维护着这个连接，在服务器重启后，客户端尝试给服务器发消息，此时服务器将返回一个RST包，导致连接复位。 解决方法： 应用层保活定时器：心跳机制 TCP层包括定时器：TCP的keepalive 4. 半关闭连接半关闭连接是指一方结束了它的发送行为，但是还能够收到来自另一方的数据。即只关闭了一个方向上的通道。这可能是应用利用半关闭特性来做一些事情(尽管并不建议这么做)，也可能是应用忘了关闭另一个方向上的通道。 通过上面的Shell命令统计出状态结果，其中状态FIN_WAIT2，即为半关闭状态。这通常也是服务器端需要注意的。","tags":[{"name":"tcp/ip","slug":"tcp-ip","permalink":"http://wudaijun.com/tags/tcp-ip/"}]},{"title":"TCP复习笔记(一) 连接的建立与终止","date":"2015-10-06T16:00:00.000Z","path":"2015/10/tcp-notes-1/","text":"一. TCP首部 首部长度以4字节为单位，4位首部长度最多可表示60字节，其中有20字节的固定长度。由于封装在在一个IP报文中，并且可根据首部长度得到数据起始位置，因此TCP首部没有数据或总长度字段，这和IP首部不一样，IP首部同时有首部长度和总长度字段，前者用于得到数据起始位置，后者用于得到结束位置，因为以太网要求桢的数据部分最小长度为46字节，IP报文小于46字节将在后面添加Padding，因此需要总长度来区分哪些是数据，哪些是Padding。这一点对TCP来说不存在，因为得到IP报文数据的结束位置，也就得到了TCP数据的结束位置： 首部中的序号对传输字节进行计数，在建立连接时，SYN置1，此时序号字段为该主机选择的初始序列号(ISN)。另外，SYN和FIN分别占用一个序列号，这样才能唯一标识一个SYN或FIN包，用于确认或超时重传。 二. 连接建立与终止 2.1 MSS在建立连接时，双方会在TCP首部加入MSS选项用于通告己方能接收的最大报文段长度(不包括TCP和IP首部)，MSS只能出现在SYN报文中，如果没有指明，则默认为536(实际上是576的IP报文长度)。在发送SYN报文时，根据MSS=MTU-固定的IP首部-固定的TCP首部长度来确定MSS大小，对于以太网，理想的MSS为1500-20-20=1460，而实际上大部分的MSS为1024，因为许多BSD的实现版本需要MSS为512的倍数。 通告双方的MSS主要是为了避免分段。分段(分片)发生在IP层，由于物理链路层限制了每次发送的数据帧最大长度，因此IP层会在必要的时候对数据进行分段，并在到达目的地时重组，这一切对传输层的TCP是透明的，因此TCP可以认为它的每份交给IP的字节流，都会以正确的形式到达目的地。 避免分片主要有两个好处： 效率更高，因为IP层的分片可能由中间路由器完成 在有IP分片丢失时，将重传整个IP报文(IP报文没有确认重传机制)。基于这一点，TCP避免分片将使得重传更为高效 2.2 ISN在建立连接的SYN报文中的序列号字段即为初始序列号(ISN)，这个值不能是硬编码的，否则可能会出现重新建立连接后，新连接将旧连接的包误认为是有效包的情况(每次建立连接的ISN是一样的)。ISN的初始化应该是动态的，防止新旧包交叠。RFC的建议是每4微妙ISN加1，这样用完32位需要4.55小时，然后ISN又从0开始。4.55小时远远大于TCP包在网络中的最大生存时间，是比较安全的。 2.3 半关闭半关闭是指TCP一端在结束发送后，还能收到来自另一端的数据。半关闭不建议被应用程序利用，但是我们至少应该理解shudown(1)和close()的区别，确保TCP连接得到正确关闭。 2.4 SYN超时在尝试主动连接服务器时，如果服务器不可达，客户端将尝试重发SYN包。比如：telnet 11.11.11.11，可通过tcpdump tcp port 23查看重试次数和重试间隔，也可以通过date; telnet 11.11.11.11; date来查看总超时时间。 2.5 SYN-ACK超时当服务器收到客户端的SYN包后，会回复SYN-ACK包，如果此时客户端迟迟不回ACK包，那么服务器将超时重发SYN-ACK包，重发次数默认为5次，重发间隔依次为1s,2s,4s,8s,16s,加之最后确认超时的32s,一共是63s。这63秒中，该连接占用了服务器的SYN队列，当SYN队列满时，新的连接请求将不能得到处理。SYN Flood攻击就是利用这一点，在发完第一个SYN之后，就下线，耗尽服务器的SYN队列，使其它的正常连接请求得不到处理。 三. 状态转变 其中比较重要的状态有： 3.1 TIME_WAIT在主动发起关闭的一方，发送完最后一个ACK后，需要等待2MSL的时间，这样做的目的有两个： 确保最后一个ACK正确到达，2MSL可让TCP再次发送最后的ACK以防这个ACK丢失(另一端超时重发FIN) 确保在建立新的连接前，任何老的重复报文在网络中超时消失 MSL(Maximum Segment Lifetime)：报文最大生存时间，用于限制TCP包在网络中最大留存时间，超过这个时间，包将被丢弃。IP层有个类似的TTL跳数来决定IP报文的去留，MSL和TTL共同限制了TCP包的生存时间。RFC建议MSL为2分钟，而Linux下为30秒。 在2MSL这段时间内，定义这个连接的套接字对将不可用，任何迟到的报文都将被丢弃。而在伯克利套接字等实现版本上，有更严格的限制：在2MSL时间内，套接字所使用的本地端口默认情况下都不能再使用。 对于客户端来说，这通常没有什么影响，因为客户端一般都使用临时端口与服务器进行连接。因此客户端主动断开连接并重启后，尽管之前使用的端口会处于TIME_WAIT状态而不能复用(bind())，但conn\u0010ect()会选择一个新的临时端口。 而对于服务器来说，由于服务器通常使用已知端口(监听端口)，如果我们终止一个已经建立连接的服务器程序，并重启它，服务器程序将不能把这个已知端口赋给新的套接字(bind())，会得到EADDRINUSE的错误，只有在2MSL之后，端口才能被再次使用。 我们可以使用SO_REUSEADDR选项来重用处于2MSL状态的端口，它的原理是需要新的连接的初始序列号(ISN)大于旧连接的最后序号。这样就可以根据序列号区分哪些是旧连接的迟到报文，哪些的新连接的报文，但是仍然是有遗留风险的。 3.2 FIN_WAIT_2在主动发起关闭的的一方发送完FIN并收到另一方的ACK后，进入FIN_WAIT_2状态。此时连接处于半关闭状态，正常情况下，如果我们的应用不利用半关闭这个特性，那么对方的应用层在收到FIN后，也会发送一个FIN关闭另一个方向上的连接。本端收到该FIN后，才进入TIME_WAIT状态。 这意味着主动关闭端将可能永远处于FIN_WAIT_2状态，被动端也一直处于CLOSE_WAIT状态。为了防止这种无限等待，当应用层执行全关闭(close)而不是半关闭(shutdown)时，多数套接字实现会生成一个定时器，定时器超时后，连接将进入CLOSED状态。这也进一步提醒我们应该确保正确关闭TCP连接。 四. TCP状态机网络上的传输的没有连接的，TCP的所谓的”连接”是依靠一个状态机来维护当前的连接状态。理解这一点是非常重要的，因为网络是随时会有异常，连接在任何阶段都可能被异常终止。通常情况下，如果状态机收到了意料之外的包，将回复一个RST重置包，对方会据此重置连接状态。具体的状态机如下： TCP状态机分组丢失：http://www.cppblog.com/qinqing1984/archive/2015/10/05/211950.html","tags":[{"name":"tcp/ip","slug":"tcp-ip","permalink":"http://wudaijun.com/tags/tcp-ip/"}]},{"title":"Unity 动画系统小记","date":"2015-10-02T16:00:00.000Z","path":"2015/10/unity-mecanim/","text":"校正：本文对动画系统理解上有很大的偏差和局限性，关于更详细正确的理解可参见： http://blog.csdn.net/cubesky/article/details/39478207 动画系统如果动画包含多层，要在初始化时设置动画层权重。 通常情况下，在我们操作角色时，脚本根据玩家输入，设置动画的行进速度(可平滑过度)，由动画的Blend Tree来做到角色走和跑之间的平滑过渡，并且将动画的实际位移作用于角色模型上。也就是说，由动画控制前进(走，跑)，脚本直接控制转向(rigidbody.MoveRotation)。模型只需提供Walk和Run两个动画片段即可，但需要导入位移信息。 Animator有个比较有用的选项，用于通过脚本而不是动画来控制模型移动。 Apply Root Motion：是否将动画的位移，转向作用于实际模型之上。即模型的运动(Position，Rotation的变化)是由动画控制还是脚本控制。 Apply Root Motion的作用示例： 当取消Apply Root Motion时，动画的转向和移动都将被屏蔽，只显示动画本身的动作。Apply Root Motion在一些场合下很有用，比如当模型自己有AI： NavMeshAgent: 寻路组件，要使用它，首先要烘培寻路地形(Windows-&gt;Navigation)，然后设置目标位置。NavMeshAgent组件会自动将游戏对象按照当前最佳路径移动至目标位置。默认情况下，Nav会自动控制模型的移动和转向。可通过nav.updateRotation/updatePosition决定NavMeshAgent对游戏对象的控制权，当两者都为False时，角色当原地不动。 在游戏中，敌人或者是小兵通常同时具有Animator和NavMeshAgent，而如果此时我们想实现一些比较灵活的控制：比如敌人在寻路至目标点附近(比如刚好看到目标)时，停止移动(进行远程射击)。 此时有两种做法： 由NavMeshAgent控制敌人移动 此时禁用Apply Root Motion，Animator只做表现。在游戏逻辑中，根据当前Nav期望速度(决定前进速度和转向速度分量)，与目标的视角(当视角小于一定角度时，直接LookAt到目标位置方向，避免缓冲时间太慢)，是否已经看到目标(此时停下来，看向目标，开枪)等条件，设置Animator的speed(前进速度)和angularSpeed(转向速度)，此时动画层会作出响应的动作，如走，跑，转向等，但是没有实际运动(位移和转向)。然后在OnAnimatorMove中： void OnAnimatorMove() { nav.velocity = anim.deltaPosition / Time.deltaTime; } 这样我们可以使得动画层和表现和实际运动基本一致，每帧Update()在OnAnimatorMove()之前调用，所有状态都记录到了Animator中。由于Nav的转向是比较生硬的，我们也可以让动画来控制转向：首先在Awake()中nav.updateRotation = false;不让Nav调整敌人转向，然后在OnAnimatorMove()中添加：transform.rotation = anim.rootRotation;。 由动画控制敌人移动 逻辑处理不变，取消OnAmimatorMove()，勾选Apply Root Motion，并且同时禁用NavMeshAgent的updateRotation和updatePosition。此时nav的速度矢量仅作为参考，最终速度将设置在动画系统中，并由动画系统作用于实际模型。","tags":[{"name":"unity","slug":"unity","permalink":"http://wudaijun.com/tags/unity/"}]},{"title":"【译】进程和错误","date":"2015-09-24T16:00:00.000Z","path":"2015/09/erlang-process-links/","text":"learn some erlang上很喜欢的一个章节，主要阐述进程，链接，监视，信号捕获等。花了两天的时间才翻译完(- -)。第一次翻译文章，真心不是件容易的事。但也受益匪浅，平时一晃而过的地方，现在却要字字推敲。这是初稿，后续慢慢校正。原文地址：http://learnyousomeerlang.com/errors-and-processes 链接链接(link)是两个进程之间的一种特殊的关系。一旦这种关系建立，如果任意一端的进程发生异常，错误，或退出(参见Errors and Exceptions)，链接的另一端进程将一并退出。 这是个很有用的概念，源自于Erlang的原则”鼓励崩溃”：如果发生错误的进程崩溃了而那些依赖它的进程不受影响，那么之后所有这些依赖进程都需要处理这种依赖缺失。让它们都退出再重启整组进程通常是一个可行的方案。链接正提供了这种方案所需。 要为两个进程设置链接，Erlang提供了基础函数link/1，它接收一个Pid作为参数。这个函数将在当前进程和Pid进程之前创建一个链接。要取消链接，可使用ulink/1。当链接的一个进程崩溃，将发送一个特殊的消息，该消息描述了哪个进程出于什么原因而发送故障。如果进程正常退出(如正常执行完其主函数)，这类消息将不会被发送。我将首先介绍这个新函数，它是linkmon.erl的一部分： myproc() -&gt; timer:sleep(5000), exit(reason). 如果你尝试下面的调用(并且在两次spawn操作之间等待5秒钟)，你就能看到shell只有在两个进程之间设置了链接时，才会因reason而崩溃。 1&gt; c(linkmon). {ok,linkmon} 2&gt; spawn(fun linkmon:myproc/0). &lt;0.52.0&gt; 3&gt; link(spawn(fun linkmon:myproc/0)). true ** exception error: reason % 译注：此时Shell Process已经崩溃，只是立即被重启了。通过self()查看前后的Pid是不同的 或者，我们可以用图片来阐述： 然后，这个{&#39;EXIT&#39;, B, Reason}消息并不能被try ... catch捕获。我们需要通过其它机制来实现这点，我们将在后面看到。 值得注意的是，链接通常被用来建立一个需要一起退出的进程组： chain(0) -&gt; receive _ -&gt; ok after 2000 -&gt; exit(&quot;chain dies here&quot;) end; chain(N) -&gt; Pid = spawn(fun() -&gt; chain(N-1) end), link(Pid), receive _ -&gt; ok end. chain函数接收一个整型参数N，创建N个依次相互链接的进程。为了能够将N-1参数传递给下一个chain进程(也就是spawn/1)，我将函数调用放在了一个匿名函数中，因此它不再需要参数。调用spawn(?MODULE, chain, [N-1])能达到同样的效果。 这里，我将有一条链式的进程组，并且随着它们的后继者退出而退出： 4&gt; c(linkmon). {ok,linkmon} 5&gt; link(spawn(linkmon, chain, [3])). true ** exception error: &quot;chain dies here&quot; 正如你所看到的，Shell将从其它进程收到死亡信号。这幅图阐述产生的进程依次链接： [shell] == [3] == [2] == [1] == [0] [shell] == [3] == [2] == [1] == *dead* [shell] == [3] == [2] == *dead* [shell] == [3] == *dead* [shell] == *dead* *dead, error message shown* [shell] &lt;-- restarted 在执行linkmon:chain(0)的进程死掉之后，错误消息沿着链接链依次传播，直播Shell进程也因此崩溃。崩溃可能发生在任何已经链接的进程中，因为链接是双向的，你只需要令其中一个死亡，其它进程都会随之死亡。 注意：如果你想要通过Shell杀掉其它进程，你可以使用`exit/2`函数，如：`exit(Pid, Reason)`。你可以试试。 链接操作无法被累加，如果你在同样的一对进程上调用`link/1`15次，也只会实际存在一个链接，并且只需要一次`unlink/1`调用就可以解除链接。 注意，link(spawn(Function))或link(spawn(M,F,A))是通过多步实现的。在一些情况下，可能进程在被链接之前就死掉了，这样引发了未知行为。出于这个原因，Erlang添加了spawn_link/1-3函数，它和spawn/1-3接收同样的参数，创建一个进程并且相link/1一样建立链接，但是它是一个原子操作(这个操作混合了多个指令，它可能成功或失败，但不会有其它未期望行为)。着通常更安全，并且你也省去了一堆圆括号。 信号捕获现在回到链接和进程故障。错误在进程之间向消息那样传递，这类特殊的消息叫做信号。退出信号是自动作用于进程的”秘密消息”，它会立即杀死进程。 我之前提到过很多次，为了高可靠性，应用程序需要能够很快的杀掉和重启进程。现在，链接很好地完成了杀死进程的任务，还差进程重启。 为了重启一个进程，我们首先需要一种方式来知道有进程挂了。这可以通过在链接之上封装一层叫系统进程的概念来完成。系统进程其实就是普通进程，只不过他们可以将退出信号转换为普通消息。在一个运行进程上执行precess_floag(trap_exit, true)可以将其转换为系统进程。没什么比例子更具有说服力了，我们来试试。我首先在一个系统进程上将重演chain例子： 1&gt; process_flag(trap_exit, true). true 2&gt; spawn_link(fun() -&gt; linkmon:chain(3) end). &lt;0.49.0&gt; 3&gt; receive X -&gt; X end. {&#39;EXIT&#39;,&lt;0.49.0&gt;,&quot;chain dies here&quot;} 现在事情变得有趣了，回到我们的图例中，现在发生的是这样： [shell] == [3] == [2] == [1] == [0] [shell] == [3] == [2] == [1] == *dead* [shell] == [3] == [2] == *dead* [shell] == [3] == *dead* [shell] &lt;-- {&#39;EXIT,Pid,&quot;chain dies here&quot;} -- *dead* [shell] &lt;-- still alive! 这就是让我们可以快速重启进程的机制。通过在程序中使用系统进程，创建一个只负责检查进程崩溃并且在任意时间都能重启故障进程的进程变得很简单。我将在下一章真正用到了这项技术时，更详细地阐述这点。 现在，我想回到我们在exceptions这一章看到的异常函数，并且展示它在设置了trap exit的进程上有何种行为。我们首先试验没有系统进程的情况。我连续地在相邻的进程上展示了未被捕获的异常，错误，和退出所造成的结果： Exception source: spawn_link(fun() -&gt; ok end) Untrapped Result: - nothing - Trapped Result: {&#39;EXIT&#39;, &lt;0.61.0&gt;, normal} 注：进程正常退出，没有任何故障。这有点像`catch exit(normal)`的结果，除了在tuple中添加了Pid以知晓是哪个进程退出了。 Exception source: spawn_link(fun() -&gt; exit(reason) end) Untrapped Result: ** exception exit: reason Trapped Result: {&#39;EXIT&#39;, &lt;0.55.0&gt;, reason} 注：进程由于客观原因而终止，在这种情况下，如果没有捕获退出信号(trap exit)，当前进程被终止，否则你将收到以上消息。 Exception source： spawn_link(fun() -&gt; exit(normal) end) Untrapped Result: - nothing - Trapped Result: {&#39;EXIT&#39;, &lt;0.58.0&gt;, normal} 注：这相当于模仿进程正常终止。在一些情况下，你可能希望像正常流程一样杀掉进程，不需要任何异常流出。 Exception source: spawn_link(fun() -&gt; 1/0 end) Untrapped Result: Error in process &lt;0.44.0&gt; with exit value: {badarith, [{erlang, &#39;/&#39;, [1,0]}]} Trapped Result: {&#39;EXIT&#39;, &lt;0.52.0&gt;, {badarith, [{erlang, &#39;/&#39;, [1,0]}]}} 注：{badarith, Reason}不会被try ... catch捕获，继而转换为&#39;EXIT&#39;消息。这一点上来看，它的行为很像exit(reason)，但是有调用堆栈，可以了解到更多的信息。 Exception source: spawn_link(fun() -&gt; erlang:error(reason) end) Untrapped Result: Error in process &lt;0.47.0&gt; with exit value: {reason, [{erlang, apply, 2}]} Trapped Result: {&#39;EXIT&#39;, &lt;0.74.0&gt;, {reason, [{erlang, apply, 2}]}} 注：和1/0的情况很像，这是正常的，erlang:error/1 就是为了让你可以做到这一点。 Exception source: spawn_link(fun() -&gt; throw(rocks) end) Untrapped Result: Error in process &lt;0.51.0&gt; with exit value: {{nocatch, rocks}, [{erlang, apply, 2}]} Trapped Result: {'EXIT', , {{nocatch, rocks}, [{erlang, apply, 2}]}} 注：由于抛出的异常没有被try ... catch捕获，它向上转换为一个nocatch错误，然后再转换为`EXIT`消息。如果没有捕获退出信号，当前进程当终止，否则工作正常。 这些都是一般异常。通常情况下：一切都工作得很好。当异常发生：进程死亡，不同的信号被发送出去。 然后来介绍exit/2，它在Erlang进程中就相当于一把枪。它可以让一个进程杀掉远端另一个进程。以下是一些可能的调用情况： Exception source: exit(self(), normal) Untrapped Result: ** exception exit: normal Trapped Result: {&#39;EXIT&#39;, &lt;0.31.0&gt;, normal} 注：当没有捕获退出信号时，exit(self(), normal)和exit(normal)作用一样。否则你将收到一条和链接进程挂掉一样格式的消息。(译注：如果忽略了{&#39;EXIT&#39;, self(), normal}，将不能通过exit(self(), normal)的方式杀掉自己。而exit(normal)则可以在任何情况结束自己。) Exception source: exit(spawn_link(fun() -&gt; timer:sleep(50000) end), normal) Untrapped Result: - nothing - Trapped Result: - nothing - 注：这基本上等于调用exit(Pid, normal)。这条命令基本没有做任何有用的事情，因为进程不能以normal的方式来杀掉远端进程。(译注：通过normal的方式kill远端进程是无效的)。 Exception source: exit(spawn_link(fun() -&gt; timer:sleep(50000) end), reason) Untrapped Result: ** exception exit: reason Trapped Result: {&#39;EXIT&#39;, &lt;0.52.0&gt;, reason} 注：外部进程通过reason终止，看起来效果和在外部进程本身执行exit(reason)一样。 Exception source: exit(spawn_link(fun() -&gt; timer:sleep(50000) end), kill) Untrapped Result: ** exception exit: killed Trapped Result: {&#39;EXIT&#39;, &lt;0.58.0&gt;, killed} 注：出乎意料地，消息在从终止进程传向根源进程(译注：调用spawn的进程)时，发生了变化。根源进程收到killed而不是kill。这是因为kill是一个特殊的信号，更多的细节将在后面提到。 Exception source: exit(self(), kill) Untrapped Result: ** exception exit: killed Trapped Result: ** exception exit: killed 注：看起来这种情况不能够被正确地捕捉到，让我们来检查一下。 Exception source: spawn_link(fun() -&gt; exit(kill) end) Untrapped Result: ** exception exit: killed Trapped Result: {&#39;EXIT&#39;, &lt;0.67.0&gt;, kill} 注：现在看起来更加困惑了。当其它进程通过exit(kill)杀掉自己，并且我们不捕获退出信号，我们自己的进程退出原因为killed。然而，当我们捕获退出信号，却不再是killed。 你可以捕获大部分的退出原因，在有些情况下，你可能想要残忍地谋杀进程：也许它捕获了退出信号，但是陷入了死循环，不能再读取任何消息。kill是一种不能被捕获的特殊信号。这一点确保了任何你想要杀掉的进程都将被终止。通常，当所有其它办法都试尽之后，kill是最后的杀手锏。 由于kill退出原因不能够捕获，因此当其它进程收到该消息时，需要转换为killed。如果不以这种方式作出改变，所有其它链接到被kill进程的进程都将相继以相同的kill原因被终止，并且继续扩散到与它们链接的进程。随之而来的是一场死亡的雪崩效应。 这也解释了为什么exit(kill)在被其它链接进程收到时转换成了killed(信号被修改了，这样才不会发生雪崩效应)，但是在本地捕获时(译注：这里我也没搞清楚，本地是指被kill的进程，还是指发出kill命令的进程)，仍然是kill。 如果你对这一切感到困惑，不用担心，很多程序员都为此困惑。退出信号是一头有趣的野兽。幸运的是，上面已经提及几乎所有特殊情况。一旦你明白了这些，你就可以轻松明白大多数的Erlang并发错误管理机制。 监视器那么，也许谋杀掉一个进程并不是你想要的，也许你并不想将你死亡的消息通告四周，也许你应该更像一个追踪者。在这种情况下，监视器就是你想要的。 严格意义上说，监视器是一种特殊类型的链接。它与链接有两处不同： 监视器是单向的 监视可以被叠加 监视器可以让一个进程知道另一个进程上发生了什么，但是它们对彼此来说都不是必不可少的。 另一点，像上面所列出的一样，监视引用是可以被叠加的。乍一看这并没什么用，但是这对写需要统计其它进程情况的库很有帮助。 正如你所了解的，链接更像是一种组织结构。当你在架构你的应用程序时，你需要决定每个进程做什么，依赖于什么。一些进程将被用来监督其它进程，一些进程不能没有其兄弟进程而独立存在，等等。这种结构通常是固定的，并且事先决定好的。链接对于这种情况是非常适用的，但除此之外，一般并没有使用它的必要。 但是当你在使用两三个不同的库，而它们都需要知道其它进程存活与否，这种情况会发送什么？如果你尝试使用链接，那么当你尝试解除链接的时候，就会很快遇到问题。因为链接是不可叠加的，一旦取消了其中一个，你就取消了所有(译注：调用库时，仍然是在当前进程)在此之上的链接，也就破坏了其它库的所有假设。这很糟糕。因此你需要可叠加的链接，监视器就是你的解决方案。它们可以被单独地移除。另外，单向特性在库中也是很有用的，因为其它进程不应该关心上述库。 那么监视器看起来是什么样子？很简单，让我们来设置一个。相关函数是erlang:monitor/2，第一个参数是原子process，第二个参数是进程Pid： 1&gt; erlang:monitor(process, spawn(fun() -&gt; timer:sleep(500) end)). #Ref&lt;0.0.0.77&gt; 2&gt; flush(). Shell got {&#39;DOWN&#39;,#Ref&lt;0.0.0.77&gt;,process,&lt;0.63.0&gt;,normal} ok 每当你监视的进程挂掉时，你都会收到类似消息。消息格式为{&#39;DOWN&#39;, MonitorReference, process, Pid, Reason}。引用被用来取消监视，记住，监视是可以叠加的，所以可能不止一个。引用允许你以独特的方式追踪它们。还要注意，和链接一样，有一个原子函数可以在创建进程的同时监控它，spawn_monitor/3： 3&gt; {Pid, Ref} = spawn_monitor(fun() -&gt; receive _ -&gt; exit(boom) end end). {&lt;0.73.0&gt;,#Ref&lt;0.0.0.100&gt;} 4&gt; erlang:demonitor(Ref). true 5&gt; Pid ! die. die 6&gt; flush(). ok 在这个例子中，我们在进程崩溃之前取消了监视，因此我们没有追踪到它的死亡。函数demonitor/2也存在，并且给出了更多信息，第二个参数是一个选项列表。目前只有两个选项，info和flush： 7&gt; f(). ok 8&gt; {Pid, Ref} = spawn_monitor(fun() -&gt; receive _ -&gt; exit(boom) end end). {&lt;0.35.0&gt;,#Ref&lt;0.0.0.35&gt;} 9&gt; Pid ! die. die 10&gt; erlang:demonitor(Ref, [flush, info]). false 11&gt; flush(). ok info选项将告诉你在你取消监视的时候监视是否存在，因此第10行返回false。使用flush选项将移除信箱中的DOWN消息(译注：其它消息不受影响)，导致flush()操作没有在当前进程信箱中取得任何消息。 命名的进程理解了链接和监视之后，还有一个问题需要解决。我们使用linkmon.erl模块的以下函数： start_critic() -&gt; spawn(?MODULE, critic, []). judge(Pid, Band, Album) -&gt; Pid ! {self(), {Band, Album}}, receive {Pid, Criticism} -&gt; Criticism after 2000 -&gt; timeout end. critic() -&gt; receive {From, {&quot;Rage Against the Turing Machine&quot;, &quot;Unit Testify&quot;}} -&gt; From ! {self(), &quot;They are great!&quot;}; {From, {&quot;System of a Downtime&quot;, &quot;Memoize&quot;}} -&gt; From ! {self(), &quot;They&#39;re not Johnny Crash but they&#39;re good.&quot;}; {From, {&quot;Johnny Crash&quot;, &quot;The Token Ring of Fire&quot;}} -&gt; From ! {self(), &quot;Simply incredible.&quot;}; {From, {_Band, _Album}} -&gt; From ! {self(), &quot;They are terrible!&quot;} end, critic(). 现在假设我们在商店购买唱片。这里有一些听起来很有趣的专辑，但是我们不是很确定。你决定打电话给你的朋友ctritic(译注：后文称”鉴定家”)。 1&gt; c(linkmon). {ok,linkmon} 2&gt; Critic = linkmon:start_critic(). &lt;0.47.0&gt; 3&gt; linkmon:judge(Critic, &quot;Genesis&quot;, &quot;The Lambda Lies Down on Broadway&quot;). &quot;They are terrible!&quot; 烦人的是，我们不久后就不能再得到唱片的评论了。为了保持鉴定家一直存活，我们将写一个基本的监督者进程，它的唯一职责就是在鉴定家挂掉之后重启它。 start_critic2() -&gt; spawn(?MODULE, restarter, []). restarter() -&gt; process_flag(trap_exit, true), Pid = spawn_link(?MODULE, critic, []), receive {&#39;EXIT&#39;, Pid, normal} -&gt; % not a crash ok; {&#39;EXIT&#39;, Pid, shutdown} -&gt; % manual termination, not a crash ok; {&#39;EXIT&#39;, Pid, _} -&gt; restarter() end. 这里，重启者就是它自己持有的进程。它会轮流启动鉴定家进程，并且一旦它异常退出，restarter/0将循环创建新的鉴定家。注意我添加了{&#39;EXIT&#39;, Pid, shudown}条目，这是为了让我们在必要时，可以手动杀掉鉴定家进程。 我们这个方法的问题是，我们没有办法获得鉴定家进程的Pid，因此我们不能调用它并获得它的评论。Erlang解决这种问题的一个解决方案是为进程取一个名字。 为进程取名字的作用是允许你用一个原子代替不可预测的Pid。之后这个原子可以像Pid一样用来发送消息。erlang:register/2被用来为进程取名。如果进程死亡，它会自动失去它的名字，你也可以使用unregister/1手动取消名字。你可以通过register/0获得一个所有注册了名字的进程列表，或者通过shell命令reg()获得更为详尽的信息。现在我们可以像下面这样重写restarter/0函数： restarter() -&gt; process_flag(trap_exit, true), Pid = spawn_link(?MODULE, critic, []), register(critic, Pid), receive {&#39;EXIT&#39;, Pid, normal} -&gt; % not a crash ok; {&#39;EXIT&#39;, Pid, shutdown} -&gt; % manual termination, not a crash ok; {&#39;EXIT&#39;, Pid, _} -&gt; restarter() end. 正如你所看到的，不管鉴定家进程的Pid是什么，register/2将总是为其取名为critic。我们还需要做的是从抽象函数中替换需要传递Pid的地方。让我们试试： judge2(Band, Album) -&gt; critic ! {self(), {Band, Album}}, Pid = whereis(critic), receive {Pid, Criticism} -&gt; Criticism after 2000 -&gt; timeout end. 这里，为了能在receive语句中进行模式匹配，Pid = whereis(critic)被用来查找鉴定家进程的Pid。我们需要这个Pid来确定我们能匹配到正确的消息(在我们说话的时候，它的信箱可能有500条消息！)。这可能是问题的来源。上面的代码假设了鉴定家进程在函数的前两行将保持一致。然而，下面的情况是完全有可能发生的： 1. critic ! Message 2. critic receives 3. critic replies 4. critic dies 5. whereis fails 6. critic is restarted 7. code crashes 当然，还有一种情况可能发生： 1. critic ! Message 2. critic receives 3. critic replies 4. critic dies 5. critic is restarted 6. whereis picks up wrong pid 7. message never matches 如果我们不处理好的话，在一个进程中出错将可能导致另一个进程错误。在这种情况下，原子critic代表的值可能被多个进程看到。这就说所谓的共享状态。这里的问题是，critic的值可以在几乎同一时间被多个进程获取和修改，导致不一致的信息和软件错误。这类情况的通用术语为竞态。竞态是特别危险的，因为其依赖于事件时序。在几乎所有的并发和并行语言中，这种时序依赖于很多不可预测的因素，比如处理器有多忙，进程执行到哪了，以及你的程序在处理哪些数据。 别麻醉了自己 你可能听说过Erlang通常是没有竞态或死锁的，这令并行代码更安全。这在很多情况下都是对的，但是永远不要认为你的代码真的那样安全。命名进程只是并行代码可能出错的多种情况之一。 其它例子还包括计算机访问文件(并修改它们)，多个不同的进程更新相同的数据库记录，等等。 对我们来说幸运的是，如果我们不假设命名进程保持不变的话，修复上面的代码是比较容易的。取而代之地，我们将使用引用(通过make_ref()创建)作为一个唯一的值来标识消息。我们需要重写critic/0为critic/2，judge/3为judge2/2： judge2(Band, Album) -&gt; Ref = make_ref(), critic ! {self(), Ref, {Band, Album}}, receive {Ref, Criticism} -&gt; Criticism after 2000 -&gt; timeout end. critic2() -&gt; receive {From, Ref, {&quot;Rage Against the Turing Machine&quot;, &quot;Unit Testify&quot;}} -&gt; From ! {Ref, &quot;They are great!&quot;}; {From, Ref, {&quot;System of a Downtime&quot;, &quot;Memoize&quot;}} -&gt; From ! {Ref, &quot;They&#39;re not Johnny Crash but they&#39;re good.&quot;}; {From, Ref, {&quot;Johnny Crash&quot;, &quot;The Token Ring of Fire&quot;}} -&gt; From ! {Ref, &quot;Simply incredible.&quot;}; {From, Ref, {_Band, _Album}} -&gt; From ! {Ref, &quot;They are terrible!&quot;} end, critic2(). 并且随之改变restarter/0，让它通过critic2/0而不是critic/0来产生新进程。其它函数应该能保持正常工作。用户并不能察觉到变化。好吧，他们能察觉到，因为我们改变了函数名和函数参数个数，但是他们并不知道实现细节的改变和为什么这些改变如此重要。他们能看到的是他们的代码更简单了，并且不在需要Pid来调用函数了： 6&gt; c(linkmon). {ok,linkmon} 7&gt; linkmon:start_critic2(). &lt;0.55.0&gt; 8&gt; linkmon:judge2(&quot;The Doors&quot;, &quot;Light my Firewall&quot;). &quot;They are terrible!&quot; 9&gt; exit(whereis(critic), kill). true 10&gt; linkmon:judge2(&quot;Rage Against the Turing Machine&quot;, &quot;Unit Testify&quot;). &quot;They are great!&quot; 现在，即使我们杀掉了critic，马上会有一个新的回来解决我们的问题。这就是命名进程的作用。如果你试图通过没有注册的进程调用linkmon:judge2/2，一个bad argument错误将会被函数内的!操作符抛出，确保依赖于命名进程的进程，将不能在没有命名进程的情况下而运行。 注意：如果你还记得之前的文章，原子可用的数量有限(尽管很高)。你不应该动态地创建原子。这意味着命名进程应该保留给一些虚拟机上唯一的伴随整个应用程序周期的重要的服务。 如果你需要为进程命名，但是它们不是常驻进程或者它们都不是虚拟机上唯一的，那可能意味着它们需要表示为一组，链接它们，并在它们崩溃后重启可能是一个理智的选择，而不是尝试为他们动态命名。","tags":[{"name":"erlang","slug":"erlang","permalink":"http://wudaijun.com/tags/erlang/"}]},{"title":"ranch","date":"2015-09-22T16:00:00.000Z","path":"2015/09/erlang-ranch/","text":"一. 简介ranch是erlang的一个开源网络库，提供一个高效的监听进程池，并且将数据传输和数据处理分离开来。使用起来非常简单，灵活。关于ranch的更多介绍和使用，参见官方文档。 二. 功能ranch将网络连接分为传输层(transport)和协议层(protocol)，传输层是底层数据的传输方式，如tcp, udp, ssl等。协议层负责数据在业务逻辑上的处理，是开发者真正需要关心的部分。而ranch的一个目标就是将传输层和逻辑层很好的分离开来。 对服务器端来说，传输层需要负责管理监听套接字和连接套接字。ranch提供一个可设置的进程池，用于高效地接受新连接，将新连接套接字交予用户定义的连接进程，进行业务逻辑上的处理。 ranch做了什么： 允许多个应用同时使用，即可有多个listener，每个listener通过名字标识 每个listenr可单独设置acceptor进程池的大小和其它选项 可设置最大连接数，并且可动态改变其大小 到达最大连接数时，后续连接(已经accept的连接)进程将被阻塞，待负载降下来或最大连接数变大后被唤醒 提供安全的网络退出方式 三. 使用ok = application:start(ranch). {ok, _} = ranch:start_listener(tcp_echo, 100, % 监听器名字和监听进程池大小 ranch_tcp, [{port, 5555}], % 定义底层transport handler及其选项 ranch_tcp由ranch提供，底层使用gen_tcp echo_protocol, [] % 自定义的protocol handler进程所在模块，及其选项 ). 之后我们需要做的，就是定义echo_protocol，ranch会在每个新连接到达时，调用echo_protocol:start_link/4，生成我们的协议处理进程。参见官网示例。使用起来非常简单。 四. 结构ranch的进程结构如下： ranch_server:维护全局配置信息，整个ranch_app唯一，由多个listener共享。通过ets维护一些配置信息和核心进程的Pid信息，格式\\{\\{Opt, Ref\\}, OptValue\\}，Ref是listener名字。 ranch_listener_sup:由ranch:start_listener/6启动，其子进程有ranch_conns_sup和ranch_acceptors_sup，以rest_for_one策略启动，亦即一旦ranch_conns_sup挂了，ranch_acceptors_sup也将被终止，然后再依次重启。 ranch_acceptors_sup:由它创建监听套接字，并启动N个ranch_accepter执行accept操作(gen_tcp:accept本身支持多process执行)。 ranch_acceptor:执行loop，不断执行accept操作，将新conn socket的所属权交给ranch_conns_sup(gen_tcp:controlling_process)，通知其启动新protocol handle进行处理，并阻塞等待ranch_conns_sup返回。 ranch_conns_sup:维护当前所有连接，当新连接到达时，调用your_protocol:start_link/4创建新进程，之后将conn socket所属权交给新连接进程。当连接到达上限时，阻塞前来通知开启新连接的Acceptor进程。直到阀值提高，或有其它连接断开，再唤醒这些Acceptor。ranch的实际最大连接数 = max_conns + NAcceptor。 your_protocol开发者定义protocol，当有新连接到达时，将调用your_protocol:start_link/4启动新进程，之后的处理交予开发者。 五. 其它 对于不需要接收其它进程消息的进程，应该定义通过receive清理进程信箱，避免意料之外的消息一直堆积在信箱中。见ranch_accepter.erl。 rest_for_one，实现更加强大灵活的监督者。 ranch将网络的退出方式(brutal_kill，Timeout，infinity等)，交给开发者定制，而不放在框架中。 注意套接字所属权的转移：ranch_acceptor -&gt; ranch_conns_sup -&gt; your_protocol。 proc_libranch中多处用到了proc_lib启动进程，proc_lib是OTP进程的基石，所有OTP behaviour进程都通过proc_lib来创建新进程。 proc_lib的使用方法： proc_lib:start_link(M, F, A)启动一个符合OTP规范的进程 在M:F(A)中，通过proc_lib:init_ack(Parent, Ret)来告诉父进程自身已经初始化完成，此时proc_lib:start_link/3方才返回 如果进程本身为OTP进程，此时可通过gen_server:enter_loop(Module, Opts, State)来进入OTP进程的主循环 proc_lib使用情形： 为了让非OTP进程，能够以OTP规范启动，这样才能融入监督树中并被正确重启。如gen_server:start_link最终也通过proc_lib:start_link来启动进程。见ranch_conns_sup.erl。 让OTP进程在init()中进行消息处理，本来在init未返回之前，进程还未初始化完成，这个时候进程处理消息，会陷入死锁，但通过proc_lib:init_ack/2可以先让本进程伪初始化完成，然后进行消息处理，最后通过gen_server:enter_loop进入gen_server主循环。","tags":[{"name":"erlang","slug":"erlang","permalink":"http://wudaijun.com/tags/erlang/"}]},{"title":"博客开始使用Hexo","date":"2015-09-18T16:00:00.000Z","path":"2015/09/use-hexo-rebuild-blog/","text":"最近又开始想在博客上实现自己一直想要的摘要功能，然后倒腾jekyll，本来也没有前端基础，就博客系统而言，对我来说，简单就好，能专注写东西。但是发现jekyll偏离了这个宗旨，缺乏成熟的主题机制，可定制性太强，学习成本高。然后发现了这个Hexo主题，觉得就是自己想要的功能。最终抛弃了jekyll，投向Hexo。 关于Hexo的安装和使用说明，参看官方中文文档。 Hexo由node.js编写，不像jekyll被Github原生支持，因此它需要本地生成html文件后，再上传到Github。不像jekyll，你的md文件，生成的html，jekyll配置，都在一个仓库中，用起来省心。Hexo在Git上存放的只是生成好的页面，像我经常切换电脑写博客，因此还需要维护： Hexo主题：Hexo的主题像vim一样，都是插件式的，因此独立出来维护完善 source目录：原始的md文件和资源文件，以便随时随地都可以编辑文档 其它文件: 如我将资源文件都放在assets目录下，因此需要生成时通过脚本将assets拷贝到public下 现在我是直接把整个Hexo放在Git上，只能说懒人有懒办法了。","tags":[{"name":"tool","slug":"tool","permalink":"http://wudaijun.com/tags/tool/"}]},{"title":"开发笔记(2) 服务器Lua战斗","date":"2015-09-15T16:00:00.000Z","path":"2015/09/erlang-server-design2-erlang-lua-battle/","text":"服务器战斗系统是自动战斗的，没有玩家实际操作，因此实际上是一份客户端的Lua战斗代码，这里讨论如何在Erlang中植入Lua代码。 1. Port Driver最开始，出于简单考虑，我使用Port Driver的方式来挂接战斗模块，使用erlualib库，通过luam:one_call执行调用，进行了简单的时间统计，其中new_state&lt;1ms，dostring: 600ms，call: 20-300ms。 由于是3D+NvN的战斗，整个Lua代码跑起来还是很耗时的，跑一场战斗需要接近1s的时间。由于Port Driver中的Lua代码是在虚拟机调度线程上下文中执行的，而Erlang虚拟机无法对原生代码进行公平调度，这会使在Lua代码执行期间，该调度器上其它任务都被挂起，得不到正常调度。 2. 异步nif为了避免阻塞调度，Port driver是行不通的，我们还剩两种方案： 用Ports，将战斗独立为一个操作系统进程 异步nif 考虑到尽量利用Erlang Node以及以后手动PVP的可能性，我选择了方案二，而刚好同事写了一个异步nif库，也就拿来测试了。所谓异步nif，就是在nif内部提供一个C原生线程池，来做nif实际的工作，而Erlang虚拟机内只需要receive阻塞等待结果即可，Erlang层面的阻塞是可被调度的，也就是不会影响到节点上其它进程的公平调度。 简单介绍一下elua，elua内部提供一个线程池，每个线程都有自己的任务队列，同一个lua state上的操作将会被推送到同一个线程的任务队列中(通过简单hash)，以保证lua state不被并发。elua使用和定制都非常灵活，可以很轻松地添加nif接口和自定义数据类型的序列化。 3. 序列化数据在erlualib中，数据序列化是在erlang层完成的，erlang层通过lua:push_xxx来将基本数据(bool,integer,atom)一个个压入Lua栈，每一次push操作，都是一次port_command，而战斗入口的数据是比较繁杂的，英雄成长，技能，装备属性等等，涉及很多key-value，一来是序列化效率低，二来是这种数据结构不能兼容于客户端。同一套战斗入口数据，最好能同时用于服务器和客户端的战斗模块。 因此在elua中，我选择使用protobuf，通过二进制传输战斗入口数据，这个二进制流也可以传输给客户端，用于支持重放。 4. 进程池由于每场战斗是独立的，原则上对lua state是没有依赖的，事先分配一个lua state池，将耗时的dostring操作提前完成，每场战斗取出一个可用的lua state，然后spawn一个battle_worker进程来跑战斗，跑完之后将战斗结果cast回逻辑进程，进行后续逻辑处理。这样receive阻塞放在battle_worker中，实际Lua代码执行由elua线程池完成，对逻辑进程来说，是完全异步的。 受限于elua内部的C线程(称为worker)和CPU核心数的多少，并不是erlang process越多，战斗就跑得越快，当战斗请求过多时，请求被阻塞在elua内部各个worker的任务队列中。并且spawn的process不够健壮，也没有重启机制。显然我们应该让worker process常驻，并且通过gen_server+sup实现，worker process的个数可以刚好等于elua worker的个数，这样process和worker可以直接保持一对一的关系，修改elua任务分配hash算法，让process[i]的战斗请求将分发到worker[i]的任务队列。这样我们只需把process的分配调度做好，elua即可高效地利用起来。每个process持有一个lua state，保证lua state不被并发。当战斗请求过多时，消息将阻塞在process的消息队列中，而不是elua worker的任务队列中。 另外，如果战斗模块负荷较重，可以将elua线程池的大小设为Erlang虚拟机可用的CPU个数-1，这样即使elua所有线程忙碌，也不会占用全部的CPU，进一步保证节点其它进程得到调度。 5. 无状态服务到这里，我们讨论的都是如何将Lua代码嵌入在逻辑服务器中，如pvp_server，这样做实际上还有两点隐患： 多个pvp_server不能有效地利用同一个pvp_node资源，因为它们具有各自的worker proces pool 我们都假设elua和Lua战斗代码是足够健壮的，虽然Lua代码本身的异常可以通过lua_pcall捕获，但是Lua虚拟机本身的状态异常，如内存增长，仍然是不稳定的因素，可能会影响到整个pvp_node的逻辑处理 因此，将所有Lua战斗相关的东西，抽象到一个battle_node上，才是最好的方案，battle_node本身没有状态，可以为来自不同ServerId，不同模块的战斗请求提供服务，battle_node上有唯一的battle_server，动态管理该节点上的battle_worker process，并且分发任务，battle_server本身不属于任何一个ServerId。battle_worker由sup监控，并且在启动和挂掉时，都向battle_server注册/注销自己。 battle_server仍然需要向cluster_server注册自己，只不过不是以逻辑Server：{NodeType,ServerId,Node,Pid}的方式，而是以服务的方式：{ServiceName,_,Node,ServicePid}注册自己，cluster_server需要为Service提供一套筛选机制，在某个服务的所有注册节点中，选出一个可用节点:cluster_server:get_service(ServiceName)。 再来看看整个异常处理流程： lua代码错误: lua_pcall捕获 -&gt; Erlang逻辑层的battle_error battle_worker crash: 向battle_server注销自己 -&gt; battle_worker_sup重启 -&gt; 重建lua state -&gt; 向battle_server重新注册自己 battle_server crash: 终止所有battle_worker -&gt; 向cluster_server注销自己 -&gt; battle_server_sup重启 -&gt; 重新创建所有battle_worker -&gt; 向cluster_server重新注册 elua crash: battle_node crash -&gt; 该节点不可用 -&gt; 外部请求仍然可能路由到该节点 -&gt; 战斗超时 -&gt; cluster_server检测到(节点心跳机制)该节点不可响应 -&gt; 在集群中删除该节点 -&gt; 外部请求路由到其它可用节点 并且整个战斗系统的伸缩性很强，可以通过简单添加机器来缓解服务器战斗压力。 6. Lua代码热更这个是Lua的强项，直接通过elua再次dofile Lua入口文件即可，但是要保证该Lua入口不具备副作用，如对一些全局符号进行了改写，否则下一次直接dofile，将叠加这种副作用从而导致代码异常。如果有一些全局初始化操作，应该单独抽离出来，放在另一个Lua文件中，只在创建Lua虚拟机时执行。 另一种热更方案是，每次都重新创建一个Lua虚拟机，这样可以保证每次热更后的Lua虚拟机状态都得以重置恢复。 最重要的是，这一切，所有外部请求来说，都是透明的。","tags":[{"name":"erlang","slug":"erlang","permalink":"http://wudaijun.com/tags/erlang/"}]},{"title":"Erlang Ports","date":"2015-08-23T16:00:00.000Z","path":"2015/08/erlang-port/","text":"OverViewErlang外部调用的几种方式： 外部接入(OS进程级)： Ports: 用C实现的可执行程序，以Port的方式与Erlang交互。 C Nodes: 用C模拟Erlang Node行为实现的可执行程序。 Jinterface: Java和Erlang的通讯接口。 Network: 通过自定义序列化格式与Erlang节点网络交互，如bert-rpc 内部接入(和虚拟机在同一个OS进程内)： BIF: Erlang大部分BIF用C实现，如erlang:now，lists:reverse等 Port Driver: 以链接库方式将Port嵌入虚拟机，也叫Linkin Driver NIF: 虚拟机直接调用C原生代码 下面主要理解常用的三种：Ports, Port Driver, NIF。 Ports 图一. Ports 通信模型 Port是连接外部程序进程和Erlang虚拟机的桥梁，外部进程通过标准输入输出与Erlang虚拟机交互，并运行于独立的地址空间。 从操作系统的角度看，外部程序和Erlang虚拟机都是独立允许的进程，只不过外部程序的标准输入输出与Erlang虚拟机对接在了一起而已。因此外部程序可以通过read(0, req_buf, len)来获取虚拟机发出的指令，也可通过write(1, ack_buf, len)来发出响应。当外部程序崩溃了，Erlang虚拟机可以检测到，可以选择重启等对应策略。由于两者在不同的地址空间，通过标准IO交互，因此外部程序的崩溃不会影响到Erlang虚拟机本身的正常运行。 每个Port都有一个owner进程，通常为创建Port的进程，当owner进程终止时，Port也将被自动关闭。Ports使用示例参考Ports。 Port的优势在于隔离性和安全性，因为外部程序的任何异常都不会导致虚拟机崩溃，并且Erlang层通过receive来实现同步调用等待外部程序响应时，是不会影响Erlang虚拟机调度的。至于Port的缺点，主要是效率低，由于传递的是字节流数据，因此需要对数据进行序列化反序列化，Erlang本身针对C和Java提供了对应的编解码库ei和Jinterface。 Port Driver 图二. Port Driver 通信模型 从Erlang层来看，端口驱动和普通端口所体现的行为模式一样，收发消息，注册名字，并且共用一套Port API。但是端口驱动本身是作为一个链接库运行于Erlang虚拟机中的，也就是和Erlang虚拟机共享一个操作系统进程。 Port Driver分为静态链接和动态链接两种，前者和虚拟机一起编译，在虚拟机启动时被加载，后者通过动态链接库的方式嵌入到虚拟机。出于灵活性和易用性的原因，通常使用后者。 虚拟机和Port Driver的交互方式与Port一样，Port和Port Driver在Erlang层表现的语义一致。 Port Driver通过一个driver_entry结构体与虚拟机交互，该结构体注册了driver针对各种虚拟机事件的响应函数。skynet挂接service的思想大概也继承于此。driver_entry结构体主要成员如下： typedef struct erl_drv_entry { // 当链接库被加载(erl_ddll:load_driver/2)时调用，同一个链接库的多个driver实例来说，只调用一次 int (*init)(void); // 当Erlang层调用erlang:open_port/2时调用，每个driver实例执行一次 ErlDrvData (*start)(ErlDrvPort port, char *command); // 当Port Driver被关闭(erlang:port_close/1,owner进程终止,虚拟机停止等)时执行 void (*stop)(ErlDrvData drv_data); // 收到Erlang进程发来的消息(Port ! {PortOwner, {command, Data}} or erlang:port_command(Port, Data)) void (*output)(ErlDrvData drv_data, char *buf, ErlDrvSizeT len); // 用于基于事件的异步Driver 通过erl_driver:driver_select函数进行事件(socket,pipe,Event等)监听 void (*ready_input)(ErlDrvData drv_data, ErlDrvEvent event); void (*ready_output)(ErlDrvData drv_data, ErlDrvEvent event); // Driver名字 用于open_port/2 char *driver_name; // 当Driver被卸载时调用(erl_ddll:unload_driver/1)，和init对应。仅针对动态链接Driver void (*finish)(void); // 被erlang:port_control/3(类似ioctl)触发 ErlDrvSSizeT (*control)(ErlDrvData drv_data, unsigned int command, char *buf, ErlDrvSizeT len, char **rbuf, ErlDrvSizeT rlen); // Driver定义的超时回调，通过erl_driver:driver_set_timer设置 void (*timeout)(ErlDrvData drv_data); // output的高级版本，通过ErlIOVec避免了数据拷贝，更高效 void (*outputv)(ErlDrvData drv_data, ErlIOVec *ev); // 用于基于线程池的异步Driver(erl_driver:driver_async) 当线程池中的的任务执行完成时，由虚拟机调度线程回调该函数 void (*ready_async)(ErlDrvData drv_data, ErlDrvThreadData thread_data); // 当Driver即将关闭时，在stop之前调用 用于清理Driver队列中的数据(?) void (*flush)(ErlDrvData drv_data); // 被erlang:port_call/3触发 和port_control类似，但使用ei库编码ETerm ErlDrvSSizeT (*call)(ErlDrvData drv_data, unsigned int command, char *buf, ErlDrvSizeT len, char **rbuf, ErlDrvSizeT rlen, unsigned int *flags); // Driver 监听的进程退出信号(erl_driver:driver_monitor_process) void (*process_exit)(ErlDrvData drv_data, ErlDrvMonitor *monitor); } ErlDrvEntry; 该结构体比较复杂，主要原因是Erlang Port Driver支持多种运行方式： 运行于虚拟机调度线程的基本模式 基于select事件触发的异步Driver 基于异步线程池的异步Driver 三种模式的示例参考Port Driver，How to Implement a Driver，Driver API接口文档：erl_driver。Erlang虚拟机提供的异步线程池可通过+A选项设置。 端口驱动的主要优势是效率高，但是缺点是链入的动态链接库本身出现内测泄露或异常，将影响虚拟机的正常运行甚至导致虚拟机崩溃。将外部模块的问题带入了虚拟机本身。对于耗时较长或阻塞的任务，应该通过异步方式设计，避免影响虚拟机调度。 NIFNIF是Erlang调用C代码最简单高效的方案，对Erlang层来说，调用NIF就像调用普通函数一样，只不过这个函数是由C实现的。NIF是同步语义的，运行于调度线程中，无需上下文切换，因此效率很高。但也引出一个问题，对于执行时间长的NIF，在NIF返回之前，调度线程不能做别的事情，影响了虚拟机的公平调度，甚至会影响调度线程之间的协作。因此NIF是把双刃剑，在使用的时候要尤其小心。 Erlang建议的NIF执行时间不要超过1ms，针对于执行时间长的NIF，有如下几种方案： 分割任务，将单次长时间调用切分为多次短时间调用，再合并结果。这种方案显然不通用 让NIF参与调度。在NIF中恰当时机通过enif_consume_timeslice汇报消耗的时间片，让虚拟机确定是否放弃控制权并通过返回值通知NIF(做上下文保存等) 使用脏调度器，让NIF在非调度线程中执行 Erlang默认并未启用脏调度器，通过--enable-dirty-schedulers选项重新编译虚拟机可打开脏调度器，目前脏调度器只能被NIF使用。 关于脏调度器，NIF测试与调优，参考： siyao blog nifwait bitwise(其中的PDF质量很高) Port Driver和NIF与虚拟机调度密切相关，想要在实践中用好它们，还是要加深对Erlang虚拟机调度的理解，如公平调度，进程规约，调度器协同等。再来理解异步线程池，脏调度器的存在的意义以及适用场景。另外，Port Driver和NIF还有一种用法是自己创建新的线程或线程池(Driver和NIF也提供了线程操作API)，我们项目组也这么用过，这基本是费力不讨好的一种方案，还极易出错。","tags":[{"name":"erlang","slug":"erlang","permalink":"http://wudaijun.com/tags/erlang/"}]},{"title":"开发笔记(1) cluster server","date":"2015-08-05T16:00:00.000Z","path":"2015/08/erlang-server-design1-cluster-server/","text":"服务发现在游戏服务器中，通常有一些功能本身非常内聚，甚至是无状态的，在这种时候，我们应该将其单独地做成一个服务，而不是嵌入到GameServer中，这种思想就是所谓的服务(microservice)思想。 服务发现本身可以看做是一个业务独立的”特殊服务”，它用于逻辑服务的注册/查找/配置信息共享。通常由如下三部分组成： 一个强一致，高可用的服务存储 提供服务注册和服务健康状态监控的能力 提供查找服务，连接服务的能力 在分布式领域中，服务发现是一个非常实用和通用的组件，并且已经有一些比较成熟的组件，如zookeeper，etcd等。服务发现组件的好处有很多：微服理念，为负载均衡，灾难恢复提供基础。更多应用场景，可参见etcd的这篇文章。 cluster_server先来谈谈我们的集群划分，基于我们的服务器设计，整个集群由N个node组成，node可根据其职责来划分，如player_node，master_node，pvp_node，每个node上跑对应类型的进程，每种node可有多个。其中master_node负责监控/管理所有业务逻辑node，新加入的node只需和master_node连接，这种粒度的划分本身是有利弊的，我们在之后的开发中对它进行了改进，就我们本身cluster_server的设计初衷而言，本质职责是没变的。 在GS中，我们在查找某个服务时，如某个PlayerId对应的player_server，我无需知道这个player_server位于哪个player_node上，甚至无需知道是否在本台物理机上，我只需获取到这个player_server的Pid，即可与其通信。显然地，为了将服务的使用者和服务本身解耦，我们需要维护这样一个 PlayerId -&gt; player_server Pid 的映射表，并且这个表是集群共同访问的，这也就是服务发现的基本需求。 服务注册/查找，状态共享在Erlang中，我们的服务本身通常是一个进程，即Pid，我们可以用分布式数据库mnesia实现一个简易的cluster_server，它处理的一件事是：根据不同Key值(Erlang Term)取出对应服务的Pid。cluster_server本身是节点唯一的进程，用于和mnesia交互，实现服务注册/服务查找。为了方便使用，我将Key定义为一个type加一个id，表的初步定义如下： -record(cluster_(TYPE)_process, {id, node, pid, share}). % TYPE: pvp player 等 share: 用于状态共享 基于这张mnesia表，可以实现如下功能： 服务注册：通过事务保证写入的原子性，将不同类型的服务写入对应的表中 服务查找：根据不同的类型访问不同的表，用mnesia的ram_copies来优化读取，使读取像本地ets一样快 服务注销：在服务不可用或被终止时，通过事务删除对应表条目 状态共享：通过share字段可以获知服务的当前状态或配置 服务创建，负载均衡上面实现了最简单的服务注册/查找机制，服务本身的创建和维护由服务提供者管理，在GS集群中，通常我们是希望所有的服务被统一监控和管理，比如某个服务节点挂了，那么上面的所有服务将被注销(主动注销/失联注销)，这个时候应该允许使用者或master重启该服务，将该服务分配到其它可用节点上。 因此我们还需要维护可用节点表，用于服务创建： -record(cluster_(type)_process, {id, node, pid, share}). 通过share字段，可以获取到节点当前的状态信息，比如当前负载，这样做负载均衡就比较容易了，将服务创建的任务分发到当前负载较轻的节点即可。 服务监控，灾难恢复对于关键的服务或者是无状态的服务，可以通过master来监控其状态，在其不可用时，对其进行选择性恢复。比如当某服务所在物理机断电或断网，此时上面的服务都来不及注销自己，通过monitor_node/2，master会在数次心跳检测失败后，收到nodedown消息，此时master节点可以代为注销失联结节点上所有服务，并且决定这些服务是否需要重建在其它节点上。 注意事项一致性问题 如果不使用事务，服务A可能覆写/误删服务B 服务注册信息同步到其它节点的时间差，可能导致的不同步(服务的写入者无论是服务的发起方还是服务本身，都会存在这个问题)。 解决方案： 使用事务 这是最”简单”的方案，主要是性能问题，特别是游戏的波峰时段，这种延迟会扩散 串行化服务管理 将服务的查找或者是注册/注销，交由一个Proxy来做(经由某种分组规则ServerId)，则可使用脏读写，避免一致性问题。但是会有单点，并且弱化了分布式的特性。 服务查询 将表不添加本地拷贝，直接使用remote类型表进行访问(事务)，在本节点对Pid进行保存，采用某种机制来确保缓存Pid的正确性(如monitor) 退化ETS 将一些频繁访问和使用的服务退化为ETS(特别是player和agent)，主要目的是减轻mnesia压力(28原则)，使mnesia可以安全的使用事务。但这部分服务也失去了使用mnesia的优势，个人觉得不如方案3。 全联通问题mnesia必须建立在全联通网络上，在节点数量超过10个时，就需要关注这个问题了。 解决方案： 可为节点分组(如5个一组)，设定代理节点，由代理节点组成mnesia集群。","tags":[{"name":"erlang","slug":"erlang","permalink":"http://wudaijun.com/tags/erlang/"},{"name":"distribution","slug":"distribution","permalink":"http://wudaijun.com/tags/distribution/"}]},{"title":"kbengine 源码导读(二) 加载流程","date":"2015-07-23T16:00:00.000Z","path":"2015/07/kbengine-study2/","text":"一. 登录流程注册 Unity3d:CreateAccount Loginapp:reqCreateAccount -&gt; dbmgr:reqCreateAccount -&gt; Loginapp:onReqCreateAccountResult -&gt; Client:onReqCreateAccountResult 登录 Step1 Unity3d:login Loginapp:login -&gt; dbmgr:onAccountLogin //检查登录 将登录结果返回给Loginapp -&gt; Loginapp:onLoginAccountQueryResultFromDbmgr //转发给BaseappMgr分配Baseapp -&gt; BaseappMgr:registerPendingAccountToBaseapp //将客户端分配到当前负载最低的Baseapp上 并返回该Baseapp的Ip Port -&gt; onLoginAccountQueryBaseappAddrFromBaseappmgr //将Baseapp的Ip Port转发给客户端 -&gt; Client:onLoginSuccessfully 登录 Step2 Unity3d:loginGateway //尝试在指定Baseapp上登录 Baseapp:loginGateway //检查登录 处理重复登录 向数据库查询账号详细信息 dbmgr:queryAccount //查询账号详细信息 返回给Baseapp Baseapp:onQueryAccountCBFromDbmgr //创建账号的Proxy并传入客户端的mailbox(用于和客户端交互)，Demo中的Account.py即继承于KBEngine.Proxy。 获角 选角 创角 Unity3d的reqAvatarList selectAvatarGame reqCreateAvatar 都将直接转到Account.py中对应的相应函数上，KBEngine.Proxy已经封装了和客户端通讯的方法(通过Mailbox)。 二. 地图创建 Baseapp启动，会回调到Python脚本层的onBaseAppReady(base/kbengine.py) 第一个Baseapp启动时，在本Baseapp上创建世界管理器spaces Entity(Baseapp:createBaseLocally) 定义于spaces.py spaces读取配置文件data/d_spaces.py，为每一个Space Entity创建一个SpaceAlloc，通过定时器分批次调用SpaceAlloc.init创建Space Entity(一秒回调创建一个) SpaceAlloc.init通过KBEngine.CreateSpaceAnyWhere()完成 Baseapp:CreateSpaceAnyWhere()会转发给BaseappMgr，最终落在当前负载最轻的Baseapp上，通过CreateEntity完成Space Entity创建 创建完成后，回调到发起方Baseapp:CreateSpaceAnywhereCallback() 最终回调到Python层SpaceAlloc.py:onSpaceCreatedCB()注意，上面提到的Space Entity并不是真正的Space，而是Baseapp用于操作Space的一个句柄，真正的Sapce需要挂在Cellapp上，在srcipts/base/Space.py中完成真正的Space创建： Space.py:_init_()中，通过Baseapp:CreateInNewSpace()创建真正的Space，之后读取该Space上需创建的所有 Entity(配置在scripts/data/d_spaces_spawns中)，等待其上面的Entity被创建 Baseapp:CreateInNewSpace()将请求转发给CellappMgr，后者会将请求分发到当前负载最轻的Cellapp上，Cellapp:onCreateInNewSpaceFromBaseapp()完成Space创建，回调Baseapp:OnEntityGetCell() 注意，此时cell/Space.py:_init_()被调用，开始加载真正的几何数据和寻路相关，回调到Baseapp:OnEntityGetCell() Baseapp:OnEntityGetCell()判断该Entity是否是客户端，如果是则需要通知客户端(Baseapp::onClientEntityEnterWorld)，之后回调脚本Space.py:OnGetCell() 至此，地图创建完成。 三. 生成NPC/Monster对于NPC/Monster，是先创建其出生点，再由出生点创建真正的NPC/Monster 接上面Space的Cell和Base部分均创建完成后，base/Space.py:OnGetCell()中，注册一个定时器，开始创建该Space上面的所有NPC/Monster的SpawnPoint，每0.1秒创建一个 base/SpawnPoint.py中，创建其Cell部分 cell/SpawnPoint.py中，通过createEntity创建其对应的真正的NPC/Monster 四. Entity (实体)Entity是服务器与客户端交互的一切实体的总称，包括：账号，角色，NCP，Monster，公会，等等。Entity通过 .def 来定义自己的属性和方法，指定属性和方法的作用域，即(Base, Cell, Client)的访问权限。因此C/S之间的消息协议实际上只是针对于Entity的远程调用。所以KBEngine本身没有消息协议一说，所有业务逻辑都围绕着Entity展开，通过.def来维护。 参见： http://kbengine.org/cn/docs/programming/entitydef.htmlhttp://kbengine.org/cn/docs/configuration/entities.html","tags":[{"name":"kbengine","slug":"kbengine","permalink":"http://wudaijun.com/tags/kbengine/"}]},{"title":"kbengine 源码导读(一) 网络底层","date":"2015-07-22T16:00:00.000Z","path":"2015/07/kbengine-study1/","text":"一. network部分EndPoint: 抽象一个Socket及其相关操作，隔离平台相关性。 TcpPacket: 代表一个TCP包，这个包只是recv收到的字节流，并不是上层协议中的消息(Message)。 MsgHandlers: 每个MessageHandler类对应一个消息的处理。MsgHanders维护MsgId -&gt; MsgHandler的映射。 Channel: 抽象一个Socket连接，每个EndPoint都有其对应的Channel，它代表和维护一个Socket连接，如缓冲Packet，统计连接状态等。提供一个ProcessPackets(MsgHanders* handers)接口处理该Channel上所有待处理数据。 EventPoller: 用于注册和回调网络事件，具体的网络事件由其子类实现processPendingEvents产生，目前EventPoller有两个子类: EpollPoller和SelectorPoller，分别针对于Linux和Windows。通过bool registerForRead(int fd, InputNotificationHandler * handler);注册套接字的可读事件，回调类需实现InputNotificationHandler接口。 EventDispatcher: 核心类，管理和分发所有事件，包括网络事件，定时器事件，任务队列，统计信息等等。它包含 EventPoller Tasks Timers64 三个组件，在每次处理时，依次在这三个组件中取出事件或任务进行处理。 ListenerReceiver/PacketReceiver: 继承自InputNotificationHandler，分别用于处理监听套接字和客户端套接字的可读事件，通过bool registerReadFileDescriptor(int fd, InputNotificationHandler * handler); 注册可读事件。 NetworkInterface: 维护管理监听套接字，创建监听套接字对应的ListenerReceiver，并且通过一个EndPoint -&gt; Channel的Map管理所有已连接套接字，提供一个processChannels(MsgHandlers* handers)接口处理所有Channel上的待处理数据。这一点上，有点像NGServer:ServiceManager。 二. LoginApp 启动流程main: 所有App都有一致的main函数，生成组件唯一ID，读取配置等，转到kbeMainT kbeMainT: 生成公钥私钥，调试相关初始化 创建单例EventDispatcher和NetworkInterface 创建LoginApp，并传入EventDispatcher和NetworkInterface 调用LoginApp:run() LoginApp:run(): 调用基类ServerApp:run()，后者调用 EventDispatcher:processUntilBreak() 开始处理各种事件 LoginAppInterface: 存放和注册LoginApp响应的所有消息的消息回调，参见loginapp_interface.h。通过LoginAppInterface::messageHandlers即可导出消息处理类 细节流程: NetworkInterface构造函数中，创建ListenSocket和ListenerReceiver，注册到EventDispatcher 当有新连接到达时，EventDispatcher触发ListenerReceiver:handleInputNotification handleInputNotification创建新套接字的Channel，并将Channel注册到NetworkInterface 新Channel初始化时，创建新套接字对应的PacketReceiver，并注册到EventDispatcher 在LoginApp::initializeEnd中，添加了一个TIMEOUT_CHECK_STATUS Timer 该Timer触发时，会最终调用networkInterface().processChannels() 处理各Channel的消息，目前该Timer是20mss","tags":[{"name":"kbengine","slug":"kbengine","permalink":"http://wudaijun.com/tags/kbengine/"}]},{"title":"C链接模型_符号解析","date":"2015-07-19T16:00:00.000Z","path":"2015/07/c-linker-model/","text":"链接器的工作主要分为两个阶段：符号解析和重定位。本文简单介绍符号解析过程。 符号解析的功能是将每个模块符号引用绑定到一个确切的符号定义。 1. 符号分类 全局符号：非静态全局变量，非静态函数 外部符号：定义于其它模块，而被本模块引用的全局变量和函数 本地符号：静态变量(包括全局和局部)，静态函数 对于静态局部变量，编译器会为其生成唯一的名字。如x.fun1，x.fun2。本地符号对链接器来说是不可见的。 2. 符号决议当编译器遇到一个不是本模块定义的符号时，会假设该函数由其它模块定义，并生成一个链接器符号表条目，交由链接器处理。如果链接器在它的任何输入模块都没有找到该符号，会给出一个类似undefined reference to &#39;xxx&#39;的链接错误。而如果链接器在输入模块中找到了一个以上的外部符号定义，这个时候就需要链接器进行符号决议，链接器对多个外部符号定义可能并不报错甚至警告，而是按照它的规则去选择其中一个符号定义。 链接器将各个模块输出的全局符号，分类为强符号和弱符号： 强符号：函数和已初始化的全局变量 弱符号：为初始化全局变量 根据强弱符号的定义，链接器按照下面的规则处理多重定义的符号： 规则1：不允许有多个强符号定义 规则2：如果有一个强符号和多个弱符号，那么选择强符号 规则3：如果有多个弱符号，那么从这些弱符号中选择sizeof大的那个，如果大小相同，则选择先链接的那个 上面的规则是很多链接错误的根源，因为编译器在决议时可能默默地替你作出了决定，你并不知晓。根据上面的规则，可以引出下面几个经典例子： 例1： // in lib1.c int x; void f() { x = 1235; } // in main1.c #include&lt;stdio.h&gt; void f(void); int x = 1234; int main(void) { f(); printf(&quot;x=%d\\n&quot;, x); return 0; } 上面的代码中，main函数printf输出： x=1235。因为链接器通过规则2决议符号x的定义为main.c中的强符号定义，而lib.c的作者并不知情，他对x的使用和修改影响到了main.c。这种交互修改，相互影响将会很复杂，因为大家都以为自己在做对的事情，在用对的变量。而整个决议过程，链接器悄无声息地完成了。 例2： // in lib2.c double x; void f() { x = -0.0; } // in main2.c #include&lt;stdio.h&gt; void f(void); int x = 1234; int y = 1235; int main() { f(); printf(&quot;x=0x%x y=0x%x \\n&quot;, x, y); return 0; } 这种情况下，程序得到输出： x=0x0 y=0x80000000，而链接器(gcc ld)也终于给出一条警告： ld: warning: tentative definition of &#39;_x&#39; with size 8 from &#39;obj/Debug/lib2.o&#39; is being replaced by real definition of smaller size 4 from &#39;obj/Debug/main2.o&#39; 链接器决议的是符号地址，而相邻的全局变量可能在.data段中的内存地址也相邻，因此也就引发了更复杂的问题。这一点和栈溢出很像，但是比栈溢出更复杂，因为问题出在多个模块之间，而不是在一个函数内部。 例3： // in lib3.c struct { int a; int b; } x; void f() { x.a = 123; x.b = 456; printf(&quot;in f(): sizeof(x)=%d, (&amp;x)=0x%08x\\n&quot;, sizeof(x), &amp;x); } // in main3.c #include&lt;stdio.h&gt; void f(void); int x; int y; int main() { f(); printf(&quot;in main(): sizeof(x)=%d, (&amp;x)=0x%08x, (&amp;x)=0x%08x, x=%d,y=%d \\n&quot;, sizeof(x), &amp;x, &amp;y, x, y); return 0; } 程序输出： in f(): sizeof(x)=8, (&amp;x)=0x02489018 in main(): sizeof(x)=4, (&amp;x)=0x02489018, (&amp;y)=0x02489020, x=123,y=0 始终记住，外部符号决议的是地址，因此无论lib3.c和main3.c中，符号x地址都是唯一的，无论其被定义了几次。其次sizeof是编译器决议，与链接无关，编译器只看得到本模块的定义或声明。最后，由于符号x决议到lib3.c中的x，其size是8，因此main3.c中的y的地址比x大8，这是由链接器将lib3.o和main3.o合并后填入可执行文件的.data段的。因此y是无关变量，被初始化为0，注意和例2的区别。 3. 总结由于符号决议容易引发的种种问题，我们在写C的时候应注意： 尽量用static属性隐藏变量和函数在模块内的声明，就像在C++中尽量用private保护类私有成员一样。 少定义弱符号，尽量初始化全局变量，这样链接器会根据规则1给出多个符号定义的错误。 为链接器设置必要选项，如gcc的 -fno-common，这样在遇到多重符号定义时，链接器会给出警告。 4. C++的符号决议C++并不支持强弱符号同时存在，所有符号都只能有一个定义(函数重载通过改写函数符号来确保其唯一)，因此在很大程度上避免了C中的链接器困扰。","tags":[{"name":"c/c++","slug":"c-c","permalink":"http://wudaijun.com/tags/c-c/"}]},{"title":"rebar的热更","date":"2015-07-08T16:00:00.000Z","path":"2015/07/rebar-hotcode/","text":"由于项目开发中早早地用到了rebar，虽然rebar在很多方面都比自己构建原生OTP应用更方便，但是每一次修改，都需要重新编译，发布，启动，非常耗费时间，而rebar本身的upgrade又比较麻烦，是针对于版本发布的，不适合开发测试使用。 因此找到了一种基于.beam文件更新加载的方法，借鉴自mochiweb reloader。 mochiweb reloader每隔一秒检查一次已加载的所有模块(code:all_loaded())，遍历模块列表，检查其所在路径的变更状况，若模块在一秒内有变动，则通过code:load_file(Module)加载模块到运行时系统，执行热更。整个过程需要我们做的就是，将编译好的beam文件放到rebar rel对应的发布版本目录下，可通过code:all_loaded()查看各lib或app所在的发布路径，该发布路径是具有版本号的，但是由于我们在开发测试中暂时无需版本号控制，因此直接通过makefile将编译好的beam文件放到发布路径即可。 mochiweb reloader在加载Module后，会执行Module:test函数(如果该函数已导出)，可通过导出该函数完成一些升级时的处理。","tags":[{"name":"erlang","slug":"erlang","permalink":"http://wudaijun.com/tags/erlang/"}]},{"title":"Erlang supervisor","date":"2015-06-30T16:00:00.000Z","path":"2015/07/erlang-superviosr/","text":"一. 简介Supervisor(监督者)用于监督一个或多个Erlang OTP子进程，Supervisor本身是个behaviour，仅有一个Callback: init/1，该函数返回{ok, {ChildSpecList, RestartStrategy}}。ChildSpecs是ChildSpec列表， ChildSpec(子进程规范)： 指定要监控的子进程的所在模块，启动函数，启动参数，进程类型等等。格式为: 123456child_spec() = &#123;id =&gt; child_id(), % 一般为Child所在Module名 start =&gt; mfargs(), % &#123;Module, StartFunc, Arg&#125; restart =&gt; restart(), % permanent:任何原因导致进程终止都重启 | transiend:意外终止时重启 | temporary:永不重启 shutdown =&gt; shutdown(), % 终止子进程时给子进程预留的时间(毫秒) | brutal_kill 立即终止 | infinity 无限等待 用于Child也是supervisor的情况 type =&gt; worker(), % 子进程类型 worker:工作者 | supervisor:监督者 modules =&gt; modules()&#125; % 子进程所依赖的模块，用于定义热更新时的模块升级顺序，一般只列出子进程所在模块 RestartStrategy(重启策略): 定义子进程的重启方式，为三元组{How, Max, Within}: How: one_for_one: 仅对终止的子进程进行重启，不会影响到其他进程 one_for_all: 一旦有某个子进程退出，讲终止该监督者下其它所有子进程，并进行全部重启 rest_for_one: 按照ChildSpecList子进程规范列表中的定义顺序，所在在终止子进程之后的子进程将被终止，并按照顺序重启 simple_one_for_one: 这是一种特殊的监督者，它管理多个同种类型的子进程，并且所有子进程都通过start_child接口动态添加并启动，在监督者启动时，不会启动任何子进程。 Max: 在Within时间片内，最多重启的次数 Within: 时间片，以秒为单位 二. 重启机制子进程终止时，监督者会重启子进程，那么此时我们关心的是我们的State数据(假设我们的子进程是gen_server)，对于simple_one_for_one类型的监督者，经测试，监督者在重启Child的时候，会传入start_child时的初始化参数(该参数分为两部分，一部分是子进程规范中Arg指定的默认参数，以及supervisor:start_child传入的参数，将这两部分合并即为StartFunc最终收到的参数)。也就是说子进程终止时，我们的State数据丢失了。 考虑Player进程，它使用simple_one_for_one类型的监督者Player_sup，假设启动参数为PlayerId，在Player进程处理逻辑挂掉时，我们在terminate中将PlayerData落地，并做一些其它处理，如通知Agent。Player_sup在重启该Player进程时，会传入其上次传入的参数，即PlayerId，因此我们可以在init中重新加载玩家数据并通知Agent(Player进程重启后Pid会变化)。","tags":[{"name":"erlang","slug":"erlang","permalink":"http://wudaijun.com/tags/erlang/"}]},{"title":"Firefly 学习(二)","date":"2015-06-02T16:00:00.000Z","path":"2015/06/firefly-study2/","text":"一. GlobalObject每个节点(即一个FFServer)对应一个GlobalObject，存放该节点的节点信息和分布式信息。GlobalObject中包含多种组件，FFServer根据节点配置信息决定为节点创建哪些组件。这样分布式配置更为灵活，一个节点可以单一职责，也可以多种职责。GlobalObject包含的组件主要有： netfactory: 前端节点，对应netport字段，监听和管理客户端连接。 root: 分布式根节点，对应字段rootport remote: 分布式子节点，对应字段remoteport db: 数据库节点 简单介绍一下netfactory, root, remote 这三个组件，已经远程调用的实现机制。 1. 前端节点netfactory：前端节点netfactory为LibrateFactory(netconnect/protoc.py)，firefly网络层使用twisted，LibrateFactory即为twisted的协议工厂，同时也是网络层到逻辑层的纽带。LibrateFactory有如下成员： connmanager: 功能: 管理所有Connection，建立ConnID(transport.sessionno)到Conn的映射。 实现: ConnectionManager(netconnection/manager.py) dataprotocl: 功能: 消息编解码器，完成消息的编解码，提供pack/unpack/getHeadlength等接口。 实现: DataPackProtoc(netconnection/datapack.py) protocol: 功能: 负责处理收到的字节数据，解决粘包半包问题等，通过DataPackProtoc拿到消息ID(command)和消息数据(request)，调用factory.doDataReceived(self, command, requeset)将消息传给netfactory统一处理。 实现: LibrateProtocol(netconnection/protoc.py) service: 功能: netfactory上挂载的Service，也就是从网络层到逻辑层的入口，逻辑层在这个Service通道中注册响应函数，netfactory会在收到消息(doDataReceived)时，通过service.callTarget(commandID, conn, data)将消息交由service处理。 实现: 目前的netfactory上挂载的是netservice，netservice默认为CommandService(utils/services.py) 值得一提的是，LibrateProtocol在处理收到的字节流时(dataHandleCoroutine)，利用yield机制非常简洁高效地完成消息解码工作，使解码函数看起来只是在一个while True循环中，无需多次调用，也自然无需保存状态。当外部数据到达时，通过send(data)即可将数据送入dataHandleCoroutine，后者yield返回即可拿到data继续工作了。 另外，LibrateProtocol解析完一条消息后，通过调用factory.doDataReceived将消息交给netfactory，也就是交给逻辑层，由于LibrateProtocol并不知道逻辑层何时返回，因此factory.doDataReceived是一个异步调用，它返回一个Deffer对象，LibrateProtocol注册callback为写回函数safeToWriteData，当逻辑层返回处理结果时，即可将数据线程安全地响应给客户端。这个Deffer对象可以是响应函数(如netservice:handle_100)返回的，如果响应函数没有返回Deffer而是直接返回的响应数据response，将由service.callTarget创建一个Deffer，并且回调deffer.callback(response)，如果响应函数返回None，那么表示这个请求消息没有响应，service.callTarget直接返回None，LibrateProtocol也无需再为其注册safeToWriteData函数了。 注意，整个过程都是在单线程中跑的(reactor)，firefly中的每个节点都使用一个reactor，netfactory在FFServer(server/server.py)中传给reactor（如果该节点配置了netport），在FFServer启动时会启动reactor。 2. 分布式根节点rootfirefly使用twisted透明代理(Perspective Broker, 简称PB, 参见twisted官方文档)，屏蔽了分布式节点之间的通信机制和细节。在FFServer中，firefly为每一个根节点(具备rootport字段)创建一个PBRoot对象，PBRoot代表分布式根节点，它包含两个构件: childmanager: 功能: 管理该根节点下面的所有子节点对象(Child对象)，Child主要包含子节点名和子节点的远程调用对象的引用(通过它调用callRemote(函数名，参数)即可调用子节点函数，剩下的细节将由twisted透明代理来完成)。 实现: ChildManager(distributed/manager.py) service: 功能: 和netfactory一样，service用于挂载本节点提供的接口(用于其它节点调用)，firefly所有的节点都抽象出一个service用于管理本节点的接口，除了netfactory的netservice以外，其它节点的service均为Service对象，Service对象根据函数名而不是commandID来调用接口。 实现: Service(utils/services.py) 子节点在连接到根节点时，由子节点发起一个takeProxy的远程调用，参数为子节点名和其远程调用对象(继承自twisted.spread.pb.Referenceble)，触发PBRoot的remote_takeProxy，该函数记录该子节点和其远程调用对象)。之后根节点PBRoot可通过callChild(子节点名，函数名，参数)调用子节点函数。关键代码如下: 12345678910111213141516171819202122class PBRoot(pb.Root): def __init__(self,dnsmanager = ChildsManager()): self.service = None self.childsmanager = dnsmanager # 远程调用: 初始化子节点 def remote_takeProxy(self,name,transport): log.msg(&apos;node [%s] takeProxy ready&apos;%name) child = Child(name,name) self.childsmanager.addChild(child) child.setTransport(transport) self.doChildConnect(name, transport) # 远程调用: 调用本节点上实现的响应函数 def remote_callTarget(self,command,*args,**kw): data = self.service.callTarget(command,*args,**kw) return data # 调用子节点方法 def callChild(self,key,*args,**kw): return self.childsmanager.callChild(key,*args,**kw) 3. 分布式子节点remoteFFServer为每一个子节点(具备remoteport字段)创建N个RemoteObject对象(N为其根节点个数，即remoteport字段的元素个数)，globalobject.remote是一个map，通过remote[根节点名]可以得到连接到指定根节点的RemoteObject。为每一个根节点都创建一个RemoteObject的好处是：同样一个子节点，可以对不同的根节点提供不同的接口。 RemoteObject包含如下构件: _reference: 功能: 这就是前面提到的远程调用对象，继承自twisted.spread.pb.Referenceble，因此它支持远程调用，即callRemote方法。前提是要将该对象传给根节点。 实现: ProxyReference(distributed/reference.py) _factory: PBClientFactory实例，用于获取跟节点的远程调用对象(getRootOBject) _name: 节点名字 在RemoteObject.connect(self, addr)中，子节点连接到根节点时，需要先远程调用根节点的takeProxy函数，并将_reference和_name传给该函数作为参数，如此根节点的childmanager会记下该子节点及其远程调用对象。关键代码如下: 123456789101112131415161718192021222324252627282930313233class RemoteObject(object): &apos;&apos;&apos;远程调用对象&apos;&apos;&apos; def __init__(self,name): self._name = name self._factory = pb.PBClientFactory() self._reference = ProxyReference() self._addr = None def connect(self,addr): &apos;&apos;&apos;初始化远程调用对象&apos;&apos;&apos; self._addr = addr reactor.connectTCP(addr[0], addr[1], self._factory) self.takeProxy() def reconnect(self): &apos;&apos;&apos;重新连接&apos;&apos;&apos; self.connect(self._addr) def addServiceChannel(self,service): &apos;&apos;&apos;设置引用对象&apos;&apos;&apos; self._reference.addService(service) def takeProxy(self): &apos;&apos;&apos;向远程服务端发送代理通道对象 &apos;&apos;&apos; deferedRemote = self._factory.getRootObject() deferedRemote.addCallback(callRemote,&apos;takeProxy&apos;,self._name,self._reference) def callRemote(self,commandId,*args,**kw): &apos;&apos;&apos;远程调用&apos;&apos;&apos; deferedRemote = self._factory.getRootObject() return deferedRemote.addCallback(callRemote,&apos;callTarget&apos;,commandId,*args,**kw) 二. Service装饰器至此，除了db和master节点之外，普通分布式节点已经能够正常通讯并且实现远程调用，由于netfactory, root, remote每个组件都抽离出了service用于挂载响应函数，因此firefly在server/globalobject.py中，实现了几个简单的装饰器：netserviceHandle remoteserviceHandle rootserviceHandle，分别用于挂载netfactory，root，remote的响应函数： 12345678910111213def netserviceHandle(target): GlobalObject().netfactory.service.mapTarget(target) def rootserviceHandle(target): GlobalObject().root.service.mapTarget(target)class remoteserviceHandle: &apos;&apos;&apos; remoteserviceHandle装饰器需要一个参数，指出该接口提供给哪一个根节点使用 def __init__(self,remotename): self.remotename = remotename def __call__(self,target): GlobalObject().remote[self.remotename]._reference._service.mapTarget(target) 这样客户端不用再知道关于globalobject的实现细节，用起来就像上一篇博客中的例子一样简单，暴露给用户globalobject组件只有root和remote，用于实现子节点和父节点之间的远程调用。","tags":[{"name":"firefly","slug":"firefly","permalink":"http://wudaijun.com/tags/firefly/"},{"name":"python","slug":"python","permalink":"http://wudaijun.com/tags/python/"}]},{"title":"Twisted","date":"2015-05-28T16:00:00.000Z","path":"2015/05/twisted/","text":"前几天学习firefly游戏服务器框架，其底层用twisted实现，twisted是一个比较出名的python异步回调框架，将reactor回调模式运用到极致，并且也对传统回调所面临的一些问题提出了很好的解决方案。 我的twisted学习主要是基于Dave的\b系列博客的，\u0010英文原版在这里，看了前面比较基础的几章，根据这些文章做个阶段性总结。顺便附上官方文档。 一. reactortwisted的核心是reactor，而提到reactor不可避免的是同步/异步，阻塞/非阻塞，在Dave的第一章概念性介绍中，对同步/异步的界限有点模糊，关于同步/异步，阻塞/非阻塞可参见知乎讨论。而关于proactor(主动器)和reactor(反应堆)，这里有一篇推荐博客有比较详细的介绍。 就reactor模式的网络IO而言，应该是同步IO而不是异步IO。而Dave第一章中提到的异步，核心在于：显式地放弃对任务的控制权而不是被操作系统随机地停止，程序员必须将任务组织成序列来交替的小步完成。因此，若其中一个任务用到另外一个任务的输出，则依赖的任务（即接收输出的任务）需要被设计成为要接收系列比特或分片而不是一下全部接收。 显式主动地放弃任务的控制权有点类似协程的思考方式，reactor可看作协程的调度器。reactor是一个事件循环，我们可以向reactor注册自己感兴趣的事件(如套接字可读/可写)和处理器(如执行读写操作)，reactor会在事件发生时回调我们的处理器，处理器执行完成之后，相当于协程挂起(yield)，回到reactor的事件循环中，等待下一个事件来临并回调。reactor本身有一个同步事件多路分解器(Synchronous Event Demultiplexer)，可用select/epoll等机制实现，当然twisted reactor的事件触发不一定是基于IO，也可以由定时器等其它机制触发。 twisted的reactor无需我们主动注册事件和回调函数，而是通过多态(继承特定类，并实现所关心的事件接口，然后传给twisted reactor)来实现。关于twisted的reactor，有几个需要注意的地方： twisted.internet.reactor是单例模式，每个程序只能有一个reactor； 尽量在reactor回调函数尽快完成操作，不要执行阻塞任务，reactor本质是单线程，用户回调代码与twisted代码运行在同一个上下文，某个回调函数中阻塞，会导致reactor整个事件循环阻塞； reactor会一直运行，除非通过reactor.stop()显示停止它，但一般调用reactor.stop()，也就意味着应用程序结束； 二. twisted简单使用twisted的本质是reactor，我们可以使用twisted的底层API(避开twisted便利的高层抽象)来使用reactor: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# 示例一 twisted底层API的使用from twisted.internet import reactofrom twisted.internet import mainfrom twisted.internet.interfaces import IReadDescriptorimport socketclass MySocket(IReadDescriptor): def __init__(self, address): # 连接服务器 self.address = address self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) self.sock.connect(address) self.sock.setblocking(0) # tell the Twisted reactor to monitor this socket for reading reactor.addReader(self) # 接口: 告诉reactor 监听的套接字描述符 def fileno(self): try: return self.sock.fileno() except socket.error: return -1 # 接口: 在连接断开时的回调 def connectionLost(self, reason): self.sock.close() reactor.removeReader(self) # 当应用程序需要终止时 调用: # reactor.stop() # 接口: 当套接字描述符有数据可读时 def doRead(self): bytes = &apos;&apos; # 尽可能多的读取数据 while True: try: bytesread = self.sock.recv(1024) if not bytesread: break else: bytes += bytesread except socket.error, e: if e.args[0] == errno.EWOULDBLOCK: break return main.CONNECTION_LOST if not bytes: return main.CONNECTION_DONE else: # 在这里解析协议并处理数据 print bytes 示例一可以很清晰的看到twisted的reactor本质：添加监听描述符，监听可读/可写事件，当事件来临时回调函数，回调完成之后继续监听事件。 需要注意： 套接字为非阻塞，如果为阻塞则失去了reactor的意义 我们通过继承IReadDescriptor来提供reactor所需要的接口 通过reactor.addReader将套接字类加入reactor的监听对象中 main.CONNECTION_LOST是twisted预定义的值，通过这些值它我们可以一定程度控制下一步回调(类似于模拟一个事件) 但是上面的MySocket类不够好，主要有以下缺点： 需要我们自己去读取数据，而不是框架帮我们读好，并处理异常 网络IO和数据处理混为一块，没有剥离开来 三. twisted抽象twisted在reactor的基础上，建立了更高的抽象，对一个网络连接而言，twisted建立了如下三个概念: Transports：网络连接层，仅负责网络连接和读/写字节数据 Protocols： 协议层，服务业务相关的网络协议，将字节流转换成应用所需数据 Protocol Factories：协议工厂，负责创建Protocols，每个网络连接都有一个Protocols对象(因为要保存协议解析状态) twisted的这些概念和erlang中的ranch网络框架很像，ranch框架也抽象了Transports和Protocols概念，在有新的网络连接时，ranch自动创建Transports和Protocols，其中Protocols由用户在启动ranch时传入，是一个实现了ranch_protocol behaviour的模块，Protocols初始化时，会收到该连接对应的Transports，如此我们可以在Protocols中处理字节流数据，按照我们的协议解析并处理数据。同时可通过Transports来发送数据(ranch已经帮你读取了字节流数据了)。 和ranch类似，twisted也会在新连接到达时创建Protocols并且将Transport传入，twisted会帮我们读取字节流数据，我们只需在dataReceived(self, data)接口中处理字节流数据即可。此时的twisted在网络IO上可以算是真正的异步了，它帮我们处理了网络IO和可能遇到的异常，并且将网络IO和数据处理剥离开来，抽象为Transports和Protocols，提高了程序的清晰性和健壮性。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# 示例二 twisted抽象的使用from twisted.internet import reactorfrom twisted.internet.protocol import Protocol, ClientFactoryclass MyProtocol(Protocol): # 接口: Protocols初始化时调用，并传入Transports # 另外 twisted会自动将Protocols的factory对象成员设为ProtocolsFactory实例的引用 # 如此就可以通过factory来与MyProtocolFactory交互 def makeConnection(self,trans): print &apos;make connection: get transport: &apos;, trans print &apos;my factory is: &apos;, self.factory # 接口: 有数据到达 def dataReceived(self, data): self.poem += data msg = &apos;Task %d: got %d bytes of poetry from %s&apos; print msg % (self.task_num, len(data), self.transport.getPeer()) # 接口: 连接断开 def connectionLost(self, reason): # 连接断开的处理class MyProtocolFactory(ClientFactory): # 接口: 通过protocol类成员指出需要创建的Protocols protocol = PoetryProtocol # tell base class what proto to build def __init__(self, address): self.poetry_count = poetry_count self.poems = &#123;&#125; # task num -&gt; poem # 接口: 在创建Protocols的回调 def buildProtocol(self, address): proto = ClientFactory.buildProtocol(self, address) # 在这里对proto做一些初始化.... return proto # 接口: 连接Server失败时的回调 def clientConnectionFailed(self, connector, reason): print &apos;Failed to connect to:&apos;, connector.getDestination() def main(address): factory = MyClientFactory(address) host, port = address # 连接服务端时传入ProtocolsFactory reactor.connectTCP(host, port, factory) reactor.run() 示例二要比示例一要简单清晰很多，因为它无需处理网络IO，并且逻辑上更为清晰，实际上ClientFactory和Protocol提供了更多的接口用于实现更灵活强大的逻辑控制，具体的接口可参见twisted源代码。 四. twisted Deferredtwisted Deferred对象用于解决这样的问题：有时候我们需要在ProtocolsFactory中嵌入自己的回调，以便Protocols中发生某个事件(如所有Protocols都处理完成)时，回调我们指定的函数(如TaskFinished)。如果我们自己来实现回调，需要处理几个问题: 如何区分回调的正确返回和错误返回?(我们在使用异步调用时，要尤其注意错误返回的重要性) 如果我们的正确返回和错误返回都需要执行一个公共函数(如关闭连接)呢? 如果保证该回调只被调用一次? Deferred对象便用于解决这种问题，它提供两个回调链，分别对应于正确返回和错误返回，在正确返回或错误返回时，它会依次调用对应链中的函数，并且保证回调的唯一性。 123456789101112d = Deferred()# 添加正确回调和错误回调d.addCallbacks(your_ok_callback, your_err_callback)# 添加公共回调函数d.addBoth(your_common_callback)# 正确返回 将依次调用 your_ok_callback(Res) -&gt; common_callback(Res)d.callback(Res)# 错误返回 将依次调用 your_err_callback(Err) -&gt; common_callback(Err)d.errback(Err)# 注意，对同一个Defered对象，只能返回一次，尝试多次返回将会报错 暂时就这么多了，又可以回去看Firefly了。","tags":[{"name":"python","slug":"python","permalink":"http://wudaijun.com/tags/python/"}]},{"title":"Firefly 学习(一)","date":"2015-05-19T16:00:00.000Z","path":"2015/05/firefly-study1/","text":"一. 简介firefly是一款python开发的开源游戏服务器框架，基于分布式，底层使用twisted。 具体介绍可参见： firefly on github firefly官网 firefly官方Wiki firefly采用多进程方案，节点之间通过网络通信(当然你也可以创建单节点，独立完成大部分功能)，具有很好的可扩展性。 二. 使用作为一个Python初学者，下面只谈一些自己对firefly的一些肤浅认识。上面的途径可以获取到更完整和深入的资料。 下面的Demo的源代码可在我的Github上下载。 1. 流程总体上看，如果你要使用firefly，所需要做的事就是： 通过配置文件定义所有节点，节点配置，节点实现文件，以及节点和节点之间的联系(通过网络端口) 定义节点实现文件 启动主节点 firefly通过配置文件来设定你的分布式服务器，然后你只需创建和启动master节点，master服务器会启动配置文件中的各个子节点： if __name__==&quot;__main__&quot;: from firefly.master.master import Master master = Master() master.config(&#39;config.json&#39;,&#39;appmain.py&#39;) master.start() config.json定义你的分布式服务，appmain.py是你的子节点公共入口，master节点已在master.start()中启动。 2. 配置文件下面是一份 config.json 实例，该配置文件配置了一个无盘节点，即没有使用数据库: { &quot;master&quot;:{&quot;rootport&quot;:9999,&quot;webport&quot;:9998}, &quot;servers&quot;:{ &quot;gate&quot;:{&quot;name&quot;:&quot;gate&quot;, &quot;rootport&quot;:10000, &quot;app&quot;:&quot;app.gateserver&quot;}, &quot;net&quot;:{&quot;name&quot;:&quot;net&quot;, &quot;netport&quot;:10001, &quot;name&quot;:&quot;net&quot;, &quot;remoteport&quot;:[{&quot;rootport&quot;:10000, &quot;rootname&quot;:&quot;gate&quot;}], &quot;app&quot;:&quot;app.netserver&quot;}, &quot;game1&quot;:{&quot;name&quot;:&quot;game1&quot;, &quot;remoteport&quot;:[{&quot;rootport&quot;:10000, &quot;rootname&quot;:&quot;gate&quot;}], &quot;app&quot;:&quot;app.game1server&quot;} } } 通过配置文件已经能够很清楚地看懂该服务器的整个分布式情况： master节点master节点管理所有的节点，它有两个端口rootport和webport，顾名思义，rootport用于和和服务器中其它节点通信，webport用于后台管理，如关闭和重启所有子节点。调用master.start()后，框架会自动创建master节点并监听rootport和webport端口，后者通过Flask实现。 分布式节点如果将master节点称为整个服务器的根节点，那么servers中定义的节点即为分布式节点，样例config中定义了四个分布式节点，gate, dbfront, net, game1。每个节点都可以定义自己的父节点(通过remoteport，可有多个父节点)，并且关联节点的实现文件(位于config所在目录 app/*.py)。其中gate是net和game1的父节点，意味着如果有网络消息需要game1节点处理，那么消息将由net-&gt;gate-&gt;game1，同理消息响应途径为：game1-&gt;gate-&gt;net。 3. 公共入口appmain是我们定义的节点公共入口，它会由firefly通过python appmain.py 节点名 配置路径调用，节点名即为gate, dbfront, net, game1之一，配置路径即为 config.json。该入口允许我们对各分布式节点做一些预先特殊处理，在Demo的appmain.py中，仅仅是读取必须配置，通过一个firefly导出的统一节点类来启动节点: 123456789101112131415161718192021222324252627282930#coding:utf8&quot;&quot;&quot;本模块在启动master时作为参数传入firefly会在每个Server(除了master)启动时都调用该模块: cmds = &apos;python %s %s %s&apos;%(self.mainpath, sername, self.configpath) [位于master/master.py, 其中self.mainpath即为本模块] &quot;&quot;&quot;import osimport json, sysfrom firefly.server.server import FFServerif __name__ == &apos;__main__&apos;: args = sys.argv servername = None config = None if len(args) &gt; 2: servername = servername = args[1] config = json.load(open(args[2], &apos;r&apos;)) else: raise ValueError dbconf = config.get(&apos;db&apos;, &#123;&#125;) memconf = config.get(&apos;memcached&apos;, &#123;&#125;) servsconf = config.get(&apos;servers&apos;, &#123;&#125;) masterconf = config.get(&apos;master&apos;,&#123;&#125;) serverconf = servsconf.get(servername) server = FFServer() server.config(serverconf, dbconfig=dbconf, memconfig=memconf, masterconf=masterconf) print servername, &apos;start&apos; server.start() print servername, &apos;stop&apos; appmain.py通过firefly的FFServer来启动节点，这里先不管FFServer如何区分各个节点。至此，我们的分布式服务器就算是启动了。 4. 节点实现最后需要我们关心的，就是节点实现了，不用多说，FFServer会根据你传入的节点实现文件，来实现节点的功能。而实际上我们需要做的事情是很少的，因为启动服务器，监听端口，节点间通信，甚至网络消息编解码等等这些功能，FFServer都帮你做了，后面会提到它如何区分和实现这些功能。 而我们要做的，就是通过装饰器响应消息就OK了，并且节点之间的消息转发也很方便： netserver实现 123456789101112131415161718192021222324252627#coding:utf8from firefly.server.globalobject import GlobalObject, netserviceHandle&quot;&quot;&quot;netservice 默认是 CommandService: netservice = services.CommandService(&quot;netservice&quot;) [位于server/server.py] CommandService 的消息响应函数格式为: HandleName_CommandID(conn, data) CommandService 会通过&apos;_&apos;解析出CommandID并注册HandleName_CommandId为其消息响应函数&quot;&quot;&quot;@netserviceHandledef netHandle_100(_conn, data): print &quot;netHandle_100: &quot;, data return &quot;netHandle_100 completed&quot;@netserviceHandledef netHandle_200(_conn, data): print &quot;netHandle_200: &quot;, data, &quot;forward to gate&quot; # 转发到 gateserver.gateHandle1 # 通过 GlobalObject().remote[父节点名]来得到父节点的远程调用对象 return GlobalObject().remote[&apos;gate&apos;].callRemote(&apos;gateHandle1&apos;, data)@netserviceHandledef netHandle_300(_conn, data): print &quot;netHandle_300: &quot;, data, &quot;forward to gate&quot; return GlobalObject().remote[&apos;gate&apos;].callRemote(&apos;gateHandle2&apos;, data) gateserver实现 12345678910111213141516#coding:utf-8from firefly.server.globalobject import GlobalObject, rootserviceHandle@rootserviceHandledef gateHandle1(data): print &quot;gateHandle: &quot;, data return &quot;gateHandle Completed&quot;@rootserviceHandledef gateHandle2(data): print &quot;gateHandle2: &quot;, data, &quot;forward to game1: &quot; # 转发到 game1.game1Handle # 通过 GlobalObject().root.callChild(节点名，节点函数，参数)远程调用孩子节点 return GlobalObject().root.callChild(&quot;game1&quot;, &quot;game1Handle&quot;, data) game1server实现 123456from firefly.server.globalobject import GlobalObject, remoteserviceHandle@remoteserviceHandle(&quot;gate&quot;)def game1Handle(data): print &quot;game1Handle: &quot;, data return &quot;game1Handle completed&quot; 运行Demo，启动测试客户端，得到结果: Server端: [firefly.netconnect.protoc.LiberateFactory] Client 0 login in.[127.0.0.1,61752] [LiberateProtocol,0,127.0.0.1] call method netHandle_100 on service[single] [LiberateProtocol,0,127.0.0.1] netHandle_100: msgdata [LiberateProtocol,0,127.0.0.1] call method netHandle_200 on service[single] [LiberateProtocol,0,127.0.0.1] netHandle_200: msgdata forward to gate [BilateralBroker,0,127.0.0.1] call method gateHandle1 on service[single] [BilateralBroker,0,127.0.0.1] gateHandle: msgdata [LiberateProtocol,0,127.0.0.1] call method netHandle_300 on service[single] [LiberateProtocol,0,127.0.0.1] netHandle_300: msgdata forward to gate [BilateralBroker,0,127.0.0.1] call method gateHandle2 on service[single] [BilateralBroker,0,127.0.0.1] gateHandle2: msgdata forward to game1: [Broker,client] call method game1Handle on service[single] [Broker,client] game1Handle: msgdata [LiberateProtocol,0,127.0.0.1] Client 0 login out. Client端: ---------------- send commandId: 100 netHandle_100 completed ---------------- send commandId: 200 gateHandle Completed ---------------- send commandId: 300 game1Handle completed 6. 总结看起来，使用firefly确实很简单，通过配置文件即可完成强大的分布式部署，节点之间的通信协议，节点间消息以及网络消息的编解码，甚至重连机制框架都已经帮你完成。你只需通过python装饰器，来实现自己的请求响应逻辑即可。 三. 实现原理简单梳理一下firefly内部替我们完成的事。 1.master启动在我们的app入口文件中，通过master.start()启动服务器，master.start()完成了: 创建一个PBRoot 在rootport监听其它节点连接 创建一个Flask 在webport 监听管理员命令 遍历配置中的servers 通过python appmain.py 节点名 配置文件启动各个分布式节点，appmain.py由使用者编写和提供 2.FFServer在appmain.py中，通过FFServer来创建和启动一个节点，firefly FFServer抽象一个服务进程，前面曾提到过，由于所有非master节点都通过FFServer启动，那么FFServer如何区分各节点功能和通讯协议？ 答案很简单，FFServer检查节点各项配置，为各项配置创建对应的组件，其中比较重要的有: webport 代表该节点希望提供web服务，FFServer通过Flask启动一个简单的web server rootport 代表该节点是一个父节点，创建并启动PBRoot类(master也有一个PBRoot成员)来监听其它节点的连接 netport 代表该节点希望接收客户端网络数据，FFServer创建LiberateFactory并监听netport，LiberateFactory中包含对网络数据的解码 db 若该配置为true，FFServer会根据config中的db配置连接到DB mem 若该配置为true，FFServer会根据config中的memcached配置连接到memchache remoteport, FFServer为每个父节点创建RemoteObject，并保存remote[name] -&gt; RemoteObject 映射 这样，一个节点可以灵活分配一个或多个职责，并且每份职责通过独立的类来处理内部逻辑和通信协议等。除此之外，FFServer还做了两件事： import 节点关联的实现文件，该实现文件通过装饰器可以导入消息回调函数。 连接master节点 3. 待续","tags":[{"name":"firefly","slug":"firefly","permalink":"http://wudaijun.com/tags/firefly/"},{"name":"python","slug":"python","permalink":"http://wudaijun.com/tags/python/"}]},{"title":"WordPress搭建历程","date":"2015-05-18T16:00:00.000Z","path":"2015/05/wordpress-setup/","text":"记录一下搭建wordpress博客的过程，做备忘之用，仅供参考。 一. 前期准备一台云服务器和一个域名(可选)。国内的服务器搭建网站需要备案，国外的话推荐linode，目前linode tokyo服务器应该是国内访问最快的，但是已经缺货了，而新开的singapore服务器线路优化又不是很好(ping 300+)，后来又换成了fremont，速度总算稳定了一些，210 ping左右。 二. 部署wordpress我的环境是 Ubuntu 14.04 LTS。 1.安装 apache2 + php5 + mysql-server// apache sudo apt-get install apache2 // 安装完成后，在本地打开浏览器 http://云服务器IP地址 测试 // php5 sudo apt-get install php5 sudo apt-get install libapache2-mod-php5 // mysql sudo apt-get install mysql-server sudo apt-get install libapache2-mod-auth-mysql sudo apt-get install php5-mysql // 重启apache 如果遇到 ServerName 警告，可在/etc/apache2/apache2.conf 中， // 添加一行: ServerName localhost /etc/init.d/apache2 restart 2.下载解压 wordpresswget https://cn.wordpress.org/wordpress-4.2-zh_CN.tar.gz // 可去cn.wordpress.org获取最新版 tar zxvf wordpress-4.2-zh_CN.tar.gz 3.为wordpress 准备 mysqlmysql -uroot -p mysql&gt; CREATE DATABASE 网站数据库名 mysql&gt; GRANT ALL PRIVILEGES ON 网站数据库名.* to 用户名@localhost identified by &#39;密码&#39; mysql&gt; exit 4.配置 wordpress// 配置前面为wordpress准备的mysql数据库和用户 cd wordpress mv wp-config-sample.php wp-config.php vim wp-config.php #在配置文件中，配置DB_NAME DB_USER DB_PASSWORD三项 5.添加 wordpress 到 apache// 将wordpress中所有内容移动到 /var/www/html下 // /var/www/html是apache的默认根目录 sudo mv wordpress/* /var/www/html 6. 安装wordpress本地浏览器中，输入 http://云服务器地址/wp-admin/install.php 安装向导提供网站管理的用户名密码等信息。即可完成安装 三. 完善wordpress1. 修改网站根目录apache2默认目录为 /var/www/html，如果要更改到/var/www: 1. 修改/etc/apache2/sites-available/000-default.conf，将其中的 DocumentRoot 改为 /var/www 2. 执行: sudo mv /var/www/html/* /var/www/ 3. 重启: service apache2 restart 2. 制作固定链接要求: 1. Apache web server，安装了mod_rewrite模块: 操作: sudo a2enmod rewrite 或: sudo ln -s /etc/apache2/mods-available/rewrite.load /etc/apache2/mods-enabled/rewrite.load 2. 在WordPress的home目录: FollowSymLinks option enabled FileInfo directives允许 (如 AllowOverride FileInfo 或 AllowOverride All) 操作: 在/etc/apache2/apache2.conf中，找到&lt;Directory /var/www/&gt;标签，将其改为: &lt;Directory /var/www/&gt; Options Indexes FollowSymLinks AllowOverride All Require all granted &lt;/Directory&gt; 3. .htaccess文件 (如果找不到，当你激活漂亮固定链接时WordPress会尝试创建它) 如果你希望WordPress自动更新.htaccess，WordPress需要有这个文件的写权限。 操作: 在你的网站根目录(wordpress文件目录)中: sudo touch .htaccess sudo chmod 777 .htaccess //最粗暴的方式，方便wordpress自动写入 准备就绪后，在wordpress 管理页面-&gt;设置-&gt;固定链接中可设置固定链接格式，地址为http://xxx/wp-admin/options-permalink.php。选定固定链接后，wordpress会自动尝试写入规则，如果写入失败，则会在最下方给出提示，让你尝试手动添加规则。 完成之后，固定连接就生效了。 3. 更换主题在wordpress中自带更换主题的功能，但默认需要FTP用户名和密码，因为web访问的用户不具有对服务器wordpress文件夹的相关操作权限。由于安装方式不一样，解决方案不一样。我最后找到比较有用的是这里提供的一些思路： // 先将wordpress相关文件全部改为 777 sudo chmod 777 -R /var/www/wp* // 然后通过wordpress管理界面，主题能够安装成功了 // 此时观察 wp-content/themes的写入者为www-data // 改回权限: sudo chmod 755 -R /var/www/wp* chown -R www-data /var/www/wp* 另外，推荐一个wordpress中文主题下载网站: http://www.wopus.org 4. 关于主页到目前为止，如果在本地浏览器直接输入http://云服务器地址 得到的将是apache的it works页面，这也是服务器上/var/www/index.html页面。而我们想使用的是/var/www/index.php作为我们的主页。此时删掉/var/www/index.html即可。 5. 域名绑定这个比较简单，在你的域名提供商中修改DNS指向为你的云服务器IP地址，然后在wordpress管理-&gt;设置 中修改站点地址为你的域名，就可以了。 四. 参考文档: 官方安装教程 网友安装教程 官方使用文档","tags":[{"name":"tool","slug":"tool","permalink":"http://wudaijun.com/tags/tool/"}]},{"title":"Erlang 服务器落地机制","date":"2015-05-07T16:00:00.000Z","path":"2015/05/erlang-gameserver-persistence/","text":"游戏服务器中用得最多的就是gen_server，比如游戏中的Player进程，由于gen_server提供的完善的进程设施，我们无需过多地担心进程崩溃而造成的数据丢失的问题(至少还有个terminate用于做善后工作)。因此在进行数据写回时，可以通过定时写回的机制来减轻数据库负担。这一点也是C服务器望尘莫及的。 落地流程落地时机应由PlayerManager触发，PlayerManager管理所有的Player进程，每隔一段时间进行数据落地。为了避免同时对所有玩家落地造成的热点，可以将Player进程简单分区，每次对其中一个区进行落地，如此轮流。 落地操作交由Player进程，因为我们的绝大部分关于Player的数据都是放在进程字典中的。 Player进程首先遍历其相关的所有Model，取出其中变化的数据，然后更新数据库。 为了模块化，将相关模块描述为： player: 玩家进程，玩家主要业务逻辑处理，消息分发 player_model: 业务逻辑与数据层的中间模块，负责数据初始化和落地 state: 辅助管理所在进程的进程字典，跟踪数据变化。提供查询和更改进程字典，获取进程字典变化数据的接口。 model: 数据层，负责和数据库交互，提供insert, update等基本接口 实现机制落地实现最核心的两个模块是 player_model 和 state，前者负责Player所有数据的初始化和落地，后者负责管理Player进程字典数据，并且追踪数据的变更状态。 player_modelplayer_model 建立了业务层到模型层的映射，它仅提供两个最重要的接口：init(PlayerId) 和 save(PlayerId)，分别负责Player所有模块的初始化(数据库 -&gt; 进程字典)和落地(进程字典 -&gt; 数据库)。 在player_model中，有所有Model的相关信息，包括名字，类型和所在模块等等。 module_map() -&gt; [ {player_info, {?INFO_STATE, single}, model_user, ?model_record(db_user_info)}, ..... %{业务逻辑模块, 进程字典中的Key和存储类型 single or list}, %{数据存储模块, 数据存储字段} ] 这样业务逻辑层和Model层被关联起来，对于save来说，最重要的是第二个字段和第三个字段，分别代表该Model在进程字典的状态，以及Model名。save流程主要如下： 遍历module_map()，获取各个Model数据在进程字典中变更数据 根据变更状态调用对应Model接口 完成回写 回写完成之后，再重置各个Model的变更状态 注意： 1，2步是事务性的，所有Model的回写要么都成功，要么都失败，以免各个模块数据之间的数据相关性造成数据不一致的问题。在写入成功后，再次遍历module_map()，重置各个Model的状态。 对于list 和 single两种类型的Model需要分开处理，它们获取变更数据和回写的接口不一样，这可能还需要Model层的支持。这一点在下面state模块介绍中会提到。 关于获取Model在进程字典中状态管理，通过state模块来管理。 statestate模块管理进程字典中的数据，进程字典虽然为简单的Key-Value，但对于我们的Model来说，Value可能为单个记录(如玩家信息)或列表(如玩家建筑列表)。 最简单的情况是，我们单独用一个进程字典，如{Name, state}来获取Model的数据状态，数据状态可分为 origin(初始化) new(创建未保存)， update(更新) delete(删除) 在数据更新时，修改状态，在每次落地同步时，取出所有被修改的Model，并且进行落地同步。之后将数据的状态重置为origin。 然而这种做法对于list类型的Model效率太低，一是业务逻辑上的每次更改都需要改动每个数组，典型的例子是任务列表，玩家对某个任务领奖，导致整个任务列表的拷贝，还可能产生不必要的查找过程。更不可忍受的是，数据落地时，也将重写整个任务列表到数据库。 因此还有另一种方案：将list Model中的记录分开存放，并且分别标记状态，提高查找和回写效率： %% ------ list Model ---------- % 存放list中各个key的状态 {Name, list} -&gt; [{key1, update}, {key2, delete}, ...] % 存放列表中各元素的实际数据 {Name, Key} -&gt; Data % 存放被删除的元素列表(将不能通过{Name, Key}找到) {Name, delete} -&gt; [DeleteData1, DeleteData2, ....] %% ------- single Model ------- % 通过 Name 存取 Name -&gt; Value % 存放Model的更改状态 {Name, state} -&gt; State 如此便对Model进行了高效灵活的管理，大大减少了回写数据量。 state封装了进程字典的增删查改操作，并维护进程字典状态。 读取直接通过erlang:get(Name, Key)，对于任务列表来说，这个Key通常是任务ID 更新时： 对于列表: 通过{Name, list}检查更新Key的状态 对{Name, Key}执行修改 对于删除操作，还需要将删除的数据放入{Name, delete}中 对于单值: 通过{Name, state}检查修改状态 对Name执行修改 落地相关接口： % 获取list Model中的变更数据 % 返回: {InsertList, UpdateList, DeleteList} get_list_changed(Name) % 获取single Model中的数据状态 % 返回: {State, Value} get_single_changed(Name) % 重置list Model中的数据状态为origin 并且删除所有状态为delete的数据 reset_list(Name) % 重置single Model reset_single(Name) player_mode根据module_map中的条目依次获取变更数据，在使用model模块更新时，可让model模块也提供对single和list两种类型回写的支持。提供各个Model的特殊化处理，如有些Model可以忽略删除列表。 数据加载最后再谈谈关于这套框架的数据加载，player_model提供一个init(PlayerId)完成数据的加载，module_map中业务逻辑模块到数据Model层的映射，也是为此准备的。 player_model遍历module_map，调用Model:get(PlayerId)，取出各个Model的数据，然后通过module_map找到对应的业务逻辑模块，回调业务逻辑层初始化函数，该函数可默认指定，比如叫init_callback，每个module_map中的业务逻辑模块都需要提供init_callback进行初始化处理，如同步客户端等等，之后也由init_callback决定是否将数据存往进程字典(通过state模块)。","tags":[{"name":"erlang","slug":"erlang","permalink":"http://wudaijun.com/tags/erlang/"}]},{"title":"Erlang 随想","date":"2015-04-21T16:00:00.000Z","path":"2015/04/erlang-thinking/","text":"接触Erlang不到两个月时间，之前一直用C++开发。Erlang这门语言确实带给我更多的思考。 并发模型在C++游戏服务器中，我们想要实现一个logger，支持多线程调用。因此该日志系统必须是线程安全的(c++标准输出std::cout不是线程安全的)。对应的主要有两种实现策略： 1.共享资源常规的思维是，为了实现这个功能，去定义一个接口(函数或类)，可提供给所有用户(线程)使用。但由于多个用户共享一个IO设备资源，因此需要在接口内部通过锁或其它同步方式来实现对资源的访问控制。而锁的设计与调试历来是并发程序中最耗费精力的一部分，并且由于代码在调用线程的上下文中执行，因此接口内部发生故障也会影响到调用线程，如死循环，内存越界等。即隔离性差(包括线程之间的隔离，和模块之间的解耦)。 2.消息传递另一种方式是，将logger抽象为一个单独的执行体，它可以是一个单独的线程，由它来独占IO设备资源，其它线程想要使用IO设备，都需要通知(发送消息)logger线程，由logger线程来操作IO。这样就不会出现资源访问控制的问题。当然你需要自己实现一个线程安全的消息队列，但这毕竟是公共设施，是属于框架层的。NGServer和skynet就是这么做的。消息传递方式的瓶颈在于消息拷贝。 再举一个例子，针对于排行榜系统，玩家需要不定期的读取排行榜，而其它玩家的数据变动也会影响到排行榜数据，如果使用共享内存的方式，你可能会使用读写锁，双缓冲，share_ptr copy on write，等多种方式来优化线程之间的同步问题。但仍然是如履薄冰，因为某一次死锁或出错，都可能造成整个游戏服务器宕机。事实上排行榜系统并不是很重要，我们宁愿它无法正常服务，也不应该影响到游戏的正常逻辑。 通过将排行榜独立为一个服务，利用消息传递进行读取于写入，可以很大程度减轻服务之间相互影响的可能性，但往往游戏中的这一类系统太多，一是不能很好地控制和管理这些服务，二是服务之间的消息协议会越来越繁多和复杂，skynet设计者云风也提到了这一点。 Erlang终于主角出场了，Erlang是消息传递的忠实拥护者，并且构建了大量的基础设施来更好支持这一特性。我觉得Erlang的精髓在于面向进程和变量不可变语义。 1.面向进程Erlang中的服务就是进程，Erlang中的进程比C++的线程更轻量，可以轻易数万数十万级别。Erlang进程相关的基础设施，包括消息队列，调度算法，消息编码都已经千锤百炼，拿来即用。加之工业级的OTP，消息分发和回调也完成了大半，并且更好地支持了容错和热更等机制。 Erlang中将逻辑抽象成进程，而不是对象，Erlang不支持面向对象。上面提到的日志和排行榜，都可以抽象成一个Erlang进程，让它来负责相关的处理。不需要锁，同时也增加了隔离性，Erlang进程封装了状态。 2.变量不可变Erlang是函数式编程语言，遵循变量不可变语义，一个变量自绑定值起，就一直代表某个值。这种语义刚开始虽然会带来一些不便，但是附带的好处却是巨大的：变量不可变进一步降低了Erlang进程之间相互影响的可能性。例如在NGServer中，你可以通过在线程之间传递指针共享信息，这样做虽通常是为了效率起见，而缺点在于线程之前的关联度增加，并且指针本身的管理和释放也并非易事。在Erlang中，所有的消息都执行拷贝，并且不存在指针引用，因此进程中拿到的消息都是独立的，不变的，这样大幅度降低了犯错的可能性。","tags":[{"name":"erlang","slug":"erlang","permalink":"http://wudaijun.com/tags/erlang/"}]},{"title":"Erlang 热更新","date":"2015-04-20T16:00:00.000Z","path":"2015/04/erlang-hotcode/","text":"erlang 热更是指在erlang系统不停止运行的情况下，对模块代码进行更新的特性，这也是erlang最神奇的特性之一。特别适用于游戏服务器，做活动更新，漏洞修复等。 一. 简单示例123456789101112131415%% 示例一 -module(test).-export([start/0, run/0]).f() -&gt; io:format(&quot;this is old code~n&quot;).run() -&gt; f(), timer:sleep(5000), ?MODULE:run().start() -&gt; spawn(fun() -&gt; run() end). 在erl shell中运行test: Eshell V6.3.1 (abort with ^G) 1&gt; c(test). {ok,test} 2&gt; test:start(). this is old code &lt;0.39.0&gt; this is old code this is old code 修改test.erl代码，将f()输出改为 io:format(&quot;this is new code~n&quot;).。 在erl shell中，重新编译并加载test模块。 可通过erlc test.erl完成模块编译，然后在erl shell中通过l(test).完成加载。也可直接在erl shell 中通过c(test).单步完成编译和加载。 3&gt; c(test). {ok,test} 观察完整test:run()运行结果： 1&gt; c(test). {ok,test} 2&gt; test:run(). this is old code &lt;0.39.0&gt; this is old code this is old code 3&gt; c(test). {ok,test} this is new code this is new code ... 二. 热更原理2.1 两个条件Erlang代码热更需要两个基本条件： 将修改后的代码重新编译并加载 只有外部调用(完全限定方式调用)才会使用新版本的代码 第一个条件在上面示例中已经做过，要注意的是，使用erlc命令行工具编译.erl源文件后，需要在erl shell中加载模块，才能将新模块代码更新到erlang虚拟机中。而我们平时通过erlc编译，然后直接进入erl shell使用模块，事实上是Erlang虚拟机自动在系统路径中查找并加载了对应模块。 第二个条件所谓的外部调用(external calls)，即 Mod:Func(Arg) 形式的调用。而对应的本地调用是指 Func(Arg)。本地调用的函数比外部调用更快，并且调用的函数无需导出。erlang热更新只会对外部调用应用最新的模块代码，而对于本地调用则会一直使用旧版本的代码。 在上面的例子中，我们在尾递归中使用?MODULE:run()实现了外部调用，因此每一次都会检查并应用最新的模块代码。而如果将该调用其改为run()。则将一直使用当前版本的代码，始终输出this is old code。 需要注意的是，erlang更新虽然以模块为单位，但却执行”部分更新”，即对于某外部调用f()，运行时系统仅更新f()函数所引用的代码，即f()函数和其依赖的函数(无论何种调用形式)的代码。比如示例一中，对run函数的外部调用，完成了对f()函数的代码更新，因为run()函数依赖f()函数。 而反过来，对f()的外部调用，不会更新run()的代码： 1234567891011121314%% 示例二-module(test2).-export([start/0, f/0]).f() -&gt; io:format(&quot;this is old code~n&quot;).run() -&gt; ?MODULE:f(), timer:sleep(5000), run().start() -&gt; spawn(fun() -&gt; run() end). 编译并运行，再修改test2.erl: 123456789101112131415%% 示例二 新版本代码-module(test2).-export([start/0, f/0]).f() -&gt; io:format(&quot;this is new code~n&quot;).run() -&gt; io:format(&quot;say hello~n&quot;), ?MODULE:f(), timer:sleep(5000), run().start() -&gt; spawn(fun() -&gt; run() end). 编译并加载新模块代码，得到的输出将和示例一类似，而不会打印出”say hello”。 2.2 新旧更迭当模块有新版本的代码被载入时，之后对该模块执行的外部调用将依次加载模块最新代码，其它没有更新模块代码的进程仍然可以使用模块的当前版本(现在已经是旧版本)代码。erlang系统中同一模块最多可以存在两个版本的代码同时运行。 如果有进程一直在执行旧版本代码，没有更新，也没有结束，那么当模块代码需要再次更新时，erlang将kill掉仍在执行旧版本代码的进程，然后再执行本次更新。 2.3 更新策略erlang中的热更是通过code_server模块来实现的，code_server模块是kernel的一部分，它的职责是将已经编译好的模块加载到运行时环境。code_server有两种启动策略，embedded和interactive(默认)两种模式： embeded模式：指模块加载顺序需要预先定义好，code_server会严格按照加载顺序来加载模块 interactive模式：模块只有在被引用到时才会被加载 三. 控制更新如果要在模块代码中实现对更新机制的控制，比如代码希望处理完某个逻辑流程之后，检查并应用更新。可以如下这样： 123456789101112131415161718192021222324252627%% 示例三-module(hotfix).-export([server/1, upgrade/1, start/0]). -record(state, &#123;version, data&#125;).server(State) -&gt; receive update -&gt; NewState = ?MODULE:upgrade(State), io:format(&quot;Upgrade Completed. Now verson: ~p~n&quot;, [NewState#state.version]), ?MODULE:server(NewState); %% loop in the new version of the module _SomeMessage -&gt; %% do something here io:format(&quot;Stay Old~n&quot;), server(State) %% stay in the same version no matter what. end.upgrade(State) -&gt; %% transform and return the state here. io:format(&quot;Upgrading Code~n&quot;), NewState = State#state&#123;version=2.0&#125;, NewState.start() -&gt; spawn(fun() -&gt; server(#state&#123;version=1.0&#125;) end). 示例三中，main loop 只有在收到update消息后，才会执行更新，否则通过本地调用，始终执行当前版本的代码。而发送update消息的时机可以由程序灵活控制。 在执行更新时，代码通过?MODULE:upgrade(State)来预热代码，对数据结构进行更新处理，upgrade函数由本次代码更新者提供，因此能够非常安全地进行版本过渡。之后再调用?MODULE:server(NewState)来进行主循环代码的更新。 测试一下(这里并没真正修改代码)： Eshell V6.3.1 (abort with ^G) 1&gt; c(hotfix). {ok,hotfix} 2&gt; Pid = hotfix:start(). &lt;0.39.0&gt; 3&gt; Pid ! hello. Stay Old hello 4&gt; Pid ! update. Upgrading Code Upgrade Completed. Now verson: 2.0 update 四. 参考 http://learnyousomeerlang.com/designing-a-concurrent-application#hot-code-loving http://www.erlang.org/doc/reference_manual/code_loading.html#id86381","tags":[{"name":"erlang","slug":"erlang","permalink":"http://wudaijun.com/tags/erlang/"}]},{"title":"Git 基础知识","date":"2015-04-16T16:00:00.000Z","path":"2015/04/git-study/","text":"之前工作中一直用svn，虽然也在git上写一些代码，但也仅限于 add commit push 这几个命令，一直没有对git比较系统的学习，花了半天多时间，学习整理了一些自己觉得比较实用的东西，大部分都是摘抄，仅供自己学习备忘之用，完整系统的介绍可参见附录。 本地仓库修改提交： 本地文件 -(git add )-&gt; 暂存区 -(git commit )-&gt; 本地仓库 本地仓库 -(git reset)-&gt; 暂存区 -(git checkout)-&gt; 本地文件 通过git commit --amend可以修改上一次提交 git show 可以查看最近一次提交修改内容 等价于 git log -n1 -p 分支： 查看分支：git branch 创建分支：git branch &lt;name&gt; 切换分支：git checkout &lt;name&gt; 创建+切换分支：git checkout -b &lt;name&gt; 合并某分支到当前分支：git merge &lt;name&gt; 删除分支：git branch -d &lt;name&gt; 远程协作1. 远程分支远程分支无法在本地修改或移动，只有在你连接远程仓库时，才会更新。它记录上次连接远程仓库时各分支的位置。它相当于远程仓库各分支在本地仓库的一个指针，每个远程分支都对应某个远程仓库的某个分支。你可以在本地仓库为其关联本地分支，然后你在本地分支上的操作和提交(commit)，都由本地仓库管理，直到你决定推送(push)本地分支的修改或拉取(pull)远程分支的更新，此时git才会和远程仓库通讯，并更新远程分支的位置。 在执行 git clone 时，git默认为你创建了本地master分支，并且关联到远程分支origin/master上。可以通过git checkout -b mybranch origin/remotebranch来创建一个mybranch本地分支并关联到origin/remotebranch远程分支。 2. 拉取远程分支git fetch origin master pull和fetch操作可以拉取远程仓库origin上的master分支的更新，并且更新origin/master的位置。pull会在拉取时会自动进行合并，而fetch只拉取本地没有的内容，而不会尝试合并。 如果省略远程分支名，调用 git pull(fetch) origin，那么将默认作用于当前本地分支所关联的远程分支，并且会更新远程仓库上的所有分支信息。比如，你上次pull之后，其它人在远程仓库origin上添加了dev分支，那么本次通过git pull(fetch) origin将得到一个新的origin/dev远程分支。注意，git此时不会为origin/dev分支创建关联的本地分支，这需要你手动创建:git checkout -b dev origin/dev或git checkout --track origin/dev，这样便能在新的dev分支上工作。 3. 推送本地分支git push origin localbranch : remotebranch 当执行push操作时，git尝试将本地的localbranch分支推送到远程仓库origin的remotebranch分支，如果origin/remotebranch已更新，则需要先pull。大多数情况下，我们只需要输入 git push origin localbranch即可，git会自动找到其对应的远程分支并执行推送。 如果localbranch为新创建的本地分支，还没有关联远程分支，那么git会在远程仓库上创建一个remotebranch分支，并与localbranch关联。如未指定remotebranch，则会创建一个分支名和本地分支一致的远程分支。 另外，通过 git push origin : remotebranch 可以删除远程仓库上的remotebranch分支。语法上可以理解为，推送了一个空分支给远程分支。 Git 参考：1.Git Book中文版 2.廖学峰的Git教程 3. Git飞行规则","tags":[{"name":"git","slug":"git","permalink":"http://wudaijun.com/tags/git/"}]},{"title":"Erlang mnesia","date":"2015-04-07T16:00:00.000Z","path":"2015/04/erlang-mnesia/","text":"mnesia是基于Erlang的分布式数据库管理系统，是Erlang OTP的重要组件。 基础特性1. 分布式的etsmnesia数据库被组织为一个表的集合，每个表由记录(通常被定义为Erlang Record)构成，表本身也包含一些属性，如类型，位置和持久性。这种表集合和记录的概念，和ets表很类似。事实上，mnesia中的数据在节点内就是以ets表存储的。因此，mnesia实际上是一个分布式的ets。 2. 表的存储形式mnesia中的表在节点内有三种存储形式： ram_copies: 表仅存储于内存，可通过mnesia:dump_tables(TabList)来将数据导入到硬盘。 disc_copies: 表存储于内存中，但同时拥有磁盘备份，对表的写操作会分为两步：1.将写操作写入日志文件 2. 对内存中的表执行写操作 disc_only_copies: 表仅存储于磁盘中，对表的读写将会更慢，但是不会占用内存 表的存储形式可以在表的创建中指出，默认为ram_copies。也可以在创建表后通过mnesia:change_table_copy_type/3来修改。 3. 表的重要属性表的属性由mnesia:create_table(Name, TableDef)中的TableDef指定，TableDef是一个Tuple List，其中比较重要的属性有： type: 表的类型，主要有set, ordered_set和bag三种。前两者要求key唯一，bag不要求key唯一，但要求至少有一个字段不同。另外set和bag通过哈希表实现，而ordered_set则使用其它数据结构(如红黑树)以对key排序。type属性默认为set。 attributes: 表中条目的字段，通常由record_info(fields, myrecord)得出，而myrecord一般则用作表名。 local_content: 标识该表是否为本地表，local_content属性为true的表将仅本地可见，不会共享到集群中。local_content默认为false。 index: 表的索引。 ram_copies: 指名该表在哪些节点上存储为ram_copies。默认值为[node()]。即新建表默认都存储为ram_copies。 disc_copies: 该表在哪些节点上存储为disc_copies。默认为[]。 disc_only_copies: 该表在哪些节点上存储为disc_only_copies。默认为[]。 4. schema表schema表是mnesia数据库一张特殊的表，又叫模式表。它记录数据库中其它表的信息，schema表只能有ram_copies或disc_copies两种存储形式。并且一旦schema表存储为ram_copies，那么该节点上的其它表，也将只能存储为ram_copies。 mnesia需要schema表的初始化自身，可在mnesia启动前，通过mnesia:create_schema/1来创建一个disc_copies类型的schema表，如果不调用mnesia:create_schema/1，直接启动mnesia:start/0，默认生成一个ram_copies类型的schema表，此时我们称该mnesia节点为”无盘节点”，因为其所有表都不能存储于磁盘中。 5. 单节点使用示例➜ ~ erl -mnesia dir &#39;&quot;Tmp/ErlDB/test&quot;&#39; Erlang/OTP 17 [erts-6.3.1] [source] [64-bit] [smp:4:4] [async-threads:10] [hipe] [kernel-poll:false] [dtrace] Eshell V6.3.1 (abort with ^G) # 创建disc_copies存储类型的schema表 但其它表的默认存储类型仍然为ram_copies 1&gt; mnesia:create_schema([node()]). ok 2&gt; mnesia:start(). ok 3&gt; rd(person, {name, sex, age}). person # 创建disc_copies存储类型的table，table的fields即为person记录的fields 4&gt; mnesia:create_table(person, [{disc_copies, [node()]}, {attributes, record_info(fields, person)}]). {atomic,ok} # 等价于mnesia:dirty_write({person, &quot;wdj&quot;, undefined, 3}) 5&gt; mnesia:dirty_write(#person{name=&quot;wdj&quot;, age=3}). ok 6&gt; mnesia:dirty_read(person, &quot;wdj&quot;). [#person{name = &quot;wdj&quot;,sex = undefined,age = 3}] record_info(fileds, person)返回[name,sex,age]，mnesia:create_table/2默认将attributes属性中的第一个field作为key，即name。 mnesia:read, mnesia:write, mnesia:select等API均不能直接调用，需要封装在事务（transaction）中使用： F = fun() -&gt; Rec = #person{name=&quot;BigBen&quot;, sex=1, age=99}, mnesia:write(Rec) end, mnesia:transaction(F). 而对应的mnesia:dirty_read，mnesia:dirty_write，即”脏操作”，无需事务保护，也就没有锁，事务管理器等。dirty版本的读写一般要比事务性读写快十倍以上。但是失去了原子性和隔离性。 6. 表名与记录名mnesia表由记录组成，记录第一个元素为是记录名，第二个元素为标识记录的键。{表名，键}可以唯一标识表中特定记录，又称为记录的对象标识(Oid)。 mnesia要求表中所有的记录必须为同一个record的实例，前面的例子中，表名即为记录名，表字段则为记录的域。而实际上，记录名可以是但不一定是表名，记录名可通过record_name属性指出，没有指定table_name则记录名默认为create_table第一参数指定的表名。 mnesia:dirty_write(Record) -&gt; Tab = element(1, Record), mnesia:dirty_write(Tab, Record). % 这里提取出表名，表名和表中记录原型实际上是分离的 表名和记录名不一致使我们可以定义多个以同一record的原型的table。 集群管理% 创建集群 需要各节点的mnesia都未启动 mnesia:create_schema([&#39;node1@host,&#39;node2@host&#39;]) % 创建表 指明在各节点上的存储类型 如果没指定，则为remote类型 mnesia:create_table(person, [{ram_copies,[&#39;node1@host&#39;]}]) % 删除表的所有备份 mnesia:delete_table(person) % 删除整个schema和表数据(包含持久化文件) mnesia:delete_table([&#39;node1@host,&#39;node2@host&#39;]) % 集群动态配置能力 % 动态加入集群(等价于启动参数：-mnesia extra_db_nodes NodeList) mnesia:change_config(extra_db_nodes, [&#39;node3@host&#39;]) % 动态修改表的存储类型 mnesia:change_table_copy_type(person, node(), disc_copies) % 添加远程表的本地备份 mnesia:add_table_copy(person, &#39;node3@host&#39;, ram_copies) % 迁移表备份 表存储类型不变 mnesia:move_table_copy(person, &#39;node3@host&#39;, &#39;node4host&#39;) % 删除表的本节点备份 mnesia:del_table_copy(person, &#39;node3@host&#39;) % 对表的元数据和所有记录进行热升级 mnesia:transform_table(Tab, Fun, NewAttributeList, NewRecordName) 这篇FAQ中归纳了mnesia集群的大多数问题 在新节点动态加入集群的过程中，如果新节点mnesia已经启动，启动的节点会尝试将其表定义与其它节点带来的表定义合并。这也应用于模式表自身的定义 mnesia会同步集群中节点上所有的表信息，如果某节点需要自己本地维护一张表而不希望共享该表，可以在创建表时指定local_content属性。该类型的表表名对mnesia可见，但每个节点写入的数据不会被同步，即每个节点都只能看到自己写入的数据 mnesia后台同步时，会形成一个全联通网络(即使集群节点都是hidden节点) 如果新加入节点和已有集群的schema表都是disc_copies，则会merge schema failed 特性总结mneisa的优势: 与Erlang的完美契合，记录字段可以是任意Erlang Term，具备强大的描述能力 和传统数据库一样，支持事务，索引，分片等特性 分布式特性，表的存储类型和位置对应用透明，支持分布式事务 强大的动态配置能力，包括集群的动态伸缩，表的动态配置，增删，转移，升级等 mnesia缺点： 多节点事务带来的开销，尽可能少使用事务(在逻辑上配合做处理) mnesia全联通网络的维护开销，在使用时需要控制集群节点数量 不适合存储大量数据，这会带来网络负载 参考文档 Erlang Mnesia Man Page Building A Mnesia Database Mnesia 中文版 用户手册","tags":[{"name":"erlang","slug":"erlang","permalink":"http://wudaijun.com/tags/erlang/"}]},{"title":"C++ 编译模型","date":"2015-03-29T16:00:00.000Z","path":"2015/03/cpp-compile-model/","text":"C++继承了C的编译模型，而C是一门古老的语言，它的编译链接模型受限于当时的硬件条件限制，并且该模型也足够用于简洁的C。而C++继承了这些机制之后，引发了更为复杂的一些问题。 C 编译模型首先简要介绍一下C的编译模型： 限于当时的硬件条件，C编译器不能够在内存里一次性地装载所有程序代码，而需要将代码分为多个源文件，并且分别编译。并且由于内存限制，编译器本身也不能太大，因此需要分为多个可执行文件，进行分阶段的编译。在早期一共包括7个可执行文件：cc(调用其它可执行文件)，cpp(预处理器)，c0(生成中间文件)，c1(生成汇编文件)，c2(优化，可选)，as(汇编器，生成目标文件)，ld(链接器)。 1. 隐式函数声明为了在减少内存使用的情况下实现分离编译，C语言还支持”隐式函数声明”，即代码在使用前文未定义的函数时，编译器不会检查函数原型，编译器假定该函数存在并且被正确调用，还假定该函数返回int，并且为该函数生成汇编代码。此时唯一不确定的，只是该函数的函数地址。这由链接器来完成。如： 12345int main()&#123; printf(&quot;ok\\n&quot;); return 0;&#125; 在gcc上会给出隐式函数声明的警告，但能编译运行通过。因为在链接时，链接器在libc中找到了printf符号的定义，并将其地址填到编译阶段留下的空白中。PS：用g++编译则会生成错误：use of undeclared identifier &#39;printf&#39;。而如果使用的是未经定义的函数，如上面的printf函数改为print，得到的将是链接错误，而不是编译错误。 2. 头文件有了隐式函数声明，编译器在编译时应该就不需要头文件了，编译器可以按函数调用时的代码生成汇编代码，并且假定函数返回int。而C头文件的最初目的是用于方便文件之间共享数据结构定义，外部变量，常量宏。早期的头文件里，也只包含这三样东西。注意，没有提到函数声明。 而如今在引入将函数声明放入头文件这一做法后，带来了哪些便利和缺陷： 优点： 项目不同的文件之间共享接口。 头文件为第三方库提供了接口说明。 缺点： 效率性：为了使用一个简单的库函数，编译器可能要parse成千上万行预处理之后的头文件源码。 传递性：头文件具有传递性。在头文件传递链中任一头文件变动，都将导致包含该头文件的所有源文件重新编译。哪怕改动无关紧要(没有源文件使用被改动的接口)。 差异性：头文件在编译时使用，动态库在运行时使用，二者有可能因为版本不一致造成二进制兼容问题。 一致性：头文件函数声明和源文件函数实现的参数名无需一致。这将可能导致函数声明的意思，和函数具体实现不一致。如声明为 void draw(int height, int width) 实现为 void draw(int width, int height)。 3. 单遍编译( One Pass )由于当时的编译器并不能将整个源文件的语法树保存在内存中，因此编译器实际上是”单遍编译”。即编译器从头到尾地编译源文件，一边解析，一边即刻生成目标代码，在单遍编译时，编译器只能看到已经解析过的部分。 意味着： C语言结构体需要先定义，才能访问。因为编译器需要知道结构体定义，才知道结构体成员类型和偏移量，并生成目标代码。 局部变量必须先定义，再使用。编译器需要知道局部变量的类型和在栈中的位置。 外部变量(全局变量)，编译器只需要知道它的类型和名字，不需要知道它的地址，就能生成目标代码。而外部变量的地址将留给连接器去填。 对于函数，根据隐式函数声明，编译器可以立即生成目标代码，并假定函数返回int，留下空白函数地址交给连接器去填。 C语言早期的头文件就是用来提供结构体定义和外部变量声明的，而外部符号(函数或外部变量)的决议则交给链接器去做。 单遍编译结合隐式函数声明，将引出一个有趣的例子： 12345678910111213141516void bar()&#123; foo(&apos;a&apos;);&#125;int foo(char a)&#123; printf(&quot;foobar\\n&quot;); return 0;&#125;int main()&#123; bar(); return 0;&#125; gcc编译上面的代码，得到如下错误： test.c:16:6: error: conflicting types for &#39;foo&#39; void foo(char a) ^ test.c:12:2: note: previous implicit declaration is here foo(&#39;a&#39;); 这是因为当编译器在bar()中遇到foo调用时，编译器并不能看到后面近在咫尺的foo函数定义。它只能根据隐式函数声明，生成int foo(int)的函数调用代码，注意隐式生成的函数参数为int而不是char，这应该是编译器做的一个向上转换，向int靠齐。在编译器解析到更为适合的int foo(char)时，它可不会认错，它会认为foo定义和编译器隐式生成的foo声明不一致，得到编译错误。将上面的foo函数替换为 void foo(int a)也会得到类似的编译错误，C语言严格要求一个符号只能有一种定义，包括函数返回值也要一致。 而将foo定义放于bar之前，就编译运行OK了。 C++ 编译模型到目前为止，我们提到的3点关于C编译模型的特性，对C语言来说，都是利多于弊的，因为C语言足够简单。而当C++试图兼容这些特性时(C++没有隐式函数声明)，加之C++本身独有的重载，类，模板等特性，使得C++更加难以理解。 1. 单遍编译C++没有隐式函数声明，但它仍然遵循单遍编译，至少看起来是这样，单遍编译语义给C++带来的影响主要是重载决议和名字解析。 1.1 重载决议12345678910111213141516171819202122#include&lt;stdio.h&gt;void foo(int a)&#123; printf(&quot;foo(int)\\n&quot;);&#125;void bar()&#123; foo(&apos;a&apos;);&#125;void foo(char a)&#123; printf(&quot;foo(char)\\n&quot;);&#125;int main()&#123; bar(); return 0;&#125; 以上代码通过g++编译运行结果为：foo(int)。尽管后面有更合适的函数原型，但C++在解析bar()时，只看到了void foo(int)。 这是C++重载结合单遍编译造成的困惑之一，即使现在C++并非真的单遍编译(想一下前向声明)，但它要和C兼容语义，因此不得不”装傻”。对于C++类是个例外，编译器会先扫描类的定义，再解析成员函数，因此类中所有同名函数都能参加重载决议。 关于重载还有一点就是C的隐式类型转换也给重载带来了麻烦： 12345678910111213141516171819// Case 1void f(int)&#123;&#125;void f(unsigned int)&#123;&#125;void test() &#123; f(5); &#125; // call f(int)// Case 2void f(int)&#123;&#125;void f(long)&#123;&#125;void test() &#123; f(5); &#125; // call f(int)// Case 3void f(unsigned int)&#123;&#125;void f(long)&#123;&#125;void test() &#123; f(5); &#125; // error. 编译器也不知道你要干啥// Case 4void f(unsigned int)&#123;&#125;void test&#123; f(5); &#125; // call f(unsigned int)...void f(long)&#123;&#125; 再加上C++子类到父类的隐式转换，转换运算符的重载… 你必须费劲心思，才能确保编译器按你预想的去做。 1.2 名字查找单遍编译给C++造成的另一个影响是名字查找，C++只能通过源码来了解名字的含义，比如 AA BB(CC)，这句话即可以是声明函数，也可以是定义变量。编译器需要结合它解析过的所有源代码，来判断这句话的确切含义。当结合了C++ template之后，这种难度几何攀升。因此不经意地改动头文件，或修改头文件包含顺序，都可能改变语句语义和代码的含义。 2. 头文件在初学C++时，函数声明放在.h文件，函数实现放在.cpp文件，似乎已经成了共识。C++没有C的隐式函数声明，也没有其它高级语言的包机制，因此，同一个项目中，头文件已经成了模块与模块之间，类与类之间，共享接口的主要方式。 C中的效率性，传递性，差异性，一致性，C++都一个不落地继承了。除此之外，C++头文件还带来如下麻烦： 2.1 顺序性由于C++头文件包含更多的内容：template, typedef, #define, #pragma, class,等等，不同的头文件包含顺序，将可能导致完全不同的语义。或者直接导致编译错误。 2.2 又见重载由于C++支持重载，因此如果头文件中的函数声明和源文件中函数实现不一致(如参数个数，const属性等)，将可能构成重载，这个时候”聪明”的C++编译器不错报错，它将该函数的调用地址交给链接器去填，而源文件中写错了的实现将被认定为一个全新的重载。从而到链接阶段才报错。这一点在C中会得到编译错误，因为C没有重载，也就没有名字改编(name mangling)，将会在编译时得到符号冲突。 2.3 重复包含由于头文件的传递性，有可能造成某上层头文件的重复包含。重复包含的头文件在展开后，将可能导致符号重定义，如： 12345678910111213141516171819// common.hclass Common&#123; // ...&#125;;// h1.h#include &quot;common.h&quot;// h2.h#include &quot;common.h&quot;// test.cpp#include &quot;h1.h&quot;#include &quot;h2.h&quot;int main()&#123; return 0;&#125; 如果common.h中，有函数定义，结构体定义，类声明，外部变量定义等等。test.cpp中将展开两份common.h，编译时得到符号重定义的错误。而如果common.h中只有外部函数声明，则OK，因为函数可在多处声明，但只能在一处定义。关于类声明，C++类保持了C结构体语义，因此叫做”类定义”更为适合。始终记得，头文件只是一个公共代码的整合，这些代码会在预编译期替换到源文件中。 为了解决重复包含，C++头文件常用 #ifndef #define #endif或#pragma once来保证头文件不被重复包含。 2.4 交叉包含C++中的类出现相互引用时，就会出现交叉包含的情况。如Parent包含一个Child对象，而Child类包含Parent的引用。因此相互包含对方的头文件，编译器展开Child.h需要展开Parent.h，展开Parent.h又要展开Child.h，如此无限循环，最终g++给出：error: #include nested too deeply的编译错误。 解决这个问题的方案是前向声明，在Child类定义前面加上 class Parent; 声明Parent类，而无需包含其头文件。前向声明不止可以用于类，还可以用于函数(即显式的函数声明)。前向声明应该被大量使用，它可以解决头文件带来的绝大多数问题，如效率性，传递性，重复包含，交叉包含等等。这一点有点像包(package)机制，需要什么，就声明(导入)什么。前向声明也有局限：仅当编译器无需知道目标类完整定义时。如下情形，类A可使用 class B;： 类A中使用B声明引用或指针； 类A使用B作为函数参数类型或返回类型，而不使用该对象，即无需知道其构造函数和析构函数或成员函数； 2.5 如何使用头文件关于头文件使用的建议： 降低将文件间的编译依赖(如使用前向声明)； 将头文件归类，按照特定顺序包含，如C语言系统头文件，C++系统头文件，项目基础头文件，项目头文件； 防止头文件重复编译(#ifndef or #pragma)； 确保头文件和源文件的一致； 3.总结C语言本身一些比较简单的特性，放在C++中却引起了很多麻烦，主要是因为C++复杂的语言特性：类，模板，各种宏… 举个例子来说，对于一个类A，它有一个私有函数，需要用到类B，而这个私有函数必须出现在类定义即头文件中，因此就增加了A头文件对B的不必要引用。这是因为C++类遵循C结构体的语义，所有类成员都必须出现在类定义中，”属于这个类的一部分”。这不仅在定义上造成不便，也在容易在语义上造成误解，事实上，C++类的成员函数不属于对象，它更像普通函数(虚函数除外)。 而在C中，没有”类的捆绑”，实现起来就要简单多了，将该函数放在A.c中，函数不在A.h中声明。由A.c包含B.h，解除了A.h和B.h之间的关联，这也是C将数据和操作分离的优势之一。 最后，看看其它语言是如何避免这些”坑”的： 对于解释型语言，import的时候直接将对应模块的源文件解析一遍，而不是将文件包含进来； 对于编译型语言，编译后的目标文件中包含了足够的元数据，不需要读取源文件(也就没有头文件一说了)； 它们都避免了定义和声明不一致的问题，并且在这些语言里面，定义和声明是一体的。import机制可以确保只到处必要的名字符号，不会有多余的符号加进来。","tags":[{"name":"c/c++","slug":"c-c","permalink":"http://wudaijun.com/tags/c-c/"}]},{"title":"C++ 右值引用","date":"2015-03-18T16:00:00.000Z","path":"2015/03/cpp-rvalue-referrence/","text":"一. 定义通常意义上，在C++中，可取地址，有名字的即为左值。不可取地址，没有名字的为右值。右值主要包括字面量，函数返回的临时变量值，表达式临时值等。右值引用即为对右值进行引用的类型，在C++98中的引用称为左值引用。 如有以下类和函数: 1234567891011class A&#123;private: int* _p;&#125;;A ReturnValue()&#123; return A();&#125; ReturnValue()的返回值即为右值，它是一个不具名的临时变量。在C++98中，只有常量左值引用才能引用这个值。 1234A&amp; a = ReturnValue(); // error: non-const lvalue reference to type &apos;A&apos; cannot bind to a temporary of type &apos;A&apos; const A&amp; a2 = ReturnValue(); // ok 通过常量左值引用，可以延长ReturnValue()返回值的生命周期，但是不能修改它。C++11的右值引用出场了： A&amp;&amp; a3 = ReturnValue(); 右值引用通过”&amp;&amp;”来声明， a3引用了ReturnValue()的返回值，延长了它的生命周期，并且可以对该临时值进行修改。 二. 移动语义右值引用可以引用并修改右值，但是通常情况下，修改一个临时值是没有意义的。然而在对临时值进行拷贝时，我们可以通过右值引用来将临时值内部的资源移为己用，从而避免了资源的拷贝： 12345678910111213141516171819202122232425262728293031323334353637#include&lt;iostream&gt;class A&#123;public: A(int a) :_p(new int(a)) &#123; &#125; // 移动构造函数 移动语义 A(A&amp;&amp; rhs) : _p(rhs._p) &#123; // 将临时值资源置空 避免多次释放 现在资源的归属权已经转移 rhs._p = nullptr; std::cout&lt;&lt;&quot;Move Constructor&quot;&lt;&lt;std::endl; &#125; // 拷贝构造函数 复制语义 A(const A&amp; rhs) : _p(new int(*rhs._p)) &#123; std::cout&lt;&lt;&quot;Copy Constructor&quot;&lt;&lt;std::endl; &#125; private: int* _p;&#125;;A ReturnValue() &#123; return A(5); &#125;int main()&#123; A a = ReturnValue(); return 0;&#125; 运行该代码，发现Move Constructor被调用(在g++中会对返回值进行优化，不会有任何输出。可以通过-fno-elide-constructors关闭这个选项)。在用右值构造对象时，编译器会调用A(A&amp;&amp; rhs)形式的移动构造函数，在移动构造函数中，你可以实现自己的移动语义，这里将临时对象中_p指向内存直接移为己用，避免了资源拷贝。当资源非常大或构造非常耗时时，效率提升将非常明显。如果A没有定义移动构造函数，那么像在C++98中那样，将调用拷贝构造函数，执行拷贝语义。移动不成，还可以拷贝。 std::moveC++11提供一个函数std::move()来将一个左值强制转化为右值： 12A a1(5);A a2 = std::move(a1); 上面的代码在构造a2时将会调用移动构造函数，并且a1的_p会被置空，因为资源已经被移动了。而a1的生命周期和作用域并没有变，仍然要等到main函数结束后再析构，因此之后对a1的_p的访问将导致运行错误。 std::move乍一看没什么用。它主要用在两个地方： 帮助更好地实现移动语义 实现完美转发(下面会提到) 考虑如下代码： 1234567891011121314class B&#123;public: B(B&amp;&amp; rhs) : _pb(rhs._pb) &#123; // how can i move rhs._a to this-&gt;_a ? rhs._pb = nullptr; &#125;private: A _a; int * pb;&#125; 对于B的移动构造函数来说，由于rhs是右值，即将被释放，因此我们不只希望将_pb的资源移动过来，还希望利用A类的移动构造函数，将A的资源也执行移动语义。然而问题出在如果我们直接在初始化列表中使用：_a(rhs._a) 将调用A的拷贝构造函数。因为参数 rhs._a 此时是一个具名值，并且可以取址。实际上，B的移动构造函数的参数rhs也是一个左值，因为它也具名，并且可取址。这是在C++11右值引用中让人很迷惑的一点：可以接受右值的右值引用本身却是个左值 这一点在后面的完美转发还会提到。现在我们可以用std::move来将rhs._a转换为右值：_a(std::move(rhs._a))，这样将调用A的移动构造。实现移动语义。当然这里我们确信rhs._a之后不会在使用，因为rhs即将被释放。 三. 完美转发如果仅仅为了实现移动语义，右值引用是没有必要被提出来的，因为我们在调用函数时，可以通过传引用的方式来避免临时值的生成，尽管代码不是那么直观，但效率比使用右值引用只高不低。 右值引用的另一个作用是完美转发，完美转发出现在泛型编程中，将模板函数参数传递给该函数调用的下一个模板函数。如： 12345template&lt;typename T&gt;void Forward(T t)&#123; Do(t);&#125; 上面的代码中，我们希望Forward函数将传入参数类型原封不动地传递给Do函数，即Forward函数接收的左值，则Do接收到左值，Forward接收到右值，Do也将得到右值。上面的代码能够正确转发参数，但是是不完美的，因为Forward接收参数时执行了一次拷贝。 考虑到避免拷贝，我们可以传递引用，形如Forward(T&amp; t)，但是这种形式的Forward并不能接收右值作为参数，如Forward(5)。因为非常量左值不能绑定到右值。考虑常量左值引用：Forward(const T&amp; t)，这种形式的Forward能够接收任何类型(常量左值引用是万能引用)，但是由于加上了常量修饰符，因此无法正确转发非常量左值引用： 1234567891011121314151617void Do(int&amp; i)&#123; // do something...&#125;template&lt;typename T&gt;void Forward(const T&amp; t)&#123; Do(t);&#125;int main()&#123; int a = 8; Forward(a); // error. &apos;void Do(int&amp;)&apos; : cannot convert argument 1 from &apos;const int&apos; to &apos;int&amp;&apos; return 0;&#125; 基于这种情况， 我们可以对Forward的参数进行const重载，即可正确传递左值引用。但是当Do函数参数为右值引用时，Forward(5)仍然不能正确传递，因为Forward中的参数都是左值引用。 下面介绍在 C++11 中的解决方案。 引用折叠C++11引入了引用折叠规则，结合右值引用来解决完美转发问题： 1234typedef const int T;typedef T&amp; TR;TR&amp; v = 1; // 在C++11中 v的实际类型为 const int&amp; 如上代码中，发生了引用折叠，将TR展开，得到 T&amp; &amp; v = 1(注意这里不是右值引用)。 这里的 T&amp; + &amp; 被折叠为 T&amp;。更为详细的，根据TR的类型定义，以及v的声明，发生的折叠规则如下： T&amp; + &amp; = T&amp; T&amp; + &amp;&amp; = T&amp; T&amp;&amp; + &amp; = T&amp; T&amp;&amp; + &amp;&amp; = T&amp;&amp; 上面的规则被简化为：只要出现左值引用，规则总是优先折叠为左值引用。仅当出现两个右值引用才会折叠为右值引用。 再谈转发那么上面的引用折叠规则，对完美转发有什么用呢？我们注意到，对于T&amp;&amp;类型，它和左值引用折叠为左值引用，和右值引用折叠为右值引用。基于这种特性，我们可以用 T&amp;&amp; 作为我们的转发函数模板参数： 123456template&lt;typename T&gt;void Forward(T&amp;&amp; t)&#123; Do(static_cast&lt;T&amp;&amp;&gt;(t));&#125; 这样，无论Forward接收到的是左值，右值，常量，非常量，t都能保持为其正确类型。 当传入左值引用 X&amp; 时： 1234void Forward(X&amp; &amp;&amp; t)&#123; Do(static_cast&lt;X&amp; &amp;&amp;&gt;(t));&#125; 折叠后： 1234void Forward(X&amp; t)&#123; Do(static_cast&lt;X&amp;&gt;(t));&#125; 这里的static_cast看起来似乎是没有必要，而它实际上是为右值引用准备的： 1234void Forward(X&amp;&amp; &amp;&amp; t)&#123; Do(static_cast&lt;X&amp;&amp; &amp;&amp;&gt;(t));&#125; 折叠后： 1234void Forward(X&amp;&amp; t)&#123; Do(static_cast&lt;X&amp;&amp;&gt;(t));&#125; 前面提到过，可以接收右值的右值引用本身却是个左值，因为它具名并且可以取值。因此在Forward(X&amp;&amp; t)中，参数t已经是一个左值了，此时我们需要将其转换为它本身传入的类型，即为右值。由于static_cast中引用折叠的存在，我们总能还原参数本来的类型。 在C++11中，static_cast&lt;T&amp;&amp;&gt;(t) 可以通过 std::forward&lt;T&gt;(t) 来替代，std::forward是C++11用于实现完美转发的一个函数，它和std::move一样，都通过static_cast来实现。我们的Forward函数最终变成了： 12345template&lt;typename T&gt;void Forward(T&amp;&amp; t)&#123; Do(std::forward&lt;T&gt;(t));&#125; 可以通过如下代码来测试：12345678910111213141516171819202122#include&lt;iostream&gt;using namespace std;void Do(int&amp; i) &#123; cout &lt;&lt; &quot;左值引用&quot; &lt;&lt; endl; &#125;void Do(int&amp;&amp; i) &#123; cout &lt;&lt; &quot;右值引用&quot; &lt;&lt; endl; &#125;void Do(const int&amp; i) &#123; cout &lt;&lt; &quot;常量左值引用&quot; &lt;&lt; endl; &#125;void Do(const int&amp;&amp; i) &#123; cout &lt;&lt; &quot;常量右值引用&quot; &lt;&lt; endl; &#125;template&lt;typename T&gt;void PerfectForward(T&amp;&amp; t)&#123; Do(forward&lt;T&gt;(t)); &#125;int main()&#123; int a; const int b; PerfectForward(a); // 左值引用 PerfectForward(move(a)); // 右值引用 PerfectForward(b); // 常量左值引用 PerfectForward(move(b)); // 常量右值引用 return 0;&#125; 四. 附注左值和左值引用，右值和右值引用都是同一个东西，引用不是一个新的类型，仅仅是一个别名。这一点对于理解模板推导很重要。对于以下两个函数1234567891011template&lt;typename T&gt;void Fun(T t)&#123; // do something...&#125;template&lt;typename T&gt;void Fun(T&amp; t)&#123; // do otherthing...&#125; Fun(T t)和Fun(T&amp; t)他们都能接受左值(引用)，它们的区别在于对参数作不同的语义，前者执行拷贝语义，后者只是取个新的别名。因此调用Fun(a)编译器会报错，因为它不知道你要对a执行何种语义。另外，对于Fun(T t)来说，由于它执行拷贝语义，因此它还能接受右值。因此调用Fun(5)不会报错，因为左值引用无法引用到右值，因此只有Fun(T t)能执行拷贝。 最后，附上VS中 std::move 和 std::forward 的源码:123456789101112131415161718192021// movetemplate&lt;class _Ty&gt; inline typename remove_reference&lt;_Ty&gt;::type&amp;&amp; move(_Ty&amp;&amp; _Arg) _NOEXCEPT&#123; return ((typename remove_reference&lt;_Ty&gt;::type&amp;&amp;)_Arg);&#125;// forwardtemplate&lt;class _Ty&gt; inline _Ty&amp;&amp; forward(typename remove_reference&lt;_Ty&gt;::type&amp; _Arg)&#123; // forward an lvalue return (static_cast&lt;_Ty&amp;&amp;&gt;(_Arg));&#125;template&lt;class _Ty&gt; inline _Ty&amp;&amp; forward(typename remove_reference&lt;_Ty&gt;::type&amp;&amp; _Arg) _NOEXCEPT&#123; // forward anything static_assert(!is_lvalue_reference&lt;_Ty&gt;::value, &quot;bad forward call&quot;); return (static_cast&lt;_Ty&amp;&amp;&gt;(_Arg));&#125;","tags":[{"name":"c/c++","slug":"c-c","permalink":"http://wudaijun.com/tags/c-c/"}]},{"title":"skynet gateserver","date":"2015-02-14T16:00:00.000Z","path":"2015/02/skynet-gateserver/","text":"skynet提供一个gateserver用于处理网络事件，位于lualib/snax/gateserver.lua。云风在skynet wiki上介绍了gateserver的功能和使用范例。用户可以通过向gateserver提供一个自定义handle来向gateserver注册事件处理(玩家登录，断开，数据到达等)。 gateserver模块使用C编写的socketdriver和netpack模块，gateserver被加载时，会注册对”socket”(PTYPE_SOCKET)类型消息的处理，并且通过netpack.filter对收到的数据进行分包。分包完成后调用传入的handler对应的处理函数进行处理。它替上层使用者，完成对PTYPE_SOCKET消息的注册分发，以及消息分包。这样在使用时，只需提供一个handler，然后调用gateserver.start(handler)即可。 在skynet中，如果你要自定义你的gate网关服务gate.lua，需要执行以下几步： gateserver = require snax.gateserver gateserver.start(handler)向gateserver注册网络事件处理。 skynet.call(gate, &quot;lua&quot;, &quot;open&quot;, conf)在外部向你定义的gate服务发送启动消息，并传入启动配置(端口，最大连接数等)来启动gate服务。 skynet中也提供了一个gate服务，位于skynet/service/gate.lua，作为使用gateserver的一个范例。gate服务由watchdog启动，gate服务维护外部连接状态，并且转发收到的数据包。skynet提供的gate服务使用agent模式，关于gate服务的工作模式，在skynet 设计综述中有段介绍： Gate 会接受外部连接，并把连接相关信息转发给另一个服务去处理。它自己不做数据处理是因为我们需要保持 gate 实现的简洁高效。C 语言足以胜任这项工作。而包处理工作则和业务逻辑精密相关，我们可以用 Lua 完成。 外部信息分两类，一类是连接本身的接入和断开消息，另一类是连接上的数据包。一开始，Gate 无条件转发这两类消息到同一个处理服务。但对于连接数据包，添加一个包头无疑有性能上的开销。所以 Gate 还接收另一种工作模式：把每个不同连接上的数据包转发给不同的独立服务上。每个独立服务处理单一连接上的数据包。 或者，我们也可以选择把不同连接上的数据包从控制信息包（建立/断开连接）中分离开，但不区分不同连接而转发给同一数据处理服务（对数据来源不敏感，只对数据内容敏感的场合） 这三种模式，我分别称为 watchdog 模式，由 gate 加上包头，同时处理控制信息和数据信息的所有数据；agent 模式，让每个 agent 处理独立连接；以及 broker 模式，由一个 broker 服务处理不同连接上的所有数据包。无论是哪种模式，控制信息都是交给 watchdog 去处理的，而数据包如果不发给 watchdog 而是发送给 agent 或 broker 的话，则不会有额外的数据头（也减少了数据拷贝）。识别这些包是从外部发送进来的方法是检查消息包的类型是否为 PTYPE_CLIENT 。当然，你也可以自己定制消息类型让 gate 通知你 skynet中提供的gate服务使用的agent模式，意味着，一开始，gate将新连接的连接控制信息转发给watchdog，如收到用户连接消息后，watchdog可以完成一些登录验证等，验证完成之后，由watchdog创建并启动agent服务，agent服务启动之后，会立即向gate服务发送一条”foward”消息，表示”现在玩家已经登录完成，你收到的消息可以交给我了”。gate收到”forward”消息会记录agent地址，并将之后玩家的数据消息转发给agent而不是之前watchdog。gate将消息转发给agent时，会通过skynet.redirect将源地址改为玩家地址，方便业务处理。","tags":[{"name":"lua","slug":"lua","permalink":"http://wudaijun.com/tags/lua/"},{"name":"skynet","slug":"skynet","permalink":"http://wudaijun.com/tags/skynet/"}]},{"title":"skynet socketserver","date":"2015-02-12T16:00:00.000Z","path":"2015/02/skynet-socketserver/","text":"1. 异步IOskynet用C编写的sokcet模块使用异步回调机制，通过lualib-src/lua-socket.c导出为socketdriver模块。skynet socket C API使用的异步回调方式是：在启动socket时，记录当前服务handle，之后该socket上面的消息(底层使用epoll机制)通过skynet消息的方式发往该服务。这里的当前服务指的是socket启动时所在的服务，对于被请求方来说，为调用socketdriver.start(id)的服务，对于请求方来说，为调用socketdriver.connect(addr,port)的服务。skynet不使用套接字fd在上层传播，因为在某些系统上fd的复用会导致上层遇到麻烦，skynet socket C API为每个fd分配一个ID，是自增不重复的。 socket C API 的核心是三个poll: socket poll位于skynet-src/socket_poll.h 底层异步IO，监听可读可写状态，对于linux系统，使用的是epoll模型。 socket_server_poll位于skynet-src/socket_server.c 使用socket poll，处理所有套接字上的IO事件和控制事件。socket_server_poll处理这些事件，并返回处理结果(返回一个type代表事件类型，通过socket_message* result指针参数返回处理结果)。 IO事件主要包括可读，可写，新连接到达，连接成功。对于可读事件，socket_server_poll会读取对应套接字上的数据，如果读取成功，返回SOCKET_DATA类型，并且通过result参数返回读取的buffer。同样对于可写事件，会尝试发送缓冲区中的数据，并返回处理结果。 而控制事件指的是上层调用，由于skynet上层使用的是一个id而不是socket fd来代表一个套接字。skynet在该id上做的所有操作(如设置套接字属性，接受连接，关闭连接，发送数据等等)都会被写入特殊的ctrl套接字(recvctrl_fd sendctrl_fd)，这些ctrl fd位于socket_server结构中，是唯一的，因此写入ctrl的控制信息要包括被操作的套接字ID。这些控制信息统一通过socket poll来处理。再在socket_server_poll中，根据id提出对应的socket fd来完成操作。 skynet_socket_poll通过socket poll 和 socket_server_poll，此时数据已就绪，新连接也已经被接受，需要通知上层处理这些数据，而skynet_socket_poll就是来完成这些工作的。它调用socket_server_poll，根据其返回的type和result来将这些套接字事件发送给套接字所属服务(服务handle已由socket_server_poll填充在result-&gt;opaque字段中)。skynet_socket_poll将socket_message* result 和 type 字段组装成skynet_socket_message，并且通过skynet_message消息发送给指定服务，消息类型为PTYPE_SOCKET。这样一次异步IO就完成了。 skynet_socket_poll通过一个单独的线程跑起来，线程入口为_socket函数，位于skynet-src/skynet_start.c。 2. lua层封装skynet socket C API提供的是异步IO，为方便使用，在lua层提供了一个socket(lualib/socket.lua)模块来实现阻塞读写。该模块是对socketdriver的封装。它通过lua协程模拟阻塞读写。 和gateserver模块一样，socket模块对PTYPE_SOCKET类型的消息进行了注册处理，它使用socketdriver.unpack作为该类型消息的unpack函数。socketdriver.unpack并不进行实际的分包，它只解析出原始数据，socket模块会缓存套接字上收到的数据。缓存结构由socketdriver提供。当调用socket.readline时，将通过socketdriver.readline尝试从缓冲区中读取一行数据。如果缓冲区数据不足，则挂起自身，待数据足够时唤醒。虽然底层仍然是异步，但是由于协程的特性，对上层体现为同步。通过socket模块的API读到的数据可以看做原始数据。 3. 消息分包大多数时候，在收到套接字数据时，要按照消息协议进行消息分包。skynet提供一个netpack库用于处理分包问题，netpack由C编写，位于lualib-src/lua-netpack.c。skynet范例使用的包格式是两个字节的消息长度(Big-Endian)加上消息数据。netpack根据包格式处理分包问题，netpack提供一个netpack.filter(queue, msg, size)接口，它返回一个type(“data”, “more”, “error”, “open”, “close”)代表具体IO事件，其后返回每个事件所需参数。 对于SOCKET_DATA事件，filter会进行数据分包，如果分包后刚好只有一条完整消息，filter返回的type为”data”，其后跟fd msg size。如果不止一条消息，那么消息将被依次压入queue参数中，并且仅返回一个type为”more”。queue是一个结构体指针，可以通过netpack.pop弹出queue中的一条消息。 其余type类型”open”，”error”, “close”分别对应于socket_message中的SOCKET_ACCEPT SOCKET_ERROR SOCKET_CLOSE事件。netpack的使用者可以通过filter返回的type来进行事件处理。 netpack会尽可能多地分包，交给上层。并且通过一个哈希表保存每个套接字ID对应的粘包，在下次数据到达时，取出上次留下的粘包数据，重新分包。","tags":[{"name":"lua","slug":"lua","permalink":"http://wudaijun.com/tags/lua/"},{"name":"skynet","slug":"skynet","permalink":"http://wudaijun.com/tags/skynet/"}]},{"title":"c++ disruptor 无锁消息队列","date":"2015-02-09T16:00:00.000Z","path":"2015/02/cpp-disruptor/","text":"前段时间关注到disruptor，一个高并发框架。能够在无锁(lock-free)的情况下处理多生产者消费者的并发问题。它可以看作一个消息队列，通过CAS而不是锁来处理并发。 因此实现了一个C++版本的disruptor，基于ring buffer，实现一个发送缓冲(多生产者，单消费者)。 写入缓冲某个生产者要写入数据时，先申请所需空间(需要共享当前分配位置)，然后直接执行写入，最后提交写入结果(需要共享当前写入位置)。整个写入过程由两个关键共享变量: atomic_ullong _alloc_count和atomic_ullong _write_count。前者负责管理和同步当前分配的空间，后者负责同步当前已经写入的空间。也就是说，整个过程分为三步：申请，写入，提交。 比如，有两个生产者P1和P2。P1申请到大小为50的空间，假设此时_alloc_count=10，那么P1将得到可写入位置10，此时_alloc_count更新为60。P1此时可以执行写入(无需上锁)。这个时候P2开始申请大小为10的空间，它将得到写入位置60，_alloc_count更新为70。因此实际上P1和P2是可以并发写的。如果P2比P1先写完，它会尝试提交，此时由于P1还没有提交它的写入结果，因此P2会自旋等待(不断尝试CAS操作)。直到P1提交写入结果后，P2才能提交。通过CAS可以保证这种提交顺序。提交操作会更新_write_count变量，提交之后的数据便可以被消费者读取使用。 上面的描述并没有提到缓冲区不够的问题，为了判断缓冲区当前可写空间，还需要一个变量 atomic_ullong _idle_count用于记录当前缓冲区空闲大小。该变量在生产者申请空间后减小，在消费者使用数据后变大。初始等于整个ring buffer的大小。 核心代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192SendBuffer::SendBuffer(size_t capacity /* = 65536 */)&#123; size_t fix_capacity = 16; while (fix_capacity &lt; capacity) fix_capacity &lt;&lt;= 1; _capacity = fix_capacity; _capacity_mask = _capacity - 1; _buffer = new char[_capacity]; _alloc_count = 0; _read_count = 0; _write_count = 0; _idle_count = _capacity;&#125;SendBuffer::~SendBuffer()&#123; delete []_buffer;&#125;bool SendBuffer::Push(const char* data, size_t len)&#123; if (nullptr == data || len == 0 || len &gt; _capacity) return false; auto idle = _idle_count.fetch_sub(len); if (idle &gt;= len) &#123; // 1.申请写入空间 auto alloc_start = _alloc_count.fetch_add(len); auto alloc_end = alloc_start + len; // 2.执行写入 auto fix_start = alloc_start &amp; _capacity_mask; auto fix_end = alloc_end &amp; _capacity_mask; if (fix_start &lt; fix_end) &#123; memcpy(_buffer + fix_start, data, len); &#125; else// 分两段写 &#123; auto first_len = _capacity - fix_start; memcpy(_buffer + fix_start, data, first_len); memcpy(_buffer, data + first_len, fix_end); &#125; // 3.提交写入结果 while (true) &#123; auto tmp = alloc_start; if (_write_count.compare_exchange_weak(tmp, alloc_end)) break; &#125; return true; &#125; else &#123; _idle_count.fetch_add(len); return false; &#125;&#125;char* SendBuffer::Peek(size_t&amp; len)&#123; if (_read_count &lt; _write_count) &#123; auto can_read = _write_count - _read_count; auto fix_start = _read_count &amp; _capacity_mask; auto fix_end = (_read_count + can_read) &amp; _capacity_mask; if (fix_start &gt;= fix_end) &#123; // 只返回第一段 can_read = _capacity - fix_start; &#125; len = static_cast&lt;size_t&gt;(can_read); return _buffer + fix_start; &#125; return nullptr;&#125;bool SendBuffer::Pop(size_t len)&#123; if (_read_count + len &lt;= _write_count) &#123; _read_count += len; _idle_count.fetch_add(len); return true; &#125; return false;&#125; 代码看起来不多，理解起来也不难。主要有以下三点： 1. 对原子变量的访问对原子变量的使用要特别小心，由于没有锁的保护，对原子变量的每一次访问都要考虑到它的值已经改变。比如在Push函数的申请空间操作中，你不能通过 1234if(_idle_count &gt; len)&#123; _idle_count.fetch_sub(len)&#125; 来判断空闲空间是否足够，因为在if中它可能大于len，但是当你执行_idle_count.fetch_sub(len)时，它的值可能就改变了，不再满足 &gt; len。同理以下代码也是错的： 12345_idle_count.fetch_sub(len);if(_idle_count &gt; 0)&#123; //....&#125; 对原子变量的访问应该做到”原子性”，即每次逻辑上使用，都只访问一次。这也是和传统锁不一样的地方。而引进_idle_count这个原子变量而不是使用_read_count和_alloc_count来算出空闲空间(_capacity-(_alloc_count-_read_count))也是基于这个原因，多个生产者依赖于这个表达式的值，并且会对表达式的值造成更改(修改_alloc_count)，就会导致P1读取表达式值后，判断空闲空间足够，在P1更改_alloc_count前，P2生产者更改_alloc_count分配了空间，使得空闲空间已经不足。这种读写分步的操作必须通过原子变量来保证访问的一致性。 而为什么我们在Peek中可以通过_write_count - _read_count来得到当前可读数据，是因为我们只有一个消费者依赖于_write_count - _read_count的值，并且其它生产者对_write_count做出的更改对消费者来说是”无害的”，即生产者只会使_write_count增加，让消费者读到更多的数据。 2. 通过CAS保证顺序提交在Push函数中的第三步提交中，生产者自旋等待，直到它前面(按照申请顺序)的所有生产者都已提交完毕，此时_write_count即为本生产者的写入位置alloc_start，代表alloc_start之前的缓冲区都已经提交完成，此时该你提交写入结果了。提交完成之后，更新_write_count，而消费者则根据_write_count来判断哪些内容是可读的。 3. 单消费者无需原子变量最后，由于只有一个消费者，因此_read_count不是原子变量。它只会在Peek和Pop中读取和修改。 源码地址：https://github.com/wudaijun/Code/tree/master/Demo/disruptor","tags":[]},{"title":"lua 协程和状态","date":"2015-01-13T16:00:00.000Z","path":"2015/01/lua-coroutine/","text":"协程(协同式多线程)是一种用户级的非抢占式线程。用户级是指它的切换和调度由用户控制，非抢占指一个协程只有在其挂起(yield)或者协程结束才会返回。协程和C线程一样，有自己的堆栈，自己的局部变量，自己的指令指针，并且和其它协程共享全局变量等信息。用户可以实现自己调度协程，这主要得益于yield函数可以自动保存协程当前上下文，这样当挂起的协程被唤醒(resume)时，会从yield处继续向下执行，看起来就像是一个”可以返回多次的函数”。协程还有一个强大的功能就是可通过resume/yield来交换数据，这样使得它可以用于异步回调：当执行异步代码时，切换协程，执行完成后，再切换回来(附带异步执行结果)。由于切换都是用户控制的，在同一时刻只有一个协同程序在运行(这也是和传统线程最大的区别之一)，因此无需考虑同步和加锁的问题。 Lua协程的相关函数封装在coroutine中，对应的 C API为lua_newthread，lua_resume等。Lua文档中的thread和coroutine是一个概念，但与操作系统的线程是两个东西。 C API通过lua_State维护一个协程的状态(以及Lua虚拟机状态的引用)，协程的状态主要指协程上下文(如交互栈)，Lua虚拟机状态是全局的，可被多个协程共享。以下描述摘自Lua5.3官方文档： An opaque structure that points to a thread and indirectly (through the thread) to the whole state of a Lua interpreter. The Lua library is fully reentrant: it has no global variables. All information about a state is accessible through this structure. A pointer to this structure must be passed as the first argument to every function in the library, except to lua_newstate, which creates a Lua state from scratch. 当调用lua_newstate时，实际上分为两步，1. 创建并初始化一个Lua虚拟机；2.创建一个主线程运行于虚拟机中。调用lua_newthread时，将在已有Lua虚拟机上，创建另一个协程执行环境，该协程与已有协程共享虚拟机状态。这两个函数返回不同的lua_State，但却共享同一个虚拟机状态，因此将lua_State理解为协程执行上下文可能更合适，lua_State本身也是一个类型为thread的GCObject，无需手动释放(Lua也没有提供对应close或destroy接口)。 两个例子pil上关于协程有两个很好的例子。 在生产者消费者例子中，当消费者需要生产者的数据时(相当于一个异步回调)，切换到生产者协程(resume)，生产者开始运行，生产完成后，挂起自己(yield)并且传入生产的数据。此时调度回到消费者协程中，消费者从resume的返回值中得到数据，使用数据，在需要数据时再次唤醒生产者。这样我们像写同步代码一样(resume相当于函数调用，yield相当于函数返回)，完成了异步功能。而无需考虑传统生产者和消费者模型中的同步问题，因为执行顺序都由我们严格控制的。代码如下： 1234567891011121314151617pfun = function() while true do local value = io.read() print(&quot;生产: &quot;, value) coroutine.yield(value) endendcfun = function(p) while true do local _, value = coroutine.resume(p) print(&quot;消费: &quot;, value) endendp = coroutine.create(pfun)cfun(p) 还有个例子是关于模拟多线程下载文件的，每个协程下载一个文件，由我们控制各个协程的调度，当某个协程暂时没有数据可读时(异步读取)，挂起(yield)自己，返回到调度器，开始调度(resume)下一个协程。这样总是能保证将时间片分给读取数据的协程上，而不是等待数据的协程上。当所有协程都没有数据可读时，分配器将进入忙查询，这样会空转CPU，可以通过select函数来优化，在所有协程都没有数据时，让出CPU。最终代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768socket = require &quot;socket&quot;-- 下载文件 在超时时挂起(返回: 连接c) 在接收完成时结束协程(返回: nil)function download(host, file) local c = assert(socket.connect(host, 80)) local count = 0 c:send(&quot;GET &quot;.. file .. &quot; HTTP/1.0\\r\\n\\r\\n&quot;) while true do local s, status = receive(c) if status == &quot;closed&quot; then break end if s then count = count + string.len(s) break end end c:close() print(&quot;-- download &quot;, file, &quot; completed. file size: &quot;, count)endfunction receive(conn) conn:settimeout(0) local s, status = conn:receive(&quot;*a&quot;) if status == &quot;timeout&quot; then coroutine.yield(conn) end return s, statusend-- 保存所有协程local threads = &#123;&#125;-- 创建一个协程 对应下载一个文件function get(host, file) local co = coroutine.create(function() download(host, file) end) table.insert(threads, co)end-- 调度线程function dispatcher() while true do local conns = &#123;&#125; local n = #threads if n == 0 then break end for i = 1,n do local status, c = coroutine.resume(threads[i]) if not c then -- 接收数据完成 即download 函数正常返回 table.remove(threads, i) -- 移除协程 break -- 重新遍历 else table.insert(conns, c) end end if #conns == n then socket.select(conns) end endendget(&quot;www.baidu.com&quot;, &quot;/index.html&quot;)get(&quot;wudaijun.com&quot;, &quot;/2014/12/shared_ptr-reference/&quot;)get(&quot;wudaijun.com&quot;, &quot;/2014/11/cpp-constructor/&quot;)local start = os.time()dispatcher()local cost = os.time()-startprint(&quot;-- cost time: &quot;, cost)","tags":[{"name":"lua","slug":"lua","permalink":"http://wudaijun.com/tags/lua/"}]},{"title":"false sharing","date":"2015-01-10T16:00:00.000Z","path":"2015/01/false-sharing/","text":"在多核的CPU架构中，每一个核心core都会有自己的缓存行(cache line)，因此如果一个变量如果同时存在不同的核心的cache line时，就会出现伪共享（false sharing)的问题。此时如果一个核心修改了该变量，该修改需要同步到其它核心的缓存。 上图说明了伪共享的问题。在核心1上运行的线程想更新变量X，同时核心2上的线程想要更新变量Y。不幸的是，这两个变量在同一个缓存行中。每个线程都要去竞争缓存行的所有权来更新变量。如果核心1获得了所有权，缓存子系统将会使核心2中对应的缓存行失效。当核心2获得了所有权然后执行更新操作，核心1就要使自己对应的缓存行失效。这会来来回回的经过L3缓存，大大影响了性能。如果互相竞争的核心位于不同的插槽，就要额外横跨插槽连接，问题可能更加严重。 我们可以通过padding来确保两个共享变量不位于同一个cache-line中，这对于链表等传统结构的共享(首尾节点通常位于同一cache-line)有重大意义。如下面这个例子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#include&lt;thread&gt;#include &lt;iostream&gt;using namespace std;struct foo &#123; int x;/* int64_t pad1; int64_t pad2; int64_t pad3; int64_t pad4; int64_t pad5; int64_t pad6; int64_t pad7; int64_t pad8; int64_t pad9; int64_t pad10; int64_t pad11; int64_t pad12; int64_t pad13; int64_t pad14; int64_t pad15; int64_t pad16;*/ int y;&#125;;static struct foo f;void sum_a(void)&#123; clock_t start = clock(); int s = 0; int i; for (i = 0; i &lt; 1000000000; ++i) s += f.x; cout &lt;&lt; &quot;sum_a cost: &quot;&lt;&lt; clock()-start &lt;&lt; &quot;ms&quot;&lt;&lt; endl;&#125;void inc_b(void)&#123; clock_t start = clock(); int i; for (i = 0; i &lt; 1000000000; ++i) ++f.y; cout &lt;&lt; &quot;inc_b cost: &quot;&lt;&lt; clock()-start &lt;&lt; &quot;ms&quot;&lt;&lt; endl;&#125;int main()&#123; std::thread t1(sum_a); std::thread t2(inc_b); t1.join(); t2.join(); return 0;&#125; 未添加padding时，在我的机器上运行结果为： inc_b cost: 4692ms sum_a cost: 5722ms 添加padding后，运行结果为： inc_b cost: 2161ms sum_a cost: 2194ms","tags":[]},{"title":"skynet lua服务","date":"2015-01-10T16:00:00.000Z","path":"2015/01/skynet-luaservice/","text":"C模块的导出从skynet核心模块来看，它只认得C服务，每个服务被编译为动态库，在需要时由skynet加载。skynet提供发送消息和注册回调函数的接口，并保证消息的正确到达，并调用目标服务回调函数。其它东西，如消息调度，线程池等，对于用户来说都是透明的。 skynet服务可以由lua编写，因此skynet将C模块核心接口通过skynet/lualib-src/lua-skynet.c导出为 skynet.so提供给lua使用。在lua层，通过skynet/lualib/skynet.lua加载C模块(require &quot;skynet.core&quot;)完成对C API的封装。主要涉及lua服务的加载和退出，消息的发送，回调函数的注册等。用户定义的lua服务通过require &quot;skynet&quot;的接口即可完成服务的注册，启动和退出等。关于skynet lua api可以参见skynet wiki。 skynet.lua 中，提供的比较重要的接口有： 1234567891011121314151617181920-- 注册特定类型消息的处理函数function skynet.dispatch(typename, func)-- 服务启动函数 在lua服务中调用该函数启动服务 并执行用户定义的start_funcfunction skynet.start(start_func)-- 启动一个lua服务，name为lua脚本名字,返回服务地址function skynet.newservice(name, ...)-- 启动一个C服务，第一个参数为服务名字，后续为服务参数。返回服务地址function skynet.launch(...)-- 为服务地址映射一个全局名字 function skynet.name(name, handle)-- 向其它服务发送消息function skynet.send(addr, typename, ...)-- 同步发送消息 并阻塞等待回应 function skynet.call(addr, typename, ...) lua服务如何关联到C核心层下面主要提一下skynet是如何在这套C框架上承载lua服务的。 skynet 预置了一个C服务，叫snlua(位于skynet/service-src/skynet_snlua.c)，这个服务的主要任务就是承载lua服务。一个snlua服务可以承载一个lua服务，可以启动任意份snlua服务。我们直接从snlua这个C服务开始，介绍一个lua服务是如何融合到C框架中的。当需要加载一个名为”console.lua””的服务时，我们将启动一个参数为”console”的snlua服务。主要流程： 调用skynet.launch(“sunlua”, “console”) skynet.launch对应C中的cmd_launch，它通过skynet_context_new加载snlua服务： a.创建服务对应的skynet_context b.加载snlua.so模块，并调用模块导出的snlua_create创建snlua服务，snlua_create会创建一个lua_State，这样每个lua服务拥有自己的lua_State。 c.创建服务消息队列，并为skynet_context绑定唯一handle，将消息队列放入全局消息队列中 d.调用snlua_init初始化服务，在snlua_init中，完成对snlua回调函数的注册。并且构造一条消息，将要加载的lua服务名(“console”)发给自己。 在snlua服务的消息回调函数中，先注销回调函数。然后通过加载并运行一个叫loader.lua的脚本，并解析收到的数据(“console”)来完成实际服务的加载。 loader.lua在指定路径(可配置)下找到console.lua脚本，并执行 console.lua 脚本 此时回调函数就返回了。由于之前已经注销了snlua的回调函数。此时snlua看似”报废”了。而事实在重点在console.lua 当中： 每个skynet lua服务都需要有一个启动函数，通过调用 skynet.start(function ... end )来启动lua服务。在skynet.start中： 1234567c = require &quot;skynet.core&quot;function skynet.start(start_func) c.callback(dispatch_message) skynet.timeout(0, function() init_service(start_func) end)end 通过c.callback注册了lua服务的回调函数dispatch_message，c.callback由skynet.so导出，它最终调用skynet_callback这个函数完成对本服务(当前是snlua)的回调函数注册。dispatch_message也定义于skynet.lua中，它主要的功能是根据消息类型(C层定义于skynet.h中，lua层定义于skynet.lua)将消息分发到lua服务指定的回调函数，前面提到过skynet.dispatch可以注册特定类型的处理函数。c.callback将dispatch_message注册为snlua新的回调函数。此时snlua这个服务就承载了lua服务，因为它收到的消息将通过dispath_message转发到lua服务注册的回调函数中。 那么c.callback如何将一个lua函数(dispatch_message)注册为一个C服务(snlua)的回调函数的呢？在skynet/lualib-src/lua-skynet.c中，c.callback对应的C函数实现如下： 12345678910111213141516171819static int_callback(lua_State *L) &#123; struct skynet_context * context = lua_touserdata(L, lua_upvalueindex(1)); int forward = lua_toboolean(L, 2); luaL_checktype(L,1,LUA_TFUNCTION); lua_settop(L,1); lua_rawsetp(L, LUA_REGISTRYINDEX, _cb); lua_rawgeti(L, LUA_REGISTRYINDEX, LUA_RIDX_MAINTHREAD); lua_State *gL = lua_tothread(L,-1); if (forward) &#123; skynet_callback(context, gL, forward_cb); &#125; else &#123; skynet_callback(context, gL, _cb); &#125; return 0;&#125; _callback将lua服务消息回调dispatch_message以_cb函数地址为key保存到lua注册表中。再将_cb函数作为lua服务的”代理回调函数”注册到C核心框架中。这样真正的回调函数_cb就能够满足C服务回调函数形式。这里C中的_cb和lua中的\\dispatch_message都是预先定义好的，可以通过lua全局注册表做一一映射。 当消息到达snlua时，在_cb中，通过lua_rawgetp(L, LUA_REGISTRYINDEX, _cb);从lua注册表中取出lua服务的真正回调函数dispatch_message，压入消息参数。然后调用dispatch_message。dispatch_message根据消息类型将消息分到到lua服务注册的回调函数中。 总结一下，snlua帮lua服务做了如下工作： 创建服务上下文skynet_context 创建lua_State 分配并绑定唯一handle 创建服务消息队列 执行指定lua服务脚本 在最后一步中，lua服务脚本会通过skynet.start启动服务，后者通过c.callback完成回调函数的替换。之后snlua便成功代理了lua服务，它收到的消息都会转发给lua层的dispatch_message。 launcher服务skynet中所有的lua服务都是通过snlua来承载的，skynet提供了一个lua服务launcher.lua(skynet/service/下)专门用来启动其它lua服务，launcer服务本身通过skynet.launch(“snlua”, “launcher”)来创建，而其它的lua服务则更推荐使用skynet.newservice(“console”)来启动： 123function skynet.newservice(name, ...) return skynet.call(&quot;.launcher&quot;, &quot;lua&quot; , &quot;LAUNCH&quot;, &quot;snlua&quot;, name, ...)end 根据前面skynet.call的原型，skynet.call向名为”.launcher”的服务发送一条类型为”lua”的消息，之后的参数便是消息数据，一般来说，消息的第一个字段代表命令，如这里向”.launcher”服务发送了一个”LAUNCH”命令。launcher.lua的实现比较简单，通过它也能了解lua服务的惯用写法。因此这里我摘录了部分重要代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253local services = &#123;&#125; -- 记录各lua服务的启动时参数local command = &#123;&#125; -- 保存各命令对应的处理函数local instance = &#123;&#125; -- for confirm (function command.LAUNCH / command.ERROR / command.LAUNCHOK)-- 通过skynet.launch完成服务的加载 并返回服务地址local function launch_service(service, ...) local param = table.concat(&#123;...&#125;, &quot; &quot;) local inst = skynet.launch(service, param) local response = skynet.response() if inst then services[inst] = service .. &quot; &quot; .. param instance[inst] = response else response(false) return end return instend-- 加载lua服务function command.LAUNCH(_, service, ...) launch_service(service, ...) return NORETend-- lua服务加载完成 通常在skynet.start完成服务初始化后 发送该命令通知launcherfunction command.LAUNCHOK(address) -- init notice local response = instance[address] if response then response(true, address) instance[address] = nil end return NORETend-- 注册&quot;lua&quot;类型(对应C中的type字段为PTYPE_LUA)消息的回调函数skynet.dispatch(&quot;lua&quot;, function(session, address, cmd , ...) cmd = string.upper(cmd) local f = command[cmd] if f then local ret = f(address, ...) if ret ~= NORET then skynet.ret(skynet.pack(ret)) end else skynet.ret(skynet.pack &#123;&quot;Unknown command&quot;&#125; ) endend)-- lua服务启动函数skynet.start(function() end) 这样lua服务的启动通过launcher服务添加一层沙盒，更加安全。launcher还会记录服务的加载状态，输出日志等。launcher一般在bootstrap.lua中创建，并且为其命名”.launcher”。bootstrap.lua是skynet启动执行的第一个lua脚本。","tags":[{"name":"lua","slug":"lua","permalink":"http://wudaijun.com/tags/lua/"},{"name":"skynet","slug":"skynet","permalink":"http://wudaijun.com/tags/skynet/"}]},{"title":"skynet C模块","date":"2015-01-09T16:00:00.000Z","path":"2015/01/skynet-c-module/","text":"这些天一直在拜读云风的skynet，由于对lua不是很熟悉，也花了一些时间来学习lua。这里大概整理一下这些天学习skynet框架的一些东西。 skynet核心概念为服务，一个服务可以由C或lua实现，服务之间的通信已由底层C框架保证。用户要做的只是注册服务，处理消息。如云风的skynet综述中所说： 作为核心功能，Skynet 仅解决一个问题： 把一个符合规范的 C 模块，从动态库（so 文件）中启动起来，绑定一个永不重复（即使模块退出）的数字 id 做为其 handle 。模块被称为服务（Service），服务间可以自由发送消息。每个模块可以向 Skynet 框架注册一个 callback 函数，用来接收发给它的消息。每个服务都是被一个个消息包驱动，当没有包到来的时候，它们就会处于挂起状态，对 CPU 资源零消耗。如果需要自主逻辑，则可以利用 Skynet 系统提供的 timeout 消息，定期触发。 Skynet 提供了名字服务，还可以给特定的服务起一个易读的名字，而不是用 id 来指代它。id 和运行时态相关，无法保证每次启动服务，都有一致的 id ，但名字可以。 在云风的这篇博客中更详细地介绍道： 这个系统是单进程多线程模型。 每个内部服务的实现，放在独立的动态库中。由动态库导出的三个接口 create init release 来创建出服务的实例。init 可以传递字符串参数来初始化实例。比如用 lua 实现的服务（这里叫 snlua ），可以在初始化时传递启动代码的 lua 文件名。 每个服务都是严格的被动的消息驱动的，以一个统一的 callback 函数的形式交给框架。框架从消息队列里取到消息，调度出接收的服务模块，找到 callback 函数入口，调用它。服务本身在没有被调度时，是不占用任何 CPU 的。框架做两个必要的保证。 一、一个服务的 callback 函数永远不会被并发。 二、一个服务向两一个服务发送的消息的次序是严格保证的。 我用多线程模型来实现它。底层有一个线程消息队列，消息由三部分构成：源地址、目的地址、以及数据块。框架启动固定的多条线程，每条工作线程不断的从消息队列取到消息。根据目的地址获得服务对象。当服务正在工作（被锁住）就把消息放到服务自己的私有队列中。否则调用服务的 callback 函数。当 callback 函数运行完后，检查私有队列，并处理完再解锁。 符合规范的C模块skynet C服务均被编译为动态链接库so文件，由框架在需要时加载并使用。前面说的”符合规范的C模块”指的是一个能被框架正确加载使用的C服务模块应该导出如下三个接口： 12345678// 服务创建接口 返回服务实例数据结构struct xyz* xyz_create(void);// 初始化服务 主要是根据param启动参数初始化服务 并注册回调函数int xyz_init(struct xyz * inst, struct skynet_context *ctx, const char * param)；// 释放服务void xyz_release(struct xyz* inst); 其中”xyz”是C服务名，需要和最终编译的动态库名一致，skynet根据这个名字来查找”xyz.so”并加载。服务模块还需要导出 xyz_create xyz_init xyz_release三个函数用于服务的创建，初始化和释放。xyz_create返回服务自定义的数据结构，代表一个服务实例的具体数据。xyz_init中根据启动参数完成服务的初始化，并且注册回调函数： 1234567891011typedef int (*skynet_cb)( struct skynet_context * context, void * ud, int type, int session, uint32_t source , const void * msg, size_t sz);// 注册消息回调函数cb和回调数据udskynet_callback(struct skynet_context * context, void *ud, skynet_cb cb); 通过skynet_callback可以注册回调函数和回调自定义数据ud(一般就是模块create函数的返回值)，之后每次调用回调函数都会传入ud。 在skynet/service-src/下，定义了四个C服务，其中最简单的是skynet_logger.c，它是C写的一个logger服务。关于C服务的写法一看便知。 C服务上下文skynet_contextskynet_context保存一个C服务相关的上下文。包括服务的消息队列，回调函数cb，回调数据ud，所在模块，以及服务的一些状态等。skynet核心层管理的每个C服务都需要对应一个skynet_context。skynet建立服务的唯一id(handle)到skynet_context的一一对应。 在向服务发送消息时，指定其handle即可。skynet根据该handle找到skynet_context，并将消息push到skynet_context的msgqueue中。skynet还为服务提供了全局名字注册，这样可以通过指定服务名向服务发送消息，skynet会根据name找到handle，最终仍通过handle来找到服务的消息队列。 msgqueue中也保存了其所属服务handle。这样消息调度器在处理到某个msgqueue时，可通过msgqueue中的handle找到skynet_context，并调用其回调函数。","tags":[{"name":"lua","slug":"lua","permalink":"http://wudaijun.com/tags/lua/"},{"name":"skynet","slug":"skynet","permalink":"http://wudaijun.com/tags/skynet/"}]},{"title":"lua 与 C 交互","date":"2014-12-30T16:00:00.000Z","path":"2014/12/lua-C/","text":"lua和C交互的核心就是lua栈，lua和C的所有数据交互都是通过lua栈来完成的。 一. C调用luaC调用lua很简单，通常C以lua作为配置脚本，在运行时读取脚本数据，主要步骤： 加载脚本 luaL_loadfile 运行脚本 lua_pcall 获取数据 lua_getglobal …. 使用数据 lua_tostring lua_pcall … 二. 在lua脚本中调用C：在C程序中，使用lua作为脚本，但是要在运行脚本时，访问C中定义的一些变量或函数。 将C变量或函数(遵从指定函数原型，见下面三 Step 1)push到lua栈中 通过lua_setglobal为当前lua栈顶的函数或变量命名，这样在lua中可通过该名字完成对变量或函数的使用 之后可在加载的lua脚本中使用C变量或函数 三. 将C函数封装为一个库，为lua所用将C函数编译为动态库文件，这样可以在lua主程序中，加载这个库文件，并使用其中的C函数。 Step 1. 在mylib.c中定义给lua调用的C函数 函数原型为： int (lua_State*)如： static int c_addsub(lua_State* L) { double a = luaL_checknumber(L,1); // 获取参数1 double b = luaL_checknumber(L,2); // 获取参数2 lua_pushnumber(L, a+b); // 压入返回值1 lua_pushnumber(L, a-b); // 压入返回值2 return 2; // 两个返回值 } Step 2. 在mylib.c中定义一个注册函数，用于lua在加载C动态库时，调用该函数完成对库中所导出的C函数的注册。如： // 将C模块中要导出的C函数放入到luaL_Reg结构体数组内 static const struct luaL_Reg l[] = { {&quot;addsub&quot;, c_addsub}, {NULL, NULL} // 以NULL标识结束 }; // 该函数在导入C库时调用 完成对库中导出的函数的注册 // 必须是non-static int luaopen_mylib(lua_State* L) { // 完成实际的注册工作 // 注册方式一: luaL_openlib(lua_State* L, const char* name, const luaL_Reg* l, int nup) // L : lua_State // name: 表明该C库被加载后，所导出的函数位于哪一个全局table之下 // 这里是&quot;clib&quot; 那么之后lua中通过clib.addsub完成对C函数的调用 // l : 要导出的函数的lua_Reg结构体数组 // luaL_openlib自动将该数组内的name-function对注册并填充到第二参数指定的table下 // nup : upvalue的个数，如果不为0，则注册的所有函数都共享这些upvalues luaL_openlib(L, &quot;clib&quot;, l, 0); // 注册方式二: luaL_newlibtable + luaL_setfuncs (等价于lua_newlib) // luaL_newlibtable(L, l); // luaL_setfuncs(L, l, 0); // 前两句等价于： // luaL_newlib(L, l); // 将包含name-cfunction键值对的table返回给lua return 1; } 注意上面方式一和方式二的主要区别：前者(luaL_openlib)为name-cfunction对在lua中注册了一个名字(“clib”)。而后者(luaL_newlib)没有，它只是将这个table返回给了lua。可在lua层通过赋值为其命名。自然，通过 luaL_openlib 和 return 1可以将name-cfuncton对注册到两个lua table下。 关于luaL_openlib函数，在官方文档中没有找到它，lua5.2文档中给出的是luaL_newlibtable和lua_setfuncs等新API用以替代以前的luaL_register，而事实上根据前面lua和C交互的基本元素，我们可以自己实现一个类似lua_openlib的注册函数： int luaopen_mylib(lua_State* L) { // luaL_openlib(L, &quot;clib&quot;, clib, 0); int i = 0; lua_newtable(L); // push a new table while(clib[i].name != NULL) { lua_pushstring(L, clib[i].name); // push name lua_pushcfunction(L, clib[i].func); // push function lua_settable(L, -3); // table[name] = function ++i; } lua_setglobal(L, &quot;clib&quot;); // set table name return 1; } 因此实际上将C作为动态库和前面二中的交互核心是一样的，只是将C作为动态库时，需要提供一个”入口函数”，用以在加载该动态库后执行，完成对库中所有导出函数的注册。 Step 3. 将相关C文件编译成动态链接库: 需要说明的是Mac OS X需要使用gcc将mylualib.c编译为动态库，编译选项不同于Linux。具体编译命令（粗体部分不同于Linux，如果不使用这些选项，liblua将会被编译到so文件中并引起“multiple lua vms detected”错误， bundle是Mac使用的文件格式）： gcc -c mylib.c gcc -O2 -bundle -undefined dynamic_lookup -o mylib.so mylib.o Step 4. 在lua中加载C动态库 方式一 : 使用 loadlib --加载C动态库 并将luaopen_mylib函数 导出到mylib变量中 mylib = loadlib(&quot;./mylib.so&quot;, &quot;luaopen_mylib&quot;) --调用mylib() 将执行lua_openmylib函数 完成对C动态库中所有导出函数的注册 --将C中返回的name-cfunction table赋给clualib变量 clualib = mylib() --通过clualib完成C函数的调用 sum, diff = clualib.addsub(5.6, 2.4); --针对于Step 2中的注册方式一，还可以通过luaL_openlib中传入的clib来使用C函数 sum, diff = clib.addsub(5.6, 2.4) loadlib会读取动态库文件的符号表，得到luaopen_mylib函数的实现，并导出到mylib变量中，通过执行mylib()，即可执行luaopen_mylib完成对整个C库导出函数的注册。luaopen_mylib将注册完成后的name-cfunction对返回给lua，lua可以通过clualib = mylib()为这个注册完成之后的table命名。之后可通过clualib调用C函数。 另外，luaL_openlib函数可以直接导出name-cfunction对并为其在lua中注册一个名字，因此通过clib也可以完成对C函数的调用。 方式二 : 使用 require clualib = require(&quot;mylib&quot;) sum,diff = clualib.addsub(5.6, 2.4) -- 对于luaL_openlib完成的注册，仍然可以通过clib来访问C函数 sum, diff = clib.addsub(5.6, 2.4) require的工作原理： 当你在脚本中使用require加载一个模块xxx的时候，首先它会在Lua的环境路径中寻找以xxx命名的DLL，如果找到了，则会在这个DLL中寻找luaopen_xxx的函数用于加载模块。我们只需要将自己需要导出给Lua调用的C内容通过这个函数导出就可以了。 比如我们通过require(“mylib”)来导入模块，lua找到mylib.so库文件，并查找luaopen_mylib函数，然后调用该函数。因此我们需要注意两点： 设置好库文件路径 确保库文件存在 确保库定义了luaopen_mylib函数(而不像前一个方法一样，可以通过loadlib函数手动指定入口函数) require的优势在于自动化，而loadlib方式则更加灵活，loadlib可以指定注册函数名字，注册函数可以无需按照luaopen_xxx格式命名。 在一些库中，使用require(“mylib.core”)之类的格式来导入C模块，没有任何库文件时，通过require的报错可以看到其查找路径和规则： lua: testmylib.lua:1: module &#39;mylib.core&#39; not found: no field package.preload[&#39;mylib.core&#39;] no file &#39;/usr/local/share/lua/5.2/mylib/core.lua&#39; no file &#39;/usr/local/share/lua/5.2/mylib/core/init.lua&#39; no file &#39;/usr/local/lib/lua/5.2/mylib/core.lua&#39; no file &#39;/usr/local/lib/lua/5.2/mylib/core/init.lua&#39; no file &#39;./mylib/core.lua&#39; no file &#39;/usr/local/lib/lua/5.2/mylib/core.so&#39; no file &#39;/usr/local/lib/lua/5.2/loadall.so&#39; no file &#39;./mylib/core.so&#39; no file &#39;/usr/local/lib/lua/5.2/mylib.so&#39; no file &#39;/usr/local/lib/lua/5.2/loadall.so&#39; no file &#39;./mylib.so&#39; 先查找 PATH/mylib/core.so 如果没有，则直接使用 PATH/mylib.so。而C中的导出函数命名则必须为: luaopen_mylib_core(lua_State* L)。","tags":[{"name":"lua","slug":"lua","permalink":"http://wudaijun.com/tags/lua/"}]},{"title":"shared_ptr的引用链","date":"2014-12-11T16:00:00.000Z","path":"2014/12/shared_ptr-reference/","text":"总结下几个使用shared_ptr需要注意的问题: 一. 相互引用链1234567891011121314151617181920212223242526272829class C;class B : public std::enable_shared_from_this&lt;B&gt;&#123;public: ~B()&#123; cout &lt;&lt; &quot;~B&quot; &lt;&lt; endl; &#125; void SetPC(std::shared_ptr&lt;C&gt;&amp; pc)&#123; _pc = pc; &#125; private: std::shared_ptr&lt;C&gt; _pc;&#125;;class C : public std::enable_shared_from_this&lt;C&gt;&#123;public: ~C()&#123; cout &lt;&lt; &quot;~C&quot; &lt;&lt; endl; &#125; void SetPB(std::shared_ptr&lt;B&gt;&amp; pb)&#123; _pb = pb; &#125; private: std::shared_ptr&lt;B&gt; _pb;&#125;;int main()&#123; std::shared_ptr&lt;C&gt; pc = std::make_shared&lt;C&gt;(); std::shared_ptr&lt;B&gt; pb = std::make_shared&lt;B&gt;(); pc-&gt;SetPB(pb); pb-&gt;SetPC(pc); return 0;&#125; 上面的代码中，B和C均不能正确析构，正确的做法是，在B和C的释放函数，如Close中，将其包含的shared_ptr置空。这样才能解开引用链。 二. 自引用还有个比较有意思的例子： 123456789101112131415161718192021222324252627282930313233class C : public std::enable_shared_from_this &lt; C &gt;&#123;public: ~C() &#123; std::cout &lt;&lt; &quot;~C&quot; &lt;&lt; std::endl; &#125; int32_t Decode(const char* data, size_t) &#123; return 0; &#125; void SetDecoder(std::function&lt;int32_t(const char*, size_t)&gt; decoder) &#123; _decoder = decoder; &#125;private: std::function&lt;int32_t(const char*, size_t)&gt; _decoder;&#125;;int main()&#123; &#123; std::shared_ptr&lt;C&gt; pc = std::make_shared&lt;C&gt;(); auto decoder = std::bind(&amp;C::Decode, pc, std::placeholders::_1, std::placeholders::_2); pc-&gt;SetDecoder(decoder); &#125; // C不能正确析构 因为存在自引用 return 0;&#125; 上面的C类包含了一个function，该function通过std::bind引用了一个std::shared_ptr，所以_decoder其实包含了一个对shared_ptr的引用。导致C自引用了自身，不能正确析构。需要在C的Close之类的执行关闭函数中，将_decoder=nullptr，以解开这种自引用。 三. 类中传递下面的例子中有个更为隐蔽的问题： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class Session : public std::enable_shared_from_this &lt; Session &gt;&#123;public: ~Session() &#123; std::cout &lt;&lt; &quot;~C&quot; &lt;&lt; std::endl; &#125; void Start() &#123; // 进行一些异步调用 // 如 _socket.async_connect(..., boost::bind(&amp;Session::ConnectCompleted, this), boost::asio::placeholders::error, ...) &#125; void ConnectCompleted(const boost::system::err_code&amp; err) &#123; if(err) return; // ... 进行处理 // 如 _socket.async_read(..., boost::bind(&amp;Session::ReadCompleted, this), boost::asio::placeholders::error, ...) &#125; void Session::ReadComplete(const boost::system::error_code&amp; err, size_t bytes_transferred) &#123; if (err || bytes_transferred == 0) &#123; DisConnect(); return; &#125; // 处理数据 继续读 // ProcessData(); // _socket.async_read(...) &#125;private: std::function&lt;int32_t(const char*, size_t)&gt; _decoder;&#125;;int main()&#123; &#123; std::shared_ptr&lt;Session&gt; pc = std::make_shared&lt;Session&gt;(); pc-&gt;Start(); &#125; return 0;&#125; 上面Session，在调用Start时，调用了异步函数，并回调自身，如果在回调函数的 boost::bind 中 传入的是shared_from_this()，那么并无问题，shared_ptr将被一直传递下去，在网络处理正常时，Session将正常运行，即使main函数中已经没有它的引用，但是它靠boost::bind”活了下来”，boost::bind会保存传给它的shared_ptr，在调用函数时传入。当网络遇到错误时，函数直接返回。此时不再有新的bind为其”续命”。Session将被析构。 而真正的问题在于，如果在整个bind链中，直接传递了this指针而不是shared_from_this()，那么实际上当函数执行完成后，Session即会析构，包括其内部的资源(如 _socket)也会被释放。那么当boost底层去执行网络IO时，自然会遇到错误，并且仍然会”正常”回调到对应函数，如ReadCompleted，然后在err中告诉你：”由本地系统终止网络连接”(或:”An attempt to abort the evaluation failed. The process is now in an indeterminate state.” )。让人误以为是网络问题，很难调试。而事实上此时整个对象都已经被释放掉了。 注：由于C++对象模型实现所致，成员函数和普通函数的主要区别如下： 成员函数带隐式this参数 成员函数具有访问作用域，并且函数内会对非静态成员变量访问做一些转换,如 _member_data 转换成 this-&gt;_member_data; 也就是说，成员函数并不属于对象，非静态数据成员才属于对象。 因此如下调用在编译期是合法的： ((A*)nullptr)-&gt;Func(); 而如果成员函数A::Func()没有访问A的非静态成员变量，这段代码甚至能正确运行，如: 123456789101112131415161718192021222324class Test&#123;public: void Say() &#123; std::cout &lt;&lt; &quot;Say Test&quot; &lt;&lt; std::endl; &#125; void Set(int data) &#123; _data = data; &#125;private: int _data;&#125;;int main()&#123; // 运行成功 ((Test*)nullptr)-&gt;Say(); // 运行会崩掉，尝试访问空指针所指内存(_data) ((Test*)nullptr)-&gt;Set(1); return 0;&#125; 正因为这种特性，有时候在成员函数中纠结半天，也不会注意到这个对象已经”不正常了”，被释放掉了。 四. shared_ptr 使用总结 尽量不要环引用或自引用，可通过weak_ptr来避免环引用：owner持有child的shared_ptr child持有owner的weak_ptr 如果存在环引用或自引用，记得在释放时解开这个引用链 对于通过智能指针管理的类，在类中通过shared_from_this()而不是this来传递本身 在类释放时，尽量手动置空其所有的shared_ptr成员，包括function","tags":[{"name":"c/c++","slug":"c-c","permalink":"http://wudaijun.com/tags/c-c/"}]},{"title":"NGServer 加入PlayerSession","date":"2014-12-11T16:00:00.000Z","path":"2014/12/ngserver-playersession/","text":"PlayerSession类在之前的网络底层设计中，Player和Session之间通过组合实现弱关联，但仍然有个诟病：Player类和Session类在网络连接到来时一并创建了。这样后面在做断线重连的时候，会有两个Player。而事实上LoginService只管登录认证，登录认证的时候并不需要创建Player类，因此可以延迟Player的创建，将其放在MapService中。而这之前LoginService的登录认证也需要用户的一些基本信息。基于这些，实现了PlayerSession类： 1234567891011121314151617181920212223class PlayerSession : public Session&#123;public: PlayerSession(const std::shared_ptr&lt;Socket&gt; socket, int32_t conn_id); ~PlayerSession(); // ..... // 网络数据解码 int32_t Decode(const char* data, int32_t len); // 发送消息 template&lt;typename MsgT&gt; bool SendMsg(MsgId msgid, MsgT&amp; t); // .... private: int32_t _sid = 0; // 所属服务ID std::shared_ptr&lt;Player&gt; _playerToken; // 玩家指针 SessionState _state = kSessionState_None; // 会话状态 std::string _owner; // 登录用户名&#125;; 解码将在PlayerSession而不是Player中完成，在登录完成之前，LoginService通过PlayerSession与玩家交互，在登录验证完成之后，LoginService将玩家登录信息和PlayerSession一并发送到MapService，MapService完成对Player的创建，并于PlayerSession建立关联： 123456789101112131415161718192021void MapService::OnPlayerLogin(SS_PlayerLogin&amp; msg)&#123; PlayerSessionPtr session = msg.session; if (session == nullptr) return; int64_t playerid = msg.login_info.playerid; // 创建 Player 并与PlayerSession关联 PlayerPtr player = std::make_shared&lt;Player&gt;(playerid, session); player-&gt;SetMapService(dynamic_pointer_cast&lt;MapService&gt;(shared_from_this())); session-&gt;SetPlayerToken(player); session-&gt;SetSid(GetSid()); AddPlayer(player); // 向数据库加载玩家信息 // ... S2D_LoadPlayer loadmsg; loadmsg.playerid = playerid; SendToDB(playerid, kS2D_LoadPlayer, loadmsg);&#125; 之后客户端业务逻辑上与MapService交互，在GameService::Process(UserMessage*)解码时提取出_playerToken，即可通过Player类完成业务逻辑。 加入了PlayerSession之后，消息注册回调机制也更复杂了一些，为了方便管理，这些注册和回调均放在GameService中。","tags":[{"name":"ngserver","slug":"ngserver","permalink":"http://wudaijun.com/tags/ngserver/"}]},{"title":"C++ 构造语义","date":"2014-11-24T16:00:00.000Z","path":"2014/11/cpp-constructor/","text":"本文是《深度探索C++对象模型》的读书笔记，主要根据自己的理解整理一下C++对象构造的实现细节以及其在实际编程中所带来的影响。 一. 构造函数C++95标准对构造函数如下描述： 对于 class X，如果没有任何user-declared constructor，那么会有一个default constructor被隐式声明出来…..一个被隐式声明出来的 default constructor 将是一个trivial(浅薄无能的，没啥用的) constructor …… 上面的话摘自《深度探索C++对象模型》P40，由于其省略了其中c++标准的部分内容，因此很容易造成误解： 编译器隐式生成的构造函数都是 trivial constructor ….. 事实上，描述中提到 default constructor 被隐式声明出来（满足语法需要），而该构造函数是否被编译器合成（实现或定义），取决于编译器是否需要在构造函数中做些额外工作，一个没有被合成的 default constructor 被视为 trivial constructor(这也是c++标准原话的意思)，而当编译器在需要时合成了构造函数，那么该类构造函数将被视为 nontrivial。 另外，一个定义了 user-decalred constructor(用户定义的任何构造函数) 的类被视为具有 nontrivial constructor。 下面将着重讨论编译器隐式声明的构造函数在哪种情况下需要被合成(nontrivial)，哪种情况下无需被合成(trivial)： 考虑下面这个类： 123456class A&#123;private: int _ivalue; float _fvalue;&#125;; 对于类A来说，编译器将为其隐式声明的默认构造函数被视为trivial。因为编译器无需在其声明的构造函数中，对A类对象进行任何额外处理。注意，编译器生成的默认构造函数不会对 _ivalue 和 _fvalue 进行初始化。因此在这种情况下，编译器隐式生成的默认构造函数可有可无，视之为”trivial”。 而对于如下四种情况，编译器隐式生成的默认构造函数(以下简称”隐式构造函数”)是 nontrivial default constructor ： a. 类中有 “带 nontrivial default constructor” 的对象成员注意，这里的notrivial default constructor包括用户定义的任何构造函数或者编译器生成的notrivial构造函数。这实际上是一个递归定义，当类X中有具备notrivial default constructor的对象成员Y _y时，X的隐式构造函数需要调用Y的默认构造函数完成对成员_y的构造。如： 1234567891011class B&#123;private: A _a;&#125;;class C&#123;private: std::string _str;&#125;; 我们说类B中的对象成员 A _a “不带default constructor”，因为它只有一个隐式生成的 trivial default constructor。因此B的隐式构造函数中，无需操心对成员_a的构造。因而实际上B的隐式构造函数也被视为trivial(无关紧要)。而对于类C，由于其对象成员类型string具备用户(库作者)声明的默认构造函数，因此string的构造函数是nontrivial，所以编译器在为C合成的默认构造函数中，需要调用string的默认构造函数来为_str初始化，此时C的构造函数便不再是”无关紧要”的，被视为 nontrivial。 b. 类继承于 “带 nontrivial default constructor” 的基类情形b和情形a类似：当类具有 “带 nontrivial default constructor”的基类时，编译器隐式生成的默认构造函数需要调用基类的默认构造函数确保基类的正确初始化。此时该类构造函数视为nontrivial。 c. 类中有虚函数(或继承体系中有虚函数)在这种情况下，编译器生成的隐式构造函数需要完成对虚函数表vtable的构造，并且将vtable的指针安插到对象中(通常是头四个字节)。此时的隐式构造函数自然是必不可少(nontrivial)。 d. 类的继承体系中具有虚基类和情形c一样，编译器需要在合成的构造函数中，对虚基类进行处理(处理方式和虚函数类似，通过一个指针来指向虚基类，以保证虚基类在其继承体系中，只有一份内容)，这样才能保证程序能在运行中正确存取虚基类数据。被视为nontrivial。 总结编译器隐式声明的默认构造函数，是 trivial or nontrivial，取决于编译器是否需要在构造函数中做一些额外的处理，主要包括对象成员和基类的初始化(取决于对象成员或基类有无nontrivial default constructor)，以及对虚函数和虚基类的处理(取决于在其继承体系中是否有虚函数或虚基类)。这些工作使隐式构造函数不再是可有可无。 不存在以上四种情况并且没有用户定义的任何构造函数时，隐式构造函数也被称作 implicit trivial default constructors。这类构造函数实际上并不会被编译器合成出来。这也是对 trivial 和 nontrivial 的直观理解。 而实际上，即使你定义了自己的构造函数，如果类中满足以上四种情形之一，编译器也会将你的构造函数展开，将必要的处理(如vtable的构造)植入到你的构造函数中(一般是你的构造代码之前)。不过仍然请注意，一旦你定义了自己的构造函数，哪怕该函数什么也不做，该类也将被视为具备 nontrivial constrcutor。 二. 复制构造函数就像 default constructor 一样，C++ Standard 上说，如果 class 没有声明一个 copy constructor，就会有隐式的声明(implicitly declared)或隐式的定义(implicitly defined)出现，和以前一样，C++ Standard 把 copy constructor 区分为 trivial 和 nontrivial 两种，只有 nontrivial 的实例才会被合成于程序之中。决定一个 copy constructor 是否为 trivial 的标准在于 class 是否展现出所谓的 “bitwise copy semantics(按位拷贝语义)”。 “按位拷贝语义”是指该类对象之间的拷贝构造，可以通过简单的”位拷贝”(memcpy)来完成，并且与该对象拷贝的原本语义一致。例如 上面的类A，它便具有按位拷贝语义。因此它的拷贝构造函数也是 trivial copy constructor。这样的拷贝构造函数不会被编译器合成到程序中。直接将其作为内存块拷贝即可(类似于 int double 之类的基本类型)。 那么类何时不具有按位拷贝语义？ 和构造函数一样，当编译器声明的拷贝构造函数需要替程序做一些事情时，视为nontrivial。具有也有如下四种情况： 类中有 “带 nontrivial copy constructor” 的对象成员 类继承于 “带 nontrivial copy constructor” 的基类 类中有虚函数(或继承体系中有虚函数) 类的继承体系中具有虚基类 对于1，2，复制构造函数需要通过调用基类或对象成员的 nontrivial 拷贝构造函数来保证它们的正确拷贝。 而对于3，考虑如下情形： 12345// class Derive public派生于 class BaseDerive d;Derive d2 = d;Base b = d; // 发生切割(sliced)行为 对于d2对象，编译器使用位拷贝并无问题(假设Derive并不存在1，2，4所述情况)，因为d和d2的虚函数表均来自于Derive。而对于Base b = d;编译器需要保证对象b的虚函数表为Base的虚函数表，而不是从对象d直接位拷贝过来的Derive类的虚函数表。否则在通过b调用Derive特有而基类Base没有的虚函数时，会发生崩溃(因为Base的虚函数表不含该函数)。因此对于有虚函数的类，编译器必须对该类的虚函数表”负责”，保证其正确初始化。 对于4，和情况3一样，编译器需要确保被构造的对象指向虚基类的指针(virtual base class point)得到正确初始化。 当类不满足以上四种情况时，我们说它的copy constructor为trivial。编译器不会合成trivial copy constrcutor到程序中。在拷贝对象时，执行简单的内存块拷贝。 三. trivial的一些扩展在std中，提供了对某个类各个trivial属性的判别。如： 1234567891011121314151617181920212223242526272829303132333435363738394041#include &lt;iostream&gt;class A&#123;public: A() &#123; &#125;private: int _i; char* _str;&#125;;class B : public A&#123;private: double _d;&#125;;int main()&#123; // 0 A 有 user-declared constructor std::cout &lt;&lt; std::is_trivially_constructible&lt;A&gt;::value &lt;&lt; std::endl; // 1 A 没有 user-declared copy constructor 并且不含abcd情形 std::cout &lt;&lt; std::is_trivially_copy_constructible&lt;A&gt;::value &lt;&lt; std::endl; // 0 B 需要调用A::A() 完成对基类的构造 std::cout &lt;&lt; std::is_trivially_constructible&lt;B&gt;::value &lt;&lt; std::endl; // 1 B 没有 user-declared copy constructor 并且基类 A 没有 nontrivial copy constructor std::cout &lt;&lt; std::is_trivially_copy_constructible&lt;B&gt;::value &lt;&lt; std::endl; std::cout &lt;&lt; std::is_trivial&lt;A&gt;::value &lt;&lt; std::endl; // 0 std::cout &lt;&lt; std::is_trivial&lt;B&gt;::value &lt;&lt; std::endl; // 0 return 0;&#125; std::is_*是c++11引入的关于类型特性(type_traits)的一些列模板，它们可以在编译器就获得有关类型的特性信息。也就是说： std::cout &lt;&lt; std::is_trivial&lt;A&gt;::value &lt;&lt; std::endl; 在运行时相当于： std::cout &lt;&lt; 1 &lt;&lt; std::endl; 这种编译期获得结果的特性让我们可以结合 static_assert 完成更多的事情。如： static_assert(std::is_trivially_constructible&lt;A&gt;::value, &quot;A is not pod type&quot;); 那么如果A不具备 trivial constructor，那么我们可以在程序编译期得到一个编译错误：A is not pod type std::is_trivial判断一个类型是否为trivial类型。C++标准把trivial类型定义如下： 没有 nontrivial constructor 没有 nontrivial copy constructor 没有 nontrivial move constructor 没有 nontrivial assignment operator 有一个 trivial destructor 由于类A和类B均有 nontrivial constructor 因此它们都不是trivial类型。","tags":[{"name":"c/c++","slug":"c-c","permalink":"http://wudaijun.com/tags/c-c/"}]},{"title":"NGServer 消息的编解码","date":"2014-11-04T16:00:00.000Z","path":"2014/11/ngserver-message-encoder/","text":"消息编解码(或序列化)主要是将消息体由一些标准库容器或自定义的类型，转化成二进制流，方便网络传输。为了减少网络IO，编解码中也可能在存在数据”压缩和解压”，但这种压缩是针对于特定的数据类型，并不是针对于二进制流的。在NGServer的消息编解码中，并不涉及数据压缩。 一. 消息编码格式NGServer的消息分为首部和消息体，首部共四个字节，包括消息长度(包括首部)和消息ID，各占两个字节。消息体为消息编码后的二进制数据。 在消息体中，针对于不同的数据类型而不同编码。对于POD类型，直接进行内存拷贝，对于非POD类型，如标准库容器，则需要自定义编码格式，以下是几种最常见的数据类型编码： std::string 先写入字符串长度，占两个字节，再写入字符串内容。std::vector 先写入vector的元素个数(占两个字节)，在对其元素逐个递归编码(如果元素类型为string，则使用string的编码方式)。std::list 编码方式与vector类似T arr[N] 对于这种类型，不需要写入元素个数，因为在消息结构体中指出了固定长度N，因此可以通过模板推导得到N。所以递归写入N个元素T即可。对于简单数据类型T，如T为char时，可以通过模板特例化对其优化。 二. ProtocolStreamNGServer的消息编解码依靠两个类：ProtocolReader和ProtocolWriter。这两个类派生于ProtocolStream，ProtocolStream简单维护一个用于编码或解码的线性缓冲区，并记录缓冲区的当前状态，如总大小，当前偏移，等等。一个ProtocolStream的缓冲区即代表一条消息，因此它ProtocolReader/ProtocolWriter总是在缓冲区头四个字节中读出或写入消息长度和消息ID。 ProtocolReader从缓冲区中读出消息，也就是解码，由于缓冲区的数据是二进制的，因此我们需要提供需要读出的数据类型。因此ProtocolReader提供的接口如下： 12template&lt;typename T&gt;bool ProtocolReader::AutoDecode(T&amp; t); Decode在缓冲区的当前偏移处，读出数据t，并返回操作结果。而根据T的类型不同，读取方式也不一样，这需要通过模板推导来完成。 三. 数据类型T的类型概括有四种： 基本POD类型，如 int, double, char 等 标准库非POD类型，如 std::string, std::vector, std::list 等 自定义POD类型，如: 12345struct A1&#123; char name[36]; char pwd[36];&#125;; 自定义非POD类型，如： 12345struct A2&#123; string name; vector&lt;int&gt; data;&#125;; C数组类型 由于其推导方式不同 因此单独归为一类 关于c++ POD类型和std::is_pod，std::is_standard_layout，std::is_trivial等函数，可参见下面两篇博客： http://m.oschina.net/blog/156796 http://www.cnblogs.com/hancm/p/3665998.html 这里说的POD指的是 std::is_trivial::value &amp;&amp; std::is_standard_layout::value 四. ProtocolReader解码推导流程推导流程如下： 1.如果T是C数组类型 (std::is_array::value == true)，那么下一个推导模型应该为： 12template&lt;typename T, size_t arraySize&gt;bool ProtocolReader::DecodeArray(const T (&amp;arr)[arraySize]); 如此便能推导出数组的元素类型，以及数组的大小 注：std::is_array用于判别一个类型是否为C风格数组类型，对于c++的容器vector，std::is_array","tags":[{"name":"ngserver","slug":"ngserver","permalink":"http://wudaijun.com/tags/ngserver/"}]},{"title":"NGServer 消息的注册与回调","date":"2014-11-04T16:00:00.000Z","path":"2014/11/ngserver-message-callback/","text":"在前面Service框架的介绍中，提到在GameService的ProcessMsg(UserMessage*)和ProcessMsg(InsideMessage*)中，都完成了消息的回调处理。消息响应函数的注册是在服务初始化(Init())中完成的。需要注册和回调的消息有InsideMessage和UserMessage，对于InsideMessage，响应函数只有一种形式：即为响应服务的成员函数。而对于UserMessage，由于UserMessage有Player指针，响应函数则会有多种形式： 作为注册Service的成员函数，并且将Player作为第一个参数。这常在登录和注册流程中发生，如 LoginService::OnPlayerLogin(Player&amp; player, const C2S_Login&amp; msg)。 登录和注册的验证流程在LoginService中统一处理。 作为Player的成员函数，当Player登录成功后，此时客户端与服务器进行的交互都是基于业务逻辑的，因此应在Player的成员函数处理。如 Player::OnEnterGate(const C2S_EnterGate&amp; msg) 其它响应函数，如全局函数。 事实上，基于UserMessage中的Player指针，我们可以实现上面的调用方式，现在就需要通过一种或多种的注册回调机制，来实现对各种响应函数形式的注册和回调。 使用消息注册与回调消息的注册通过指定消息ID和消息响应函数来完成，注册函数主要有如下形式： 123456789101112bool MapService::Init()&#123; // 注册用户消息 响应函数原型： void MapService::OnWorldChat(Player* player, C2S_WorldChat&amp; msg) RegistPlayer(MsgId::kC2S_WorldChat, &amp;MapService::OnWorldChat, this); // 注册用户消息 响应函数原型： void Player::OnEnterGate(const C2S_EnterGate&amp; msg) RegistPlayer(yuedong::protocol::kC2S_EnterGate, &amp;Player::OnEnterGate); // 注册用户消息 响应函数原型：void Test(Player* player, Test); // 注册响应服务之间的内部消息 RegistInside(SSMsgId::kSS_PlayerLogin, &amp;MapService::OnPlayerLogin, this); return true;&#125; 上面注册了三种主要消息，通过RegistPlayer注册玩家消息，通过RegistInside注册内部消息。RegistPlayer通过模板推导和函数重载完成了三种响应函数原型的注册。下面以RegistPlayer为例，讲述消息注册机的内部机制： 1234567891011121314151617181920212223242526272829class GameService : public Service&#123;// ....// 消息注册public: // 注册第一个参数为Player*的回调函数 // 当 F 模板推导为全局函数时，第一个参数为Player* // 如 void Test(Player*, C2S_Test&amp;) // 当 F 模板推导为Player成员函数时，将解析出来的Player*直接作为this指针调用该成员函数 // 如 Player::OnEnterGate(C2S_EnterGate&amp;) template&lt;typename MsgEnum, typename F&gt; void RegistPlayer(MsgEnum msgid, F f) &#123; _calltype[(uint16_t)msgid] = cbPlayerAgent; _player_delegate.Regist((uint16_t)msgid, f); &#125; // 注册第一个参数为Player*的MapService成员函数 // 如MapService::OnWorldChat(Player*, C2S_WorldChat&amp;) template&lt;typename MsgEnum, typename F, typename ObjT&gt; void RegistPlayer(MsgEnum msgid, F f, ObjT* obj) &#123; _calltype[(uint16_t)msgid] = cbPlayerAgent; _player_delegate.Regist((uint16_t)msgid, f, obj); &#125;//....private: DelegateManager&lt;std::pair&lt;Player*, ProtocolReader&amp;&gt;&gt; _player_delegate;&#125;; 在bool MapService::ProcessMsg(UserMessage* msg)中回调响应函数： 12345678910111213141516171819202122232425262728293031323334353637383940414243bool MapService::ProcessMsg(UserMessage* msg)&#123; UserMessageT&lt;PlayerPtr&gt;* msgT = dynamic_cast&lt;UserMessageT&lt;PlayerPtr&gt;*&gt;(msg); if (msgT == nullptr) return true; PlayerPtr player = msgT-&gt;GetClient(); int32_t sid = player-&gt;GetSid(); // 不是发送给当前服务的消息 转发 if (sid != _sid) &#123; return ServiceManager::Send(sid, msg); &#125; // 客户端断开连接 if (msg-&gt;_len == 0) &#123; player-&gt;Offline(); _session_manager-&gt;RemoveSession(player-&gt;GetConnId()); return true; &#125; ProtocolReader reader(msg-&gt;_data, msg-&gt;_len); uint16_t msgid = reader.ReadMsgId(); CallBackType cbType = _calltype[msgid]; switch (cbType) &#123; case cbPlayerDelegate: auto arg = std::pair&lt;Player*, ProtocolReader&amp;&gt;(player.get(), reader); _player_delegate.Call(msgid, arg); break; //case ... // break; default: break; &#125; return true;&#125; 在bool MapService::ProcessMsg(UserMessage* msg)中，取出UserMessage中的PlayerPtr指针，将其与ProtocolReader一起打包成std::pair，而事实上，这个pair才是最终的解码器，在这一点上，也可以专门写一个UserMessageReader类来读取UserMessage的Player指针，以及消息数据。后面也会向这方面改进。可以注意到这个pair也是 _player_delegate的DelegateManager模板参数,下面介绍DelegateManager. DelegateManagerDelegateManager是一个模板类，它第一个模板参数Decoder，是解码器 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109// AutoCall.htemplate&lt;typename Decoder, size_t Capacity = 65535&gt;class DelegateManager&#123; typedef typename IDelegate&lt;Decoder&gt;* DelegatePtr; DelegatePtr _caller[Capacity];public: DelegateManager() &#123; memset(_caller, 0, sizeof(_caller)); &#125; ~DelegateManager() &#123; for (size_t i = 0; i &lt; Capacity; i++) &#123; if (_caller[i] != nullptr) &#123; delete _caller[i]; _caller[i] = nullptr; &#125; &#125; &#125; bool Call(uint16_t id, Decoder&amp; s) &#123; if (_caller[id] != nullptr) return _caller[id]-&gt;Call(s); else return false; &#125; // 内部注册接口 DelegatePtr Regist(uint16_t id, DelegatePtr dp) &#123; if (_caller[id] != nullptr) delete _caller[id]; _caller[id] = dp; return dp; &#125;// 外部具体注册方式 省略了函数原型不完全匹配时的注册接口 此时参数类型需要显式给出 在调用时隐含转换public: // 完全匹配 全局函数 template&lt;typename R&gt; DelegatePtr Regist(uint16_t id, R(*f)()) &#123; return Regist(id, CreateDelegate0&lt;Decoder&gt;(f));; &#125; template&lt;typename R, typename T1&gt; DelegatePtr Regist(uint16_t id, R(*f)(T1)) &#123; return Regist(id, CreateDelegate1&lt;Decoder, T1&gt;(f)); &#125; template&lt;typename R, typename T1, typename T2&gt; DelegatePtr Regist(uint16_t id, R(*f)(T1, T2)) &#123; return Regist(id, CreateDelegate2&lt;Decoder, T1, T2&gt;(f)); &#125; // 完全匹配 成员函数 template&lt;typename R, typename ObjT&gt; DelegatePtr Regist(uint16_t id, R(ObjT::*f)(), ObjT* obj) &#123; std::function&lt;R()&gt; bindf = std::bind(f, obj); return Regist(id, CreateDelegate0&lt;Decoder&gt;(bindf)); &#125; template&lt;typename R, typename ObjT, typename T1&gt; DelegatePtr Regist(uint16_t id, R(ObjT::*f)(T1), ObjT* obj) &#123; std::function&lt;R(T1)&gt; bindf = std::bind(f, obj, std::placeholders::_1); return Regist(id, CreateDelegate1&lt;Decoder, T1&gt;(bindf)); &#125; template&lt;typename R, typename ObjT, typename T1, typename T2&gt; DelegatePtr Regist(uint16_t id, R(ObjT::*f)(T1, T2), ObjT* obj) &#123; std::function&lt;R(T1, T2)&gt; bindf = std::bind(f, obj, std::placeholders::_1, std::placeholders::_2); return Regist(id, CreateDelegate2&lt;Decoder, T1, T2&gt;(bindf)); &#125; // 完全匹配 成员函数 该成员函数的this指针从Decoder中读取 // 这里必须要使用bind函数 预留出this指针的位置 template&lt;typename R, typename ObjT&gt; DelegatePtr Regist(uint16_t id, R(ObjT::*f)()) &#123; auto bindf = std::bind(f, placeholders::_1); return Regist(id, CreateDelegate1&lt;Decoder, ObjT*&gt;(bindf)); &#125; template&lt;typename R, typename ObjT, typename T1&gt; DelegatePtr Regist(uint16_t id, R(ObjT::*f)(T1)) &#123; auto bindf = std::bind(f, placeholders::_1, placeholders::_2); return Regist(id, CreateDelegate2&lt;Decoder, ObjT*, T1&gt;(bindf)); &#125; template&lt;typename R, typename ObjT, typename T1, typename T2&gt; DelegatePtr Regist(uint16_t id, R(ObjT::*f)(T1, T2)) &#123; auto bindf = std::bind(f, placeholders::_1, placeholders::_2, placeholders::_3); return Regist(id, CreateDelegate3&lt;Decoder, ObjT*, T1, T2&gt;(bindf)); &#125;&#125;; DelegateManager管理所有消息ID到消息响应的映射，并提供注册和回调结果。Regist的多种重载识别出需要创建的Delegate对象，由DelegateManager统一管理。 注册主要通过Regist函数的重载和模板推导来进行三种注册方式(实际上不止三种)：全局函数，Service成员函数，Player成员函数。 DelegateManager中，通过Delegate类来代理响应函数。CreateDelegate用于创建响应函数对应的Delegate： 123456789101112131415161718// AutoCall.htemplate&lt;typename Decoder, typename FuncT&gt;IDelegate&lt;Decoder&gt;* CreateDelegate0(FuncT f)&#123; return new Delegate0&lt;Decoder, FuncT&gt;(f);&#125;template&lt;typename Decoder, typename T1, typename FuncT&gt;IDelegate&lt;Decoder&gt;* CreateDelegate1(FuncT f)&#123; return new Delegate1&lt;Decoder, T1, FuncT&gt;(f);&#125;template&lt;typename Decoder, typename T1, typename T2, typename FuncT&gt;IDelegate&lt;Decoder&gt;* CreateDelegate2(FuncT f)&#123; return new Delegate2&lt;Decoder, T1, T2, FuncT&gt;(f);&#125; 最终的Delegate，需要保存回调函数，并提供调用接口Call: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475template &lt;typename Decoder&gt;class IDelegate&#123;public: virtual ~IDelegate()&#123;&#125; virtual bool Call(Decoder&amp; s) = 0;&#125;;/***********************************************************//* 默认的Delegate，所有参数都通过Decode全局函数解码得出 *//***********************************************************/// 0个参数的响应函数template&lt;typename Decoder, typename FuncT&gt;class Delegate0 : public IDelegate &lt; Decoder &gt;&#123; FuncT _func;public: Delegate0(FuncT func) : _func(func)&#123;&#125; bool Call(Decoder&amp; s) override &#123; _func(); return true; &#125;&#125;;// 1个参数的响应函数template&lt;typename Decoder, typename T1, typename FuncT&gt;class Delegate1 : public IDelegate &lt; Decoder &gt;&#123; FuncT _func;public: Delegate1(FuncT func) : _func(func)&#123;&#125; bool Call(Decoder&amp; s) override &#123; std::remove_const &lt; std::remove_reference&lt;T1&gt;::type &gt;::type t1; if (!Decode(s, t1)) return false; _func(t1); return true; &#125;&#125;;// 2个参数template&lt;typename Decoder, typename T1, typename T2, typename FuncT&gt;class Delegate2 : public IDelegate &lt; Decoder &gt;&#123; FuncT _func;public: Delegate2(FuncT func) : _func(func)&#123;&#125; bool Call(Decoder&amp; s) override &#123; std::remove_const&lt; std::remove_reference&lt;T1&gt;::type &gt;::type t1; std::remove_const&lt; std::remove_reference&lt;T2&gt;::type &gt;::type t2; if (!Decode(s, t1)) return false; if (!Decode(s, t2)) return false; _func(t1, t2); return true; &#125;&#125;; Delegate保存回调函数，并且提供调用接口，调用接口Call仅有一个参数，就是解码器，也是DelegateManager的模板参数。对于我们的_player_delegate来说，就是pair。而上面的Delegate类是默认实现，通过Decode全局函数完成对Decoder的解码，在前面消息编解码中提到过，ProtocolReader实现了这样一个接口。而对于我们的pair，需要特例化，方式一是特例化Decode，方式二是特例化Delegate类。我们采用方法二： 123456789101112131415161718192021222324252627282930313233343536373839404142// AutoCallSpecial.h/*******************************************************************************************************//* 特例化Decoder: std::pair&lt;T1, ProtocolReader&amp;&gt; T1是响应函数的第一个参数 其他参数从ProtocolReader中读取 *//*******************************************************************************************************/// 带一个参数 T1template&lt;typename T1, typename FuncT&gt;class Delegate1&lt;std::pair&lt;T1, ProtocolReader&amp;&gt;, T1, FuncT&gt; : public IDelegate &lt; std::pair&lt;T1, ProtocolReader&amp;&gt; &gt;&#123; FuncT _func;public: Delegate1(FuncT f) : _func(f)&#123;&#125; bool Call(std::pair&lt;T1, ProtocolReader&amp;&gt;&amp; s) override &#123; _func(s.first); return true; &#125;&#125;;// 带两个参数 第一个参数为T1 第二个参数从ProtocolReader中读取template &lt; typename T1, typename T2, typename FuncT &gt; class Delegate2&lt;std::pair&lt;T1, ProtocolReader&amp;&gt;, T1, T2, FuncT&gt; : public IDelegate &lt; std::pair&lt;T1, ProtocolReader&amp;&gt; &gt; &#123; FuncT _func;public: Delegate2(FuncT f) : _func(f)&#123;&#125; bool Call(std::pair&lt;T1, ProtocolReader&amp;&gt;&amp; s) override &#123; std::remove_const&lt; std::remove_reference&lt;T2&gt;::type &gt;::type t2; if (!Decode(s.second, t2)) return false; _func(s.first, t2); return true; &#125;&#125;; 如果通过一个UserMessageReader来对UserMessage特殊解码的话，便可以直接特例化Decode，更加简便一些。 AutoCallSpecial.h中还对InsideMessage完成了特例化，而消息的回调方式也不仅限于cbPlayerDelegate一种。添加一种自定义的回调方式也比较简单： 先自定义一个解码器，将所需参数包含进去，解码器可以是个自定义类，也可以是个容器或其它，将其作为DelegateManager的模板参数 在CallBackType中添加该回调类型 在对应ProcessMsg中，组建自己的解码器，调用DelegateManager::Call函数 DelegateManager会最终调到 Delegate::Call 因此如果有必要 需要对Delegate进行特例化，保证使用你的解码器能正确解码 或者直接使用默认Delegate类中的Decode方式，特例化全局Decode函数。","tags":[{"name":"ngserver","slug":"ngserver","permalink":"http://wudaijun.com/tags/ngserver/"}]},{"title":"NGServer Session -> Player","date":"2014-10-30T16:00:00.000Z","path":"2014/10/ngserver-player/","text":"在服务器中，一般都有一个代表客户端或玩家的类，用来处理一些相关逻辑并保存必要数据。也就是NGServer中的Player类，在网络模型中，一般一个Player对应一次会话(Session)，因此在很多服务器模型中，客户端类直接从Session类派生，这样客户端可以直接通过父类Session的接口发送数据，并且通过实现Sessoin的虚函数对数据进行处理。这种模型的好处在于简单，客户端类能够完全控制网络IO，并且对IO事件进行及时地处理。比如连接断开，那么客户端类可以通过实现Session的OnClose()函数完成一些业务逻辑上的处理，比如保存用户数据。而这种编程模型，将客户端和网络会话的耦合性提到了最高：Client is a Session。方便的同时，很大程度上限制了模块的可拓展性，比如客户端的断线重连，由于这种继承关系，导致Session在销毁的同时必然导致Client”逻辑上”的断线，这样玩家重连的时候，数据只能重新加载，建立新的Session和Client。这种情况还会发生在客户端异处登录时，原有客户端被挤下线的同时，逻辑上的数据也丢失了，而新的客户端将重新加载数据。除了断线重连之外，该模型还会造成不必要的编译依赖。因此我们需要将逻辑上的客户端和底层的网络Session解耦。 一种可行的解耦方式是让Session和Player以”包含”的方式并存。即让Session指针或引用作为Player的一个成员。如此数据的发送仍然比较简单，而数据的接收和处理则需要Session通知Player类，这里有两种方式： 让Session也包含一个Player的引用，如此Session在收到数据或连接关闭时也能调用Player接口进行业务逻辑上的处理。 通过std::bind直接让Player的数据解码接口(Decoder)交给Session，Session的数据接收和关闭均通过Decoder交由Player,如此实现更弱的耦合性。 NGServer采用第二种方式： 123456789101112131415void PlayerManager::OnConnect(const std::shared_ptr&lt;Socket&gt;&amp; socket)&#123; uint32_t id = ++_connect_id; // 创建Player和Session 并将Player和Session关联 std::shared_ptr&lt;Session&gt; session = std::make_shared&lt;Session&gt;(socket, id); std::shared_ptr&lt;Player&gt; player = std::make_shared&lt;Player&gt;(session, LoginService::sDefaultSid); std::function&lt;int32_t(const char*, size_t len)&gt; decoder = std::bind(&amp;Player::Decode, player, std::placeholders::_1, std::placeholders::_2); player-&gt;SetConnId(id); session-&gt;SetDecoder(decoder); AddPlayer(player); session-&gt;StartRecv();&#125; PlayerManager管理所有Player的连接，它继承于AsyncTcpListener，一个连接监听器，提供OnConnect接口处理客户端连接事件。因此PlayerManager负责Session和Player的创建和管理，并将Session和Player关联。当有新用户连接时，在OnConnect中，创建Player和Session，并相互关联。Session将收到的数据通过Player::Decode解码，该函数返回解包完成后缓冲区的剩余长度，以便Session调整缓冲区。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// Session收到新的数据void Session::ReadComplete(const boost::system::error_code&amp; err, size_t bytes_transferred)&#123; if (err || bytes_transferred == 0) &#123; DisConnect(); return; &#125; _recv_total += bytes_transferred; _recv_off += bytes_transferred; // 处理数据 if (ProcessData()) &#123; // 继续接收数据 AsyncReadSome(); &#125;&#125;// 对缓冲区中的数据解包 返回false则断开连接bool Session::ProcessData()&#123; assert(_decoder); // 将数据交由解码器处理 返回处理之后的缓冲区剩余字节数 返回-1表示服务器主动断线 int32_t remain = _decoder(_recv_buf,_recv_off); // 服务器断开连接 if (remain &lt; 0) &#123; ShutDown(ShutDownType::shutdown_receive); DisConnect(); return false; &#125; // 处理之后的偏移 if (remain &gt; 0 &amp;&amp; remain &lt; kBufferSize) &#123; size_t remain_off = _recv_off - remain; _recv_off = (size_t)remain; memcpy(_recv_buf, _recv_buf + remain_off, _recv_off); &#125; else &#123; _recv_off = 0; &#125; return true;&#125; 网络底层部分到此结束，焦点将由Player::Decode转向逻辑层。","tags":[{"name":"ngserver","slug":"ngserver","permalink":"http://wudaijun.com/tags/ngserver/"}]},{"title":"NGServer Service框架","date":"2014-10-30T16:00:00.000Z","path":"2014/10/ngserver-service/","text":"NGServer的核心概念便是服务(Service)，它对逻辑层表现为一个线程，处理各种特定的相关业务。如日志服务(LogService)，数据库服务(DBService)，登录服务(LoginService)。服务之间通过消息进行交互。Service实际上并不是一个独立线程，Service与线程是一种”多对多”的关系。即所有的Service通过ServiceManager来管理，后者维护一个线程池，并将线程池与”服务池”以某种调度方式关联，让线程充分被利用。 下面由下至上对Service框架和运行机制简单阐述： Message定义NGServer中的消息定义于Message.h中，主要定义了如下几种消息，它们的继承体系如下： Message实现对消息的最高抽象，并不包含任何数据，只提供 GetType纯虚函数接口。用于标识消息类型。 UserMessage是用户发来的消息，内部包含 char* data , size_t len数据成员。 UserMessageT是更具体的用户消息，它是一个模板类，多了一个成员字段 T* user。在本服务器中 T 就是 Player 这样每条消息和包含一个用户指针。这在Service处理以及函数回调的时候非常重要： 1234567891011121314151617181920212223242526272829303132333435363738394041// 客户端的消息class UserMessage : public Message&#123;public: UserMessage(const char* data, size_t len) &#123; if (data != nullptr) &#123; _data = new char[len]; memcpy(_data, data, len); _len = len; &#125; else &#123; _data = nullptr; _len = 0; &#125; &#125; MessageType GetType() const override &#123; return MessageType::kUserMessage; &#125;public: char* _data; size_t _len;&#125;;// 附加一个成员T的客户端消息template&lt; typename T &gt;class UserMessageT : public UserMessage&#123;public: UserMessageT(const char* data, size_t len, T user) : UserMessage(data, len), _user(user)&#123;&#125; inline T GetClient() const &#123; return _user; &#125;public: T _user;&#125;; 对于其他消息放到后面介绍。纵观Message，通过继承完成对多类消息的分类处理，通过模板和继承完成对消息类的扩展，而模板参数则为消息结构(对于InsideMessageT)或其它附加成员(对UserMessageT)。 Service 服务整个NGServer核心概念便是Service,Service完成传统游戏服务器一个线程的任务，但它不完全是线程。目前先把它看作是一个线程。在NGServer中，包含如下Service： LoginService(登录服务) MapService(地图服务) DBService(数据库服务) LogService(日志服务) 它们的继承体系如下： 下面简要介绍一下Service每一层实现的一些接口，以及意义： 服务基类Service： 抽象服务的公共接口，如压入消息，处理消息，发送消息等，以及提供一些服务会用到的公共组件，比如定时器，当前时间，处理情况等。下面是一些重要接口： ServiceService包含一个消息队列MessageQueue,保存待处理的消息。MessageQueue和ByteBuff类似，使用双缓冲。每个Service都包含一个_sid用于唯一标识自己。以下是一些主要接口： 123456789101112// 消息投递Service::PushMsg(Message* msg) // 向该Service推送消息，即将消息压入消息队列// 消息处理Service::Receive() // 处理消息队列中的消息 取出消息队列中的消息并调用ReceiveMsg(msg)处理Service::ReceiveMsg(Message* msg) // 处理单条消息 它取出消息类型，还原消息为本身指针，最后分发到ProcessMsgService::ProcessMsg( ... ) // 虚函数接口，通过重载处理各类消息// 消息转发Service::SendMsg( ... ) // 创建InsideMessage 并将消息通过Service::Send()转发到其它服务Service::Send( int32_t sid, Message* msg ) // 静态函数 将msg转发到sid对应的Service GameServiceGameService是游戏业务逻辑处理服务的基类，它主要在Service的基础上加入服务器的具体业务，主要扩展了： 关联PlayerManager PlayerManager管理了所有玩家的连接，当GameService::ProcessMsg(UserMessage*)收到客户端断开的消息时，需要通过PlayerManager管理所有连接的玩家。并且在游戏逻辑处理中，有时需要通过用户的连接ID获取用户(此时用户还没有对于服务器的ID，比如还在登录状态)。 回调和消息处理机制: 消息的注册于回调机制：提供RegistMsg RegistPlayer RegistInside等注册消息回调函数的方法。这些函数的具体处理和实现到后面再解析，这里只需明白可以通过它实现对消息的注册与回调。GameService重写了ProcessMsg(InsideMsg ) 和 ProcessMsg(UserMessage )，在其中完成对消息回调的处理。这样只要调用Service::Receive()，将发生如下流程： 12Service::Receive() -&gt; Service::ReceiveMsg(msg) -&gt; GameService::ProcessMsg(msg) -&gt; 消息回调机制 -&gt; 对应回调函数 关联数据库和日志服务： 添加LogService，DBService 和 HeroManager成员，并且提供设置它们的接口。方便游戏服务更加专心方便地处理业务逻辑。 消息发送和转发： 定义SendToDB SendToLog函数，与日志或数据库通信，它们将调用Service::SendMsg将消息推送到日志服务或数据库服务的消息队列。 添加SendToClient 将消息群发给所有管理的用户，将消息体编码成数据流，最后调用Send(data ,len)来发送数据。 Send(char* data, int len)是纯虚函数接口，用于服务具体定义如何将消息发送到所管理的所有用户(群发)。 DBService LogService相对于GameService，LogService和DBService则要简单许多，它们负责接收GameService发来的消息，并且将记录写入日志或数据库。因此它们只处理InsideMsg消息。并不处理具体的玩家业务逻辑(UserMessage)，它们与数据库和日志系统打交道。但是由于直接派生于Service，因此对比于GameService，它们也需要消息注册与回调机制。另外，由于Service在运行时是单线程的(后面ServiceManager中解释)，因此它的处理是串行的，所以它可以通过记录_last_recv_service_id 来对源Service进行响应。比如响应数据库操作结果等。这样就实现了纯异步的交互。 LoginService MapService得益于GameService的再次封装，具体业务处理服务就真的只需要关心业务逻辑了，让我们以用户登录为例，看看LoginService需要做些什么： 通过RegistPlayer注册用户登录消息响应函数OnPlayerLogin(Player&amp; player, C2S_Login&amp; msg) 并注册数据库响应消息 OnDBHeroLogin(Player&amp; player, D2S_Login&amp; msg) 在OnPlayerLogin中处理用户登录，通过SendToDB SendToLog与数据库交互 在OnDBPlayerLogin中处理数据发来的处理结果 Done 注：消息回调机制会自动将UserMessageT中的client提取出来，并且将对应消息体解包，传入回调函数，因此OnDBHeroLogin可以获取到Player的引用，而UserMessageT中的client初始化是在消息构造时传入的，这中消息编解码中详解。对于其他类型消息处理，比如CycleMessage LoginService需要自己重写ProcessMsg(CycleMessage*) ServiceManagerServiceManager是整个NGServer的消息集散中心，负责管理所有Service和Message。它将Service和它的_sid对应起来。事实上Service::Send就是通过ServiceManager::Send来转发消息的。 前面提到，Service对于业务逻辑层来说，可以看作一个线程。而它实际上并不是个线程，ServiceManager中提供一个线程池，由它们来将所有的Service”跑起来”，此时的Service相当于一个特殊的”消息队列”，只不过它提供了处理这些消息的接口，也就是Receive(): 123456789101112131415161718192021222324252627282930313233// 取出消息队列中的消息 调用ReceiveMsg处理消息// 如果处理完之后 队列中还有剩余消息 则返回true 否则返回falsebool Service::Receive()&#123;#ifdef _DEBUG if (!_recvcheck.TryLock()) &#123; std::cerr &lt;&lt; &quot; # service Receive is not runing in single thread ! &quot; &lt;&lt; std::endl; assert(0); &#125;#endif std::vector&lt;Message*&gt;* msgs = _msgqueue.PopAll(); for(auto msg : *msgs) &#123; std::unique_ptr&lt;Message&gt; autodel(msg); if (!ReceiveMsg(msg)) &#123; autodel.release(); &#125; &#125; msgs-&gt;clear(); if (_msgqueue.Size()) return true;#ifdef _DEBUG _recvcheck.UnLock();#endif _readylock.UnLock(); return false;&#125; 该接口确保单线程运行(Service内部MessageQueue双缓冲只能单线程处理数据)，取出消息队列中的消息，调用ReceiveMsg进行处理，后者通过Message::GetType()还原消息类型，调用ProcessMsg重载，然后GameService::ProcessMsg中完成对消息的回调….. 然而Receive()仅处理Service消息队列中已有的消息，并没有让Service一直”run”起来，这也是Service比直接用线程更为高效的地方：充分利用线程。只有当Service中有消息时，Service::Receive才会被调用，处理完成之后，线程就”离开”，去跑别的Service。而要做到这点，有两个要点： 保证Service::Receive()同一时刻只被一个线程运行 捕捉Service中MessageQueue的状态变化，在MessageQueue中有消息时，在1的前提下，能够第一时间让Service分配到线程。 为了做到以上两点，ServiceManager中维护一个Service队列ServiceQueue _ready_services，该队列线程安全。它保存那些消息队列不为空的Service，也就是”就绪”的Service。_ready_services可以看作一个特殊的”消息队列”：它们维护一组消息，并提供这些消息的处理接口。而ServiceManager中的线程池，则在处理这个特殊的”消息队列”(通过调用Service::Receive())。一个Service是否”就绪”，可以用一个锁_readylock来实现，_readylock锁定表示该Service消息队列不为空，已经就绪，否则表示该Service处于”空闲”状态。_readylock可能会在两个地方改变状态： Service::PushMsg()中，可能使消息队列由空变为不空。这可以通过 _readlock.TryLock()来检测并改变该状态。 Service::Receive()中，处理完消息队列中的消息后，如果消息队列为空(由于双缓冲机制，在处理读缓冲的数据时，可能有新的数据到达写缓冲)，则释放_readylock：_readylock.UnLock();否则_readylock仍然为Lock状态。 接下来就是对Service _readylock的监测，如果_readlock为Lock状态，则将其加入到”就绪服务”队列_ready_services中。最好的办法当然是在状态可能改变的地方： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849// 发送消息到指定Service msg的管理权将转交 调用者不需再关心msg的释放问题bool ServiceManager::Send(int32_t sid, Message* msg)&#123; if (sid &lt; kMaxServiceNum) &#123; ServicePtr sptr = _serviceMap[sid]; if (sptr != nullptr) &#123; if (sptr-&gt;PushMsg(msg)) &#123; // 将该服务加入到就绪服务队列 该队列线程安全 PushService(sptr); &#125; return true; &#125; &#125; delete msg; return false;&#125;// ServiceManager线程入口，通过该入口让所有Service Run起来// 该函数不断从就绪服务队列中取出服务，并执行其Receive入口处理Service中的消息void ServiceManager::ExecThread()&#123; try &#123; // 不断执行_ready_services中的Service while (_runing) &#123; ServicePtr sptr = _ready_services.Pop(); if (sptr != nullptr) &#123; if (sptr-&gt;Receive()) &#123;// 如果执行完成后 还有未处理消息 // 重新投递到待执行队列 PushService(sptr); &#125; &#125; else &#123; std::this_thread::sleep_for(std::chrono::milliseconds(1)); &#125; &#125; &#125; catch (std::runtime_error&amp; err) &#123; std::cerr &lt;&lt; &quot;runing thread catch one exception : &quot; &lt;&lt; err.what() &lt;&lt; std::endl; &#125;&#125; ExecThread函数，就是整个Service，乃至整个框架的发动机，通过让多个thread执行该入口，即可充分利用多线程，均衡处理所有Service中的消息: 123456789101112131415161718192021// 开始运行// threadNum：指定运行的线程数量// 如果ServiceManager已经在运行中 则在原有线程基础上再新开threadNum个线程void ServiceManager::Start(int threadNum)&#123; AutoLocker aLock(&amp;_locker); if (_runing == false) &#123; // ServiceManager需要一个TimerThread用于管理所有定时消息 _runing = true; std::thread* t = new std::thread(TimerThread); _threads.push_back(t); &#125; for (int i = 0; i &lt; threadNum; i++) &#123; std::thread* t = new std::thread(ExecThread); _threads.push_back(t); &#125;&#125; 整个流程一. 框架消息处理流程 ServiceManager::Start(int threadNum) 指定线程池线程数 开始运行所有Service::Receive() Service::Receive()从双缓冲消息队列中取出已有消息，逐个调用Service::ReceiveMsg(Message* msg)处理单条消息 Service::ReceiveMsg(Message* msg)通过Message::GetType()得到每条消息类型，并且通过std::dynamic_cast将msg转换成对应类型nmsg，最后调用ProcessMsg(nmsg)完成分发 基类Service::ProcessMsg定义了所有消息的处理接口： 1234567// 接口 处理各类消息 返回true代表消息将由框架删除 返回false自行管理该消息virtual bool ProcessMsg(Message* msg);virtual bool ProcessMsg(TimerMessage* msg);virtual bool ProcessMsg(UserMessage* msg);virtual void ProcessMsg(CycleMessage* msg);virtual bool ProcessMsg(InsideMessage* msg); 如果调度的Service本身重写了对应ProcessMsg,那么将调用重写的ProcessMsg，否则将使用基类Service的ProcessMsg,后者只是忽略消息，不对消息做处理。对于GameService，它重写了ProcessMsg: 123bool ProcessMsg(UserMessage* msg) override;bool ProcessMsg(InsideMessage* msg) override; 并完成了对消息的解码和响应函数的回调，因此对于LoginService和MapService，它们只需调用Regist注册消息响应函数后，ProcessMsg会将消息解码并回调到对应函数。ProcessMsg中的回调机制将逻辑由框架导出到了业务层。 二. 服务的消息推送流程 前面说的是消息的处理流程，下面从消息的产生开始讨论消息的生命周期和传递流程。消息一共有四种：UserMessage(T) InsideMessage(T) CycleMessag TimerMessage，后两种定时器相关的消息由ServiceManager统一管理，因此这里不作阐述。 UserMessage是来自客户端的消息，在前面的博客中，讲到了网络层到框架的接口函数：Player::Decode(const char* data, size_t len)，网络层将收到的数据交给该函数(当len==0时，表示客户端断开连接)： 12345678910111213141516171819202122232425262728293031323334353637int32_t Player::Decode(const char* data, size_t len)&#123; // 客户端断线 if (data == nullptr || len == 0) &#123; // 通知业务逻辑层 处理下线逻辑 Message* msg = new UserMessageT&lt;PlayerPtr&gt;(data, len, shared_from_this()); ServiceManager::Send(_sid, msg); return 0; &#125; // 消息的解包 const char* buff = data; size_t remainLen = len; static const uint16_t headLen = ProtocolStream::kHeadLen + ProtocolStream::kMsgIdLen; while (remainLen &gt; headLen) &#123; int32_t msgLen = std::max(headLen, *((uint16_t*)buff)); if (remainLen &lt; msgLen) &#123; break; &#125; // 发送到Service框架层 Message* msg = new UserMessageT&lt;PlayerPtr&gt;(buff, msgLen, shared_from_this()); if (!ServiceManager::Send(_sid, msg)) &#123; // 服务器主动断线 return -1; &#125; remainLen -= msgLen; buff += msgLen; &#125; return remainLen;&#125; Player::Decode简单解决粘包问题，当客户端有数据来临(len!=0)或断开连接时(len==0)，均创建UserMessageT并传入Player指针，通过ServiceManager::Send发送到Service框架。这里传入的Player指针很重要，框架的消息回调机制就是通过这个指针来将消息关联到Player的。在PlayerManager::OnConnect()中，有新用户连接时，创建Player的同时为Player指定了一个所属服务，这个服务的sid保存在Player中。Player的所有消息均发往其所属服务。对于刚连接的Player，该服务自然是LoginService。当Player登录成功时，将所属服务特换为MapService，之后所有的业务逻辑都在MapService上面跑。 InsideMessage是服务之间的内部消息，它在Service之间转发消息时产生，通过Service::SendMsg创建内部消息，最后通过Service::Send发送。 123456789101112131415161718192021222324// 发送只包含消息ID的内部消息bool SendMsg(int32_t sid, int64_t sessionid, int16_t msgid)&#123; InsideMessage* msg = new InsideMessage(); msg-&gt;_dessid = sid; msg-&gt;_srcsid = GetSid(); msg-&gt;_sessionid = sessionid; msg-&gt;_msgid = msgid; Service::Send(sid, msg);&#125;// 发送包含消息数据的内部消息template &lt; typename MsgT &gt; bool SendMsg(int32_t sid, int64_t sessionid, int16_t msgid, MsgT&amp; t)&#123; InsideMessageT* msg = new InsideMessageT&lt;MsgT&gt;(); msg-&gt;_dessid = sid; msg-&gt;_srcsid = GetSid(); msg-&gt;_sessionid = sessionid; msg-&gt;_msgid = msgid; msg-&gt;_data = t; Service::Send(sid, msg);&#125; UserMessage和InsideMessage在创建之后，都会交给ServiceManager::Send，之后便不用关心其生命周期。Message由框架管理。在Service处理这些消息时： 1234567891011121314151617181920212223242526// 取出消息队列中的消息 调用ReceiveMsg处理消息// 如果处理完之后 队列中还有剩余消息 则返回true 否则返回falsebool Service::Receive()&#123; //.... std::vector&lt;Message*&gt;* msgs = _msgqueue.PopAll(); for (auto msg : *msgs) &#123; // 确保消息处理完成后自动删除 std::unique_ptr&lt;Message&gt; autodel(msg); if (!ReceiveMsg(msg)) &#123; // ReceiveMsg返回false 取消自动删除 autodel.release(); &#125; &#125; msgs-&gt;clear(); if (_msgqueue.Size()) return true; // ... return false;&#125; ReceiveMsg处理完消息后，返回true，消息将由框架自动删除，否则消息将由逻辑自行保管。通常不自动删除的消息是帧消息，该消息始终只有一条，处理完成之后，调整下次触发时间，再将其加入到定时器队列。 三. 完整的消息请求与响应1.用户连接PlayerManager::OnConnect 创建并关联Player和Session 并且为Player指定所属登录服务的_sid -&gt; Session::StartRecv 开始接收数据 2.用户请求与响应推送请求：Session::ReadComplete 数据到达 -&gt; Player::Decode 解包 -&gt; ServiceManager::Send 推送消息到指定服务 -&gt; Service::PushMsg 此时消息已经在服务的消息队列 处理和响应请求：Service::Receive 取出消息 -&gt; Service::ReceiveMsg 还原消息 -&gt; Service::ProcessMsg 重载各类消息的处理方式 GameService和DBService的ProcessMsg中，完成对消息的解码和回调 -&gt; 消息响应函数 -&gt; Player::SendMsg 发送响应 -&gt; Session::SendMsg 完成对消息的编码 -&gt; Session::SendAsync 发送消息数据","tags":[{"name":"ngserver","slug":"ngserver","permalink":"http://wudaijun.com/tags/ngserver/"}]},{"title":"NGServer Session设计","date":"2014-10-29T16:00:00.000Z","path":"2014/10/ngserver-session/","text":"在网络编程模型中，一个Session代表一次会话，主要维护网络数据的发送和接收。对外提供发送数据和处理数据的接口。一个高效的Session主要通过缓冲和异步来提高IO效率。NGServer的Session运用双缓冲和boost::asio的异步机制，很好地做到了这一点。 一. 双缓冲在网络IO中，读写线程的互斥访问一直都是一个关乎性能的大问题。为了减少互斥锁的使用，环形缓冲和双缓冲是常见的策略。NGServer使用后者作为消息和数据缓冲。在NGServer MessageQueue.h中，定义了两种双缓冲：基于消息的MessageQueue和基于数据的ByteBuff。下面简要介绍ByteBuff类： ByteBuff类的基本思想是通过两个缓冲区_buff_read和_buff_write来使读写分离。通过size_t Push(const char* data, size_t len)来写入数据： 1234567891011121314// 压入字节流数据 压入成功 则返回当前缓冲区长度 否则返回0size_t Push(const char* data, size_t len)&#123; if(data != nullptr &amp;&amp; len &gt; 0) &#123; AutoLocker aLock(&amp;_lock); if(_size+len &lt;= _capacity) &#123; memcpy(_buff_write+_size, data, len); return _size += len; &#125; &#125; return 0;&#125; Push方法是线程安全的，它通过AutoLocker来保证对_buff_write的互斥访问。 char* PopAll(size_t&amp; len)用于读取数据: 1234567891011121314151617// 返回当前缓冲区指针 长度由len返回 若当前缓冲区无消息 返回nullptrchar* PopAll(size_t&amp; len)&#123; if(_size &gt; 0) &#123; AutoLocker aLock(&amp;_lock); if(_size &gt; 0) &#123; swap(_buff_read, _buff_write); len = _size; _size = 0; return _buff_read; &#125; &#125; len = 0; return nullptr;&#125; 它返回当前_buff_write的指针，并且交换_buff_write和_buff_read的指针，这样下次再Push数据时，实际上写到了之前的_read_buff中，如此交替，完成读写分离。 需要注意到，Push接口是线程安全的，而对于PopAll：由于PopAll直接返回缓冲区指针(避免内存拷贝)，因此同一时刻双缓冲中，必有一读一写，故同一时刻只能有一个线程读取和处理数据(处理数据时,_buff_read仍然是被占用的)。读取线程需要将上次PopAll的数据处理完成之后再次调用PopAll。因为调用PopAll时，之前的读缓冲已变成写缓冲，并且写缓冲将从头开始写。 基于消息的MessageQueue原理与ByteBuff一样，只不过_buff_read和_buff_write均为vector* 类型。MsgT是用户定义的消息类。由于使用的MsgT*，提高效率的同时，需要注意消息的释放问题。这在使用到MessageQueue时再提。 二. Session类的设计Session类利用boost::asio异步读写提高IO性能，它使用线性缓冲作为接收缓冲，使用ByteBuff作为发送缓冲，提高发送性能。由于ByteBuff同一时刻只能由一个线程读取和处理，Session需要使用一个锁来保证同一时刻只有一个线程来读取ByteBuff并发送其中的数据： 1234567891011121314151617181920212223242526272829303132333435363738394041// 发送数据bool Session::SendAsync(const char* data, size_t len)&#123; if (!_run) return false; if (_send_buf.Push(data, len)) &#123; if (_sending_lock.TryLock()) &#123; size_t sendlen; const char* data = _send_buf.PopAll(sendlen); // 异步发送数据 同一时刻仅有一个线程调用该函数 SendData(data, sendlen); return true; &#125; &#125; else assert(0); // 发送缓冲区满&#125;void Session::SendComplete(const boost::system::error_code&amp; err, size_t bytes_to_transfer, size_t bytes_transferred)&#123; if (err) return; assert(bytes_to_transfer == bytes_transferred); _send_total += bytes_transferred; size_t len; const char* data = _send_buf.PopAll(len); if (data) // 如果还有数据 继续发送 &#123; SendAsync(data, len); &#125; else &#123; _sending_lock.UnLock(); &#125;&#125; 当网络空闲时，在SendAsync中，消息通过Push压入缓冲区后，将即时发送。当网络IO繁忙时，调用SendAsync中，可能已有数据正在发送，在将新数据压入缓冲区后，_sending_lock.TryLock()将返回false，此时数据被放在缓冲区中。待已有数据发送完成后，_sending_lock解锁。那么下次调用SendAsync发送的数据将和缓冲区中已有的数据立即发送。而ByteBuff双缓冲最大程度避免了这个过程中的内存拷贝。 Session将收到的数据放在线性缓冲区中，如此方便解包。在每次接收数据完成后，都尝试解包，并校正缓冲区新的偏移。","tags":[{"name":"ngserver","slug":"ngserver","permalink":"http://wudaijun.com/tags/ngserver/"}]},{"title":"NGServer 简介","date":"2014-09-19T16:00:00.000Z","path":"2014/09/ngserver-startpage/","text":"NGServer是一个迷你型C++游戏服务器框架。Github地址：https://github.com/wudaijun/NGServer。 主要特性： 框架用C++(11)和boost库实现。 基于单进程多线程。 框架屏蔽了多线程实现，上层体现为服务(Service)，服务之间通过消息进行通信。 有比较完善灵活的的消息回调和序列化机制，更方便地实现RPC。 设计原则： 尽量小巧灵活，减少第三方库依赖，尽可能使用C++11新特性。 降低模块之间的耦合性，增强灵活性。 目前缺点： 当并发很高时，消息分流和拥塞控制做得不是很好。 服务之间，只支持通过消息异步通信。 更多文档：关于NGServer的更详细的系列介绍可以在我的博客找到：http://wudaijun.com/tags/#NGServer。博客上的文章仅代表NGServer的最初设想，可能与Github上的最新代码有差异。","tags":[{"name":"ngserver","slug":"ngserver","permalink":"http://wudaijun.com/tags/ngserver/"}]}]